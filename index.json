[{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Vo Hoang Minh\nPhone Number: 0826400643\nEmail: minh7n3@gmai.com\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"1. High-Level Architecture The IELTS BandUp platform is built on a robust, highly available architecture on AWS. The system is designed to handle user traffic securely while providing low-latency access to study materials and AI-powered features.\n2. Core AWS Services To achieve the goals of scalability, security, and high availability, we utilize the following key AWS services:\nNetworking \u0026amp; Content Delivery Amazon VPC (Virtual Private Cloud): The foundational network layer. We utilize a custom VPC with isolated Public and Private subnets to strictly control traffic flow. NAT Gateway: Allows instances in private subnets (like our Backend containers) to access the internet for updates or external API calls without being exposed to incoming public traffic. Application Load Balancer (ALB): Distributes incoming application traffic across multiple targets (containers) in different Availability Zones, ensuring fault tolerance. Amazon Route 53: A scalable Domain Name System (DNS) web service used for domain registration and traffic routing. Compute \u0026amp; Containers Amazon ECS (Elastic Container Service) on Fargate: A serverless compute engine for containers. We use Fargate to run both our Next.js Frontend and Spring Boot Backend, removing the need to provision or manage servers. Amazon ECR (Elastic Container Registry): A fully managed container registry where we store, manage, and deploy our Docker container images. Database \u0026amp; Storage Amazon RDS (Relational Database Service): We use PostgreSQL in a Multi-AZ deployment (Primary and Standby) to ensure data durability and disaster recovery for user profiles and test data. Amazon ElastiCache (Redis): Acts as an in-memory data store to cache frequent queries and manage user sessions, significantly improving application performance. Amazon S3 (Simple Storage Service): Stores static assets, media files (audio for listening tests), and user-generated content securely. AI \u0026amp; Serverless Integration To power the intelligent features of BandUp (Writing/Speaking Feedback, Flashcard Generation), we use a Serverless approach:\nAmazon Bedrock \u0026amp; Google Gemini API: The core Generative AI models used to analyze user inputs and generate personalized study feedback. AWS Lambda: Serverless compute functions that orchestrate the AI workflow, connecting the application to AI models. Amazon SQS (Simple Queue Service): Decouples the backend from the AI processing layer, allowing requests to be queued and processed asynchronously to prevent system overload. Amazon API Gateway: Acts as the \u0026ldquo;front door\u0026rdquo; for the AI services, managing RESTful API calls securely. DevOps \u0026amp; CI/CD AWS CodePipeline: Automates the release pipelines for fast and reliable application and infrastructure updates. AWS CodeBuild: Compiles source code, runs tests, and produces software packages (Docker images) ready to deploy. Security AWS WAF (Web Application Firewall): Protects the web application from common web exploits. AWS Secrets Manager: Securely stores and manages sensitive credentials (database passwords, API keys) throughout their lifecycle. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup-fe/5.4.1-docker/","title":"Setup ECR &amp; Push Image","tags":[],"description":"","content":"In this step, we will containerize our Next.js Frontend application and push the Docker image to Amazon Elastic Container Registry (ECR). This image will later be used by ECS Fargate to launch the application.\n1. Prepare Dockerfile We use a multi-stage Dockerfile optimized for Bun (a fast JavaScript runtime) and Next.js. This configuration reduces the final image size and improves security.\nBase Image: oven/bun:1.1.26 Builder: Compiles the Next.js application. Runner: A lightweight production environment exposing port 3000. 2. Build Docker Image Run the following command in your project root to build the image. We tag it as band-up-frontend.\ndocker build -t band-up-frontend . The build process will install dependencies using bun install and compile the project.\n3. Verify Local Image Once the build is complete, verify that the image exists locally.\ndocker image ls You should see band-up-frontend with the latest tag.\nTest Locally: You can try running the container locally to ensure it starts up correctly before pushing to AWS.\ndocker run -p 3000:3000 band-up-frontend:latest 4. Push to Amazon ECR Now we need to upload this image to AWS.\nStep 1: Create Repository\nGo to Amazon ECR \u0026gt; Repositories. Click Create repository. Visibility settings: Private. Repository name: band-up-frontend. Click Create repository. Step 2: Push Image Use the AWS CLI to authenticate and push the image. Replace [AWS_ACCOUNT_ID] and [REGION] with your details.\nLogin to ECR:\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com Tag the Image:\ndocker tag band-up-frontend:latest [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:latest Push the Image:\ndocker push [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:latest Once the push is complete, your image is hosted on AWS ECR and ready for deployment.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-setup-be/5.5.1-ecr/","title":"Setup ECR &amp; Push Image","tags":[],"description":"","content":"In this step, we will containerize the Spring Boot Backend and push the optimized Docker image to Amazon ECR.\n1. Dockerfile Strategy For the Backend, we utilize a Multi-Stage Build strategy with eclipse-temurin:21 (Java 21). This ensures a small and secure final image.\nStage 1 (Deps): Resolves and downloads Maven dependencies. Stage 2 (Package): Builds the application and extracts the Spring Boot Layered Jar. This splits the application into layers (dependencies, spring-boot-loader, application code), allowing Docker to cache unchanged layers (like dependencies) effectively. Stage 3 (Final): Copies the extracted layers into a lightweight JRE image. It also creates a non-privileged user appuser for security. 2. Build Docker Image Run the build command in the backend project root. We tag the image as band-up-backend.\ndocker build -t band-up-backend . Docker will execute the stages defined above.\n3. Create ECR Repository We need a repository to store this image.\nNavigate to Amazon ECR \u0026gt; Create repository. Repository name: band-up-backend (Ensure this matches your push command). Visibility: Private. Image tag mutability: Mutable. Click Create repository. 4. Push Image to ECR Once the image is built and the repository is ready, proceed to push.\nStep 1: Tag the Image Tag the local image with a version number (e.g., v1.0.0).\ndocker tag band-up-backend:latest [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-backend:v1.0.0 Step 2: Push to ECR Upload the layers to AWS.\ndocker push [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-backend:v1.0.0 5. Verify Navigate to the Amazon ECR Console and select the bandup-backend repository. You should see the image tagged v1.0.0.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-network/5.3.1-vpc/","title":"VPC, Subnets &amp; Routing","tags":[],"description":"","content":"In this step, we establish the isolated network environment for IELTS BandUp. We will create a Virtual Private Cloud (VPC), partition it into subnets across multiple Availability Zones, and configure routing for internet access.\n1. Create the VPC First, we need a private network space.\nNavigate to the VPC Dashboard. Click Create VPC. Choose VPC only. Name tag: band-up-vpc IPv4 CIDR block: 10.0.0.0/16 (This provides 65,536 IP addresses, sufficient for future scaling). Leave other settings as default and click Create VPC. 2. Create Subnets Next, we divide the VPC into smaller networks (Subnets) distributed across two Availability Zones (AZs) for High Availability. We will follow this IP schema:\nSubnet Name Type CIDR Block Availability Zone public-subnet-1 Public 10.0.0.0/24 ap-southeast-1a public-subnet-2 Public 10.0.1.0/24 ap-southeast-1b private-app-subnet-1 Private 10.0.2.0/24 ap-southeast-1a private-app-subnet-2 Private 10.0.3.0/24 ap-southeast-1b private-database-subnet-1 Database 10.0.4.0/24 ap-southeast-1a private-database-subnet-2 Database 10.0.5.0/24 ap-southeast-1b Steps:\nGo to Subnets \u0026gt; Create subnet. Select the VPC ID: band-up-vpc. Enter the Subnet name, Availability Zone, and IPv4 CIDR block for each subnet according to the table above. Repeat the process until all 6 subnets are created. 3. Create Internet Gateway (IGW) By default, a VPC is closed to the internet. To allow resources in our Public Subnets to communicate with the outside world, we need an Internet Gateway.\nGo to Internet gateways \u0026gt; Create internet gateway. Name tag: band-up-igw. Click Create internet gateway. After creation, click Actions \u0026gt; Attach to VPC. Select band-up-vpc and click Attach internet gateway. 4. Configure Route Tables Finally, we need to direct traffic from our Public Subnets to the Internet Gateway.\nGo to Route tables \u0026gt; Create route table. Name: public-route-table. VPC: band-up-vpc. Click Create route table. Add Route to Internet:\nSelect the newly created public-route-table. Go to the Routes tab \u0026gt; Edit routes. Add a new route: Destination: 0.0.0.0/0 (All traffic). Target: Select Internet Gateway -\u0026gt; band-up-igw. Click Save changes. Associate Subnets:\nGo to the Subnet associations tab \u0026gt; Edit subnet associations. Select only the Public Subnets (public-subnet-1 and public-subnet-2). Click Save associations. 5. Verify Configuration To verify that the network architecture is correctly established, navigate back to your VPC Dashboard, select band-up-vpc, and view the Resource map tab. You should see a clear structure linking your Public Subnets to the Route Table and the Internet Gateway.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Post-Event Report: “The First Cloud Journey (FCJ) Program Kick-off” Event Objectives Officially launch the 12-week The First Cloud Journey (FCJ) program. Connect and socialize among members, Mentors, and the Organizing Committee (OC). Provide an overview of the organization, learning goals, and project implementation roadmap. Guide the team working process and proceed with forming project teams. Speaker List Representative of the Organizing Committee (OC) - Introduced the program\u0026rsquo;s vision and mission. Featured Mentors - Shared experiences and successful Cloud career paths. Former Members (Alumni) - Shared practical lessons and insights. Key Highlights Program Framework and 12-Week Roadmap Detailed presentation of the 12-week roadmap, covering everything from basic AWS knowledge (VPC, EC2) to implementing a complete Serverless project. Established specific goals regarding knowledge acquisition and the minimum viable product (MVP) deliverable. Working Rules and Organizational Culture Rules on discipline and participation: Emphasized individual commitment and responsibility throughout the journey. Culture of sharing and support: Built a strong sense of community among members. Team Formation Process and Project Guidelines Guidance on how to create effective teams (role assignment, communication tools). Provided initial direction for Cloud projects for teams to begin research. AWS Account Security Instructions on setting up Two-Factor Authentication (2FA) and basic steps for cost management (Budget) from the start. Key Takeaways Mindset and Commitment Importance of discipline: Clearly recognized the seriousness and level of commitment required to complete the program. Role of community: Understood the value of learning and support from Mentors and other members. Organizational Knowledge Program objectives: Clearly understood the Cloud knowledge and skills the program aims to equip participants with. Clear learning path: Grasped the main modules to be conquered over the 12 weeks. Initial Practical Skills Team formation process: Successfully completed the creation of the project team with new members. Basic security: Learned the initial steps to set up a secure and cost-optimized AWS account. Application to Work Immediately formed the team and assigned initial tasks. Began researching the organization and fundamental Cloud concepts. Executed basic operations on the AWS account such as setting up 2FA and Budget (as per Worklog Week 1). Event Experience Participating in the Kick-off was an experience full of energy and clear direction. This event laid the foundation for the entire 12-week journey:\nConnection and Community Energy Feeling welcomed: Met and socialized with all members of FCJ, creating a positive learning and teamwork environment. Seriousness level: The Kick-off clearly conveyed the OC\u0026rsquo;s commitment, making me aware of the necessary seriousness throughout the internship. Roadmap and Objectives Clear direction: Saw the entire 12-week roadmap visually for the first time, helping me proactively plan my studies. Project value: Understood that the ultimate goal is not just to learn, but also to create a real Cloud product. Initial Practical Lessons Importance of 2FA and Budget: Received direct guidance on AWS security and cost management operations—essential lessons from day one of using the Cloud. Teamwork: Quickly formed the team and began discussing coordination methods. Conclusion The Kick-off was more than just an introductory event; it was a training session that provided sufficient information and motivation for me to confidently start the 12-week journey of conquering the Cloud.\nEvent Photos Add your photos here Overall, the event successfully achieved its goals: providing information, establishing discipline, and igniting enthusiasm for a journey of specialized learning and development.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives Connect and get acquainted with members of First Cloud Journey (FCJ). Understand the organization and basic AWS services. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Attend the FCJ kick-off session.\n- Learn about the FCJ organization.\n- Form a team for collaborative project work. 06/09/2025 06/09/2025 3 - Create an AWS account.\n- Study cloud computing concepts.\n- Draw a sample architecture using draw.io. 09/09/2025 09/09/2025 REFER HERE 4 - Explore the objectives of the First Cloud Journey program and the AWS website.\n- Perform initial setup on the AWS account:\n+ Create a budget.\n+ Create user groups.\n+ Enable two-factor authentication (2FA).\n- Learn about the AWS Support Center and how to submit support requests. 10/09/2025 10/09/2025 REFER HERE 5 - Create a VPC and configure its settings.\n- Create and configure Subnets (including a public Subnet with auto-assigned public IPs).\n- Set up an Internet Gateway and attach it to the VPC.\n- Create and configure a Route Table, connecting it to the Internet Gateway.\n- Configure Subnet Associations.\n- Create Security Groups (public and private). 11/09/2025 14/09/2025 REFER HERE Week 1 Achievements Successfully participated in the FCJ kick-off session and connected with team members. Gained an understanding of the FCJ organization and its objectives. Formed a team to collaborate on projects. Created an AWS account and completed initial setup tasks: Set up a budget to monitor costs. Created user groups for access management. Enabled two-factor authentication (2FA) for enhanced security. Studied basic cloud computing concepts and successfully drew a sample architecture using draw.io. Learned how to navigate the AWS Support Center and submit support requests. Successfully completed VPC-related tasks: Created and configured a VPC. Set up Subnets, including a public Subnet with auto-assigned public IPs. Created and attached an Internet Gateway to the VPC. Configured a Route Table and connected it to the Internet Gateway. Established Subnet Associations. Created public and private Security Groups for secure resource access. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This page documents the entire Worklog carried out throughout the First Cloud Journey (FCJ) internship program at AWS. This document details the process of learning, implementing the Bandup IELTS project, troubleshooting system errors, and participating in specialized events over 12 weeks (approximately 3 months).\nDuring these 12 weeks, I transitioned from familiarizing myself with core Cloud concepts to building and optimizing a complete AI-powered Serverless application on AWS, completing 50+ AWS Skill Builder courses along the way.\nSummary of Work by Week: Week Focus Area Week 1 Familiarization with FCJ, AWS account creation, basic Cloud concepts, and foundational network setup (VPC, Subnets, Internet Gateway). Week 2 Mastered Amazon EC2 and VPC fundamentals, completed AWS Skill Builder courses on IAM, Budgets, EC2, and attended Cloud Day event for AI/Data insights. Week 3 Resolved AWS account issues, configured Hybrid DNS with Route 53 Resolver and VPC Peering, learned CloudFormation and Cloud9 for IaC development. Week 4 Mastered AWS Transit Gateway for centralized network management, deep dive into EC2 Auto Scaling, Lightsail, and Migration services (DMS, VM Import/Export). Week 5 Analyzed and optimized AWS costs, designed Serverless infrastructure architecture, learned RDS, DynamoDB, ElastiCache, and set up AWS Toolkit for VS Code. Week 6 Mastered AWS Storage services (S3, Glacier, Storage Gateway), enhanced Python skills, finalized project architecture, and attended DevSecOps \u0026amp; Amazon Q Developer Webinar. Week 7 Comprehensive review and knowledge consolidation of core AWS services (Compute, Storage, Networking, Database, Security) in preparation for the mid-term exam. Week 8 Completed mid-term exam, began implementing foundational CRUD functionalities, researched serverless architecture (Lambda, API Gateway, DynamoDB), and set up development environment. Week 9 Transitioned to AWS SAM, refactored CRUD functionalities, integrated Docker for build environment, and successfully deployed project to AWS overcoming local debugging challenges. Week 10 Debugged CORS and template validation errors, integrated Frontend/Backend, completed Read/Delete functions, resolved Cognito authentication issues, and attended AWS Cloud Mastery Series #1. Week 11 Implemented Multi-Stack architecture for optimization, fixed the persistent CORS error, and began integrating AI Services (Lambda, Bedrock). Week 12 Finalized AI-powered Lambda functions, integrated Gemini API for IELTS evaluation, completed RAG pipeline for flashcard generation, and attended the final AWS Cloud Mastery Series. AWS Skill Builder Learning Path (Weeks 2-5) Category Courses Completed Networking VPC, Route 53, VPC Peering, Transit Gateway, Networking Workshop Compute EC2, EC2 Auto Scaling, Lightsail, Lightsail Containers Security IAM, IAM Roles for EC2 Database RDS, DynamoDB, ElastiCache Migration VM Import/Export, DMS, SCT, Elastic Disaster Recovery DevOps CloudFormation, Cloud9, AWS CLI, AWS Toolkit for VS Code Cost Management AWS Budgets, Cost Explorer, Service Quotas, Right-Sizing Architecture Building Highly Available Web Applications AWS Skill Builder Learning Path (Weeks 6-10) Category Courses Completed Storage Static Website Hosting with S3, AWS Backup, CloudFront Reliability Data Protection with AWS Backup Development AWS Toolkit for VS Code, Serverless patterns Learning Progression Weeks 1-5: Foundation \u0026amp; Exploration\nCore AWS services (EC2, S3, VPC, IAM) Networking fundamentals (VPC, Route 53, Transit Gateway) Cost optimization and architecture design Infrastructure as Code (CloudFormation, Cloud9) Weeks 6-7: Consolidation \u0026amp; Assessment\nStorage services mastery (S3, Glacier, Storage Gateway) Disaster recovery and backup strategies Comprehensive exam preparation Mid-term exam completion Weeks 8-10: Implementation \u0026amp; Deployment\nServerless architecture implementation (Lambda, API Gateway, DynamoDB) AWS SAM framework adoption Docker integration for consistent builds Frontend-Backend integration Production deployment and debugging Weeks 11-12: Advanced Features \u0026amp; AI Integration\nMulti-stack architecture optimization AI services integration (Bedrock, Gemini API) RAG pipeline implementation Final project completion "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-ai-service/5.6.1-api-gateway/","title":"API Gateway","tags":[],"description":"","content":"Overview Create Amazon API Gateway as the entry point for AI service requests.\nCreate REST API Navigate to API Gateway → Create API → REST API Setting Value API name ielts-ai-api API type REST API Endpoint type Regional Create Resources and Methods Endpoints:\nMethod Path Description POST /writing/evaluate Submit writing sample POST /speaking/evaluate Submit audio recording POST /flashcards/generate Generate flashcard POST /upload/audio Upload audio POST /upload/document Upload document Configure SQS Integration For each POST endpoint, configure SQS integration:\nSelect method → Integration Request Integration type: AWS Service AWS Service: SQS HTTP method: POST Action: SendMessage Execution role: API Gateway role with SQS permissions Request Mapping Template:\nAction=SendMessage\u0026amp;MessageBody=$util.urlEncode($input.body)\u0026amp;QueueUrl=$util.urlEncode(\u0026#39;https://sqs.ap-southeast-1.amazonaws.com/{account}/ielts-writing-queue\u0026#39;) Enable CORS Select resource → Enable CORS Access-Control-Allow-Origin: * (or specific domain) Access-Control-Allow-Methods: POST, GET, OPTIONS Deploy API Actions → Deploy API Stage name: dev Note the invoke URL: https://{api-id}.execute-api.ap-southeast-1.amazonaws.com/dev AWS CLI Commands # Create REST API API_ID=$(aws apigateway create-rest-api \\ --name ielts-ai-api \\ --endpoint-configuration types=REGIONAL \\ --query \u0026#39;id\u0026#39; --output text) # Get root resource ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#39;items[?path==`/`].id\u0026#39; --output text) # Create /ai resource AI_RESOURCE=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part ai \\ --query \u0026#39;id\u0026#39; --output text) # Create /ai/writing-assessment resource aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AI_RESOURCE \\ --path-part writing-assessment Next Steps Proceed to SQS Queues.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-cicd-pipeline/5.7.1-create-gwe/","title":"Connect GitLab repo &amp; create CodeBuild project","tags":[],"description":"","content":"Objective Configure two CodeBuild projects (frontend and backend) and a trigger from GitLab Release events that starts CodePipeline. CodePipeline invokes CodeBuild using the repository’s existing frontend-buildspec.yml and backend-buildspec.yml, and then deploys to ECS.\nAWS Resources CodeBuild projects: Frontend: Source = CodePipeline; Buildspec = frontend-buildspec.yml Backend: Source = CodePipeline; Buildspec = backend-buildspec.yml CodePipeline (later step) consuming artifacts and deploying to ECS Create CodeBuild Projects \u0026amp; Connect GitLab Repository In the creating new CodeBuild Project configuration section, select Default project. In the Source section, choose GitLab and Band-Up repository. Leave default configurations for Environment. Specify Band-Up own buildspec for frontend/backend like the following image: Submit and create the other frontend/backend CodeBuild project likewise. Summary You now have two CodeBuild projects (frontend and backend) ready to be invoked by CodePipeline. A GitLab Release event can trigger CodePipeline, which then runs each project with its corresponding frontend-buildspec.yml and backend-buildspec.yml. In subsequent steps, CodePipeline will take the build artifacts and deploy to ECS.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Post-Event Report: “DX Talk#7: Reinventing DevSecOps with AWS Generative AI” Event Objectives Explore strategic and practical perspectives on the powerful transformation of AI in DevSecOps. Introduce comprehensive AI integration solutions: From process automation, intelligent risk prediction, to continuous security testing in CI/CD, and rapid response to potential threats. \u0026ldquo;Dissect\u0026rdquo; Case Studies from CMC Global and AWS on how leading businesses apply AI in addressing security and system operations issues. Provide orientation for DevSecOps Engineers in a highly demanding market, emphasizing continuous learning and specialization. Speaker List Mr. Le Thanh Duc – Cloud Delivery Manager, CMC Global Mr. Du Quoc Thanh – Technical Leader, CMC Global Special Guest: Mr. Van Hoang Kha – Cloud Engineer, AWS Community Builder Key Highlights Comprehensive AI Integration Solution in DevSecOps Process automation, intelligent risk prediction. Continuous security testing in the CI/CD pipeline. Rapid response to potential threats. Case Studies and Practical Lessons Directly \u0026ldquo;dissected\u0026rdquo; projects from CMC Global and AWS to learn how businesses apply AI in addressing security and system operations issues. Explored how AI helps eliminate rigid \u0026ldquo;gatekeepers,\u0026rdquo; 24/7 on-call shifts, and costly manual response procedures in DevSecOps. Career Orientation and Professional Development Discussed that continuously updating trends and enhancing specialization are the key to engineers conquering complex projects. Key Takeaways AI Integration and Automation Clearly understood the Comprehensive AI Integration Solution in DevSecOps, including process automation and continuous security testing in CI/CD. Grasped how AI can enable intelligent risk prediction and accelerate threat response. Modern DevSecOps Mindset Recognized the importance of integrating security into the SDLC (Software Development Life Cycle). Understood how popular tools like Jenkins (CI/CD), SonarQube (SAST), OWASP ZAP (DAST), and Terraform (IaC) operate. Generative AI Tools Explored Amazon Q Developer – a powerful generative AI assistant supporting code generation, testing, vulnerability scanning, and software development optimization on AWS. Application to Work (From Worklog Week 6) Apply the DevSecOps mindset to the current project, integrating security from the early development stages. Research Amazon Q Developer for integration into the workflow to support code generation and security testing. Investigate CI/CD tools and SAST/DAST security testing for inclusion in the project\u0026rsquo;s development roadmap. Event Experience The workshop \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; provided a strategic and practical, multi-faceted view of the future of Cloud security and system operations:\nPerspectives from Experts Listened to strategic perspectives from leading experts like Mr. Le Thanh Duc, Mr. Du Quoc Thanh, and special guest Mr. Van Hoang Kha. Through Case Studies, I understood how large enterprises apply Generative AI to solve complex security and operational challenges. Value of AI in DevSecOps Clearly saw AI\u0026rsquo;s potential to eliminate manual bottlenecks (like rigid gatekeepers, 24/7 shifts), transforming DevSecOps into a more automated and intelligent process. The event provided specific solutions for comprehensive AI integration, from process automation to risk prediction. Professional Orientation Emphasized the importance of continuously updating trends and enhancing specialization for DevSecOps engineers to conquer complex future projects. Had the opportunity for direct Q\u0026amp;A with the speakers to address questions about integrating AI into my own project. Conclusion The event was an invaluable source of information, helping me understand not only the technology but also career direction, especially how to use powerful tools like Amazon Q Developer to boost productivity and integrate security into the development workflow.\nEvent Photos Overall, the event successfully explored the strategic and practical perspectives of DevSecOps with Generative AI, providing inspiration and clear direction for young engineers.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives Complete Module 2, mastering Amazon EC2 and VPC fundamentals. Prepare and configure essential resources for launching EC2 instances. Explore Amazon Route 53 and DNS management concepts. Participate in Cloud Day event for AI and Data insights. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Study VPC architecture and networking components in depth.\n- Learn AWS architecture design principles from Mentor Gia Hung\u0026rsquo;s lectures.\n- Complete AWS Skill Builder: Networking Essentials with Amazon VPC. 15/09/2025 16/09/2025 AWS VPC Documentation 3 - Create VPC resources to prepare for EC2 instance deployment.\n- Launch EC2 instances using configured resources.\n- Study Security Groups and Network ACLs.\n- Complete: Compute Essentials with Amazon EC2. 16/09/2025 17/09/2025 AWS EC2 Documentation Introduction to Amazon EC2 Deploying FCJ Management Application with Auto Scaling Group 4 - Resolve AWS account authentication issues by submitting verification documents.\n- Complete: Creating Your First AWS Account and Getting Help with AWS Support. 17/09/2025 20/09/2025 AWS Support Request Support with AWS Support 5 - Attend Cloud Day event.\n- Gain insights into AI and Data trends.\n- Network with prominent mentors in the AWS community. 18/09/2025 18/09/2025 Cloud Day Event AWS Skill Builder Courses Completed Course Category Status Creating Your First AWS Account Getting Started ✅ Managing Costs with AWS Budgets Cost Management ✅ Getting Help with AWS Support Support ✅ Access Management with AWS IAM Security ✅ Networking Essentials with Amazon VPC Networking ✅ Compute Essentials with Amazon EC2 Compute ✅ Instance Profiling with IAM Roles for EC2 Security ✅ Week 2 Achievements Technical Skills Acquired:\nGained comprehensive understanding of VPC architecture and EC2 fundamentals Mastered the process of preparing resources for EC2 instance deployment: Created and configured Subnets for network segmentation Set up Internet Gateway for external connectivity Configured Route Tables to manage traffic routing Implemented Security Groups to control inbound/outbound traffic Understood IAM roles and instance profiles for secure EC2 access Learned AWS cost management strategies using AWS Budgets Cloud Day Event Highlights:\nParticipated in networking sessions with AWS mentors and industry professionals Gained valuable insights into AI and Data market trends Understood the future potential and market demand for AI technologies Received commemorative gift from event organizers Key Takeaways:\nVPC is the foundation for all AWS networking - understanding it is critical Security Groups act as virtual firewalls at the instance level IAM roles eliminate the need for hardcoded credentials in EC2 instances AWS Budgets help prevent unexpected costs through proactive monitoring "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-network/5.3.2-alb/","title":"Application Load Balancer (ALB)","tags":[],"description":"","content":"The Application Load Balancer (ALB) serves as the entry point for all incoming traffic to our platform. It distributes requests to the appropriate containers (Frontend or Backend) and handles SSL termination.\n1. Create Security Group for ALB Before creating the load balancer, we need a firewall rule that allows public access.\nGo to EC2 Dashboard \u0026gt; Security Groups \u0026gt; Create security group. Security group name: alb-sg. Description: Allow http and https traffic. VPC: Select band-up-vpc. Inbound rules: Add the following rules to allow traffic from anywhere: Type: HTTP | Port: 80 | Source: Anywhere-IPv4 (0.0.0.0/0). Type: HTTPS | Port: 443 | Source: Anywhere-IPv4 (0.0.0.0/0). Click Create security group. 2. Create Target Group The ALB needs to know where to route the traffic. We will create a Target Group for our Frontend service first.\nGo to EC2 Dashboard \u0026gt; Target groups \u0026gt; Create target group. Choose a target type: Select IP addresses (Required for ECS Fargate). Target group name: target-bandup-fe. Protocol: HTTP. Port: 3000 (Our Next.js frontend runs on port 3000). VPC: Select band-up-vpc. Click Next. Register targets: Since we haven\u0026rsquo;t deployed the ECS tasks yet, skip this step and click Create target group. 3. Create Application Load Balancer Now, we aggregate everything into the Load Balancer.\nStep 1: Basic Configuration\nGo to Load Balancers \u0026gt; Create load balancer. Select Application Load Balancer and click Create. Load balancer name: bandup-public-alb. Scheme: Internet-facing (To allow public access). IP address type: IPv4. Step 2: Network Mapping\nVPC: Select band-up-vpc. Mappings: Select two Availability Zones (ap-southeast-1a and ap-southeast-1b). Subnets: IMPORTANT - Select the Public Subnets (public-subnet-1 and public-subnet-2) created in the previous section. Note: Ensure you do not select Private subnets, otherwise the ALB cannot be reached from the internet. Step 3: Security Groups \u0026amp; Listeners\nSecurity groups: Deselect the default one and choose alb-sg. Listeners and routing: Protocol: HTTP | Port: 80. Default action: Forward to target-bandup-fe. Click Create load balancer. Your ALB is now provisioning. Once active, it will be ready to route traffic to your frontend application.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-setup-be/5.5.2-rds/","title":"Create PostgreSQL RDS","tags":[],"description":"","content":"In this step, we provision an Amazon RDS for PostgreSQL instance. This will serve as the primary persistent data store for the IELTS BandUp platform. We will configure it for high availability and security within our VPC.\n1. Configure Security Groups Before creating the database, we need to define the firewall rules.\nStep 1.1: Create Backend Security Group This group is for the ECS Fargate tasks (the application layer) to control outbound traffic.\nName: ecs-backend-sg. Inbound: Allow port 8080 (Spring Boot default) from the ALB. Step 1.2: Create RDS Security Group This group is attached to the database itself.\nName: rds-sg. Inbound: Allow PostgreSQL traffic (Port 5432) only from the ecs-backend-sg created above (or the VPC CIDR for testing). This ensures only our application can talk to the database. 2. Create DB Subnet Group RDS needs to know which subnets it is allowed to use. We will group our private database subnets together.\nNavigate to Amazon RDS \u0026gt; Subnet groups \u0026gt; Create DB subnet group. Name: bandup-db-subnet-group. VPC: Select band-up-vpc. Add subnets: Select the Availability Zones and choose the dedicated private-database-subnet-1 and private-database-subnet-2. 3. Create the Database Now, we provision the PostgreSQL instance.\nNavigate to Databases \u0026gt; Create database. Choose a database creation method: Standard create. Engine options: PostgreSQL (Version 17.6 or latest). Availability and durability: Select Multi-AZ DB instance. This creates a primary DB and a synchronous standby replica in a different Availability Zone for automatic failover. Settings: DB instance identifier: bandup-db. Master username: postgres. Credential management: Self managed. Master password: Set a strong password (save this for later). Instance configuration: DB instance class: Burstable classes -\u0026gt; db.t4g.micro (Cost-effective for workshops/dev). Storage: gp3 (General Purpose SSD) with 20 GiB. Connectivity: Compute resource: Don\u0026rsquo;t connect to an EC2 compute resource. VPC: band-up-vpc. DB subnet group: bandup-db-subnet-group (Created in step 2). Public access: No (Critical for security). VPC security group: Select existing -\u0026gt; rds-sg. Database authentication: Password authentication. Monitoring: Enable Performance Insights (retention 7 days). Additional configuration: Initial database name: band_up (Important: Hibernate will look for this DB name). Backup: Enable automated backups. Encryption: Enable encryption. Click Create database. The provisioning process will take a few minutes. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"This workshop is designed for DevOps Engineers, Cloud Architects, and Full-stack Developers who aim to deploy a modern, AI-integrated application on AWS.\nTo successfully complete this workshop, participants are expected to possess the following knowledge, skills, and tools.\n1. Technical Knowledge Requirements AWS Fundamentals Console Navigation: Familiarity with the AWS Management Console. Core Services: Basic understanding of compute services like Amazon EC2 and AWS Fargate, networking concepts within Amazon VPC, and storage using Amazon S3. IAM \u0026amp; Security: Understanding of AWS Identity and Access Management (IAM), specifically Roles, Policies, and the principle of least privilege. Containerization \u0026amp; Orchestration Docker: Proficiency in creating Dockerfiles, building images, and running containers locally. Understanding of concepts like layers, exposing ports, and environment variables is essential. Refer to the Docker Documentation. ECS Concepts: Familiarity with Amazon ECS terminology including Task Definitions, Services, Clusters, and the operational differences between EC2 and Fargate launch types. DevOps \u0026amp; CI/CD Git: Proficiency in version control (commit, push, branching) to manage source code and trigger automated pipelines. CI/CD Flow: Understanding of Continuous Integration and Continuous Delivery principles using tools like AWS CodePipeline and AWS CodeBuild. Networking Basics Protocols: Understanding of HTTP/HTTPS, DNS resolution with Amazon Route 53, and load balancing concepts using Application Load Balancer. Network Security: Knowledge of IP addressing (CIDR blocks) and traffic control using Security Groups. 2. Environment Setup Before starting the workshop, ensure your local development environment is equipped with the following tools:\nAWS Account: An active AWS account with Administrator access to provision resources. IDE: A code editor such as Visual Studio Code or IntelliJ IDEA. Command Line Tools: AWS CLI (v2): Installed and configured with your account credentials. Installation Guide. Git: Installed for cloning repositories. Downloads. Docker Desktop: Running locally to inspect or build images if necessary. Get Docker. 3. Service Quotas \u0026amp; Costs Cost Alert: This workshop utilizes resources that are not covered by the AWS Free Tier, including:\nNAT Gateways (Hourly charge + Data processing fees) Application Load Balancers ECS Fargate Tasks (vCPU/Memory usage) Amazon RDS \u0026amp; ElastiCache Please ensure you clean up resources immediately after finishing the workshop to avoid unexpected charges. A cleanup guide is provided at the end of this documentation.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup-fe/5.4.2-ecr/","title":"Setup ECR &amp; IAM Role","tags":[],"description":"","content":"In this step, we prepare the AWS infrastructure required to store our container images. This involves verifying the initial state, creating a necessary IAM Role for ECR replication, and provisioning the repository.\n1. Verify ECR State First, verify the current state of the Private Registry. Initially, there are no repositories created.\n2. Create IAM Role for ECR We need to create a Service-Linked Role that allows Amazon ECR to perform replication actions across regions and accounts.\nNavigate to IAM \u0026gt; Roles \u0026gt; Create role. Select trusted entity: Choose AWS service. Service or use case: Select Elastic Container Registry from the list. Use case: Select Elastic Container Registry - Replication to allow ECR to replicate images. Add permissions: Confirm that the ECRReplicationServiceRolePolicy is attached. This managed policy grants the necessary permissions. Name, review, and create: The role name is automatically set to AWSServiceRoleForECRReplication. Review the configuration and create the role. Result: The role is successfully created and listed in the IAM Roles dashboard. 3. Create ECR Repository Now we create the repository to store the frontend image.\nNavigate to Amazon ECR \u0026gt; Create repository. General settings: Repository name: band-up-frontend. Visibility settings: Private. Image tag settings: Keep Mutable enabled to allow overwriting image tags. Result: The band-up-frontend repository is successfully created with AES-256 encryption enabled by default. 4. Configure CLI Access To push images from your local machine, you need programmatic access via the AWS CLI. We will generate an Access Key for your IAM User.\nNavigate to IAM Dashboard \u0026gt; Users \u0026gt; Select your user (e.g., NamDang). Open the Security credentials tab and click Create access key. Use case: Select Command Line Interface (CLI). Description tag: Enter a meaningful description (e.g., ECR Push Key) and click Create access key. Retrieve Keys: Important! Copy or download the Access Key ID and Secret Access Key immediately, as you cannot retrieve the Secret Key later. 5. Configure AWS CLI Open your terminal and configure the AWS CLI with the credentials you just generated.\naws configure Enter the following details when prompted:\nAWS Access Key ID: [Paste your key] AWS Secret Access Key: [Paste your secret] Default region name: ap-southeast-1 Default output format: json 6. Push Image to ECR Now that the CLI is configured, we can authenticate Docker and push our image.\nStep 1: Login to ECR Run the login command to authenticate your Docker client with the AWS registry.\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin [Account-ID]https://www.google.com/search?q=.dkr.ecr.ap-southeast-1.amazonaws.com Output: Login Succeeded\nStep 2: Tag the Image We need to tag our local image band-up-frontend:latest with the full ECR repository URI and a version tag (e.g., v1.0.0).\ndocker tag band-up-frontend:latest [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:v1.0.0 Step 3: Push the Image Execute the push command to upload the layers to AWS.\ndocker push [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:v1.0.0 7. Final Verification Return to the Amazon ECR Console and open the band-up-frontend repository. You should see the image with the tag v1.0.0 listed successfully.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-cicd-pipeline/5.7.2-test-gwe/","title":"Create CodePipeline with GitLab tag trigger","tags":[],"description":"","content":"CodePipeline Design (Source -\u0026gt; Build → Deploy) Build (CodeBuild) Project: the project’s CodeBuild project (Source = CodePipeline) Environment variables: as needed for this project’s build Buildspec: use the repository’s existing buildspec.yml CodePipeline Set up guidelines On choosing pipeline creation option section, choose Build custom pipeline. In pipeline settings tab, specify the pipeline name and use default settings for the pipeline. Turn on webhook events and add filter type of tags. Make sure to set the tag patterns of \u0026ldquo;v*\u0026rdquo;. Add frontend/backend project using AWS CodeBuild in build stage. For the deploy stage, choose Amazon ECS as deploy provider. Specify its cluster and service to deploy. Also make sure to fill in the image definition file form like the image. Submit and then the pipeline is now created. "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"IELTS Self-Learning Web System An Intelligent Platform for Independent IELTS Preparation 1. Executive Summary The IELTS Self-Learning Web System is a comprehensive online platform designed to support students in their independent IELTS preparation journey. The platform provides an all-in-one solution featuring user management, educational blogs, interactive study rooms, practice tests, and AI-powered flashcard systems. Leveraging modern web technologies and AI integration, the system offers personalized learning experiences, real-time collaboration features, and automated assessment tools to help students achieve their IELTS goals efficiently.\n2. Problem Statement What\u0026rsquo;s the Problem? Many IELTS learners face challenges in finding affordable, comprehensive, and interactive self-study platforms. Traditional learning methods lack real-time collaboration, personalized feedback, and integrated practice tools. Students often struggle with:\nLimited access to quality practice materials and mock tests Lack of immediate feedback on Speaking and Writing tasks Difficulty finding study partners and maintaining study discipline Fragmented resources across multiple platforms High costs of traditional IELTS preparation courses The Solution The IELTS Self-Learning Web System provides a unified platform with five core features:\nUser Management System: Multi-tier membership (Guest, Member, Premium, Admin) with social authentication (Google, Facebook), personal profiles, and messaging capabilities.\nEducational Blog Platform: Community-driven content with CRUD operations, genre/tag categorization, advanced filtering, commenting, reporting, and favoriting features.\nInteractive Study Rooms: Virtual study spaces with scheduling, voice/video calls, Pomodoro timers, background music, dictionary integration, and translation support for enhanced learning.\nIELTS Practice Tests: Comprehensive mock tests for Reading and Listening with automatic vocabulary extraction into flashcards, plus AI-powered assessment for Speaking and Writing tasks, and dictation exercises.\nQuizlet Flashcard System: Intelligent flashcard creation with multiple study modes, automatic vocabulary extraction from texts, AI-generated quizzes from uploaded materials, and sharing capabilities.\nBenefits and Return on Investment For Students: Cost-effective alternative to expensive courses, personalized learning paths, 24/7 access to study materials, AI-powered feedback, and collaborative learning environment. Educational Value: Develops self-discipline, provides comprehensive IELTS preparation, enables peer learning, and offers trackable progress. Technical Benefits: Scalable architecture, modern tech stack, AI integration for automated assessment, and potential for future feature expansion. Market Potential: Growing demand for online IELTS preparation, subscription-based revenue model (Guest → Member → Premium), and potential for partnerships with educational institutions. 3. Solution Architecture The platform employs a modern full-stack web architecture designed for scalability, real-time collaboration, and AI integration. The system consists of five major modules working together to provide a comprehensive IELTS learning experience. The infrastructure uses an active-passive Multi-AZ deployment on AWS ECS for high availability, where AZ-1 handles all active traffic and AZ-2 serves as a standby for automatic failover.\nSystem Architecture Overview: 3.1. Architecture Overview The IELTS Self-Learning Web System is built on AWS cloud infrastructure with a multi-layered architecture that ensures high availability, scalability, and security. The system follows a microservices-inspired approach with a monolithic backend for core services and serverless architecture for AI-powered features.\nKey Architecture Principles:\nHigh Availability: Active-passive Multi-AZ deployment with automatic failover Scalability: Horizontal scaling with ECS Auto Scaling and serverless Lambda functions Security: Multi-layer security with WAF, VPC isolation, and encryption at rest and in transit Performance: CDN distribution, caching layers, and optimized database queries Cost Efficiency: Serverless AI services, active-passive standby resources, and pay-per-use pricing 3.2. Network Architecture The network infrastructure is built within a Virtual Private Cloud (VPC) spanning two Availability Zones (AZ-1 and AZ-2) for redundancy and high availability.\nVPC Structure:\nPublic Subnets (AZ-1 \u0026amp; AZ-2): Application Load Balancer (ALB) endpoints NAT Gateways for outbound internet access Bastion hosts for secure access (optional) Private Subnets (AZ-1 \u0026amp; AZ-2): ECS Fargate tasks (Next.js frontend and Spring Boot backend) Amazon RDS PostgreSQL instances Amazon ElastiCache Redis clusters Internal service communication Network Components:\nRoute 53: DNS management and domain routing AWS Certificate Manager (ACM): SSL/TLS certificates for HTTPS AWS WAF: Web Application Firewall protecting against common web exploits Internet Gateway: Public internet access for public subnets NAT Gateways: Outbound internet access for private subnets Security Groups: Stateful firewall rules controlling traffic between components Network ACLs: Additional layer of subnet-level security Traffic Flow:\nUser requests → Route 53 → CloudFront CDN (for static assets) API requests → Route 53 → AWS WAF → Application Load Balancer ALB routes to ECS tasks in private subnets (AZ-1 active, AZ-2 standby) ECS tasks communicate with RDS, ElastiCache, and S3 via private subnets AI service requests → API Gateway → SQS → Lambda functions 3.3. Application Architecture The application layer consists of frontend and backend services deployed on Amazon ECS (Fargate) with an active-passive Multi-AZ configuration.\nFrontend Layer (Next.js):\nDeployment: Containerized Next.js application on ECS Fargate Location: Private subnets in AZ-1 (active) and AZ-2 (standby) Features: Server-side rendering (SSR) for SEO optimization Static asset delivery via CloudFront CDN Real-time WebRTC for study room video/voice calls Socket.io client for real-time messaging Responsive design for mobile and desktop Backend Layer (Spring Boot):\nDeployment: Monolithic Spring Boot REST API on ECS Fargate Location: Private subnets in AZ-1 (active) and AZ-2 (standby) Components: RESTful API endpoints for all modules Spring Security for authentication and authorization Spring WebSocket for real-time features JWT token management OAuth 2.0 integration (Google, Facebook) File upload handling for S3 integration Load Balancing \u0026amp; High Availability:\nApplication Load Balancer (ALB): Routes traffic to healthy ECS tasks in AZ-1 (active) Health checks every 30 seconds Automatic failover to AZ-2 if AZ-1 becomes unavailable SSL/TLS termination with ACM certificates Sticky sessions for WebSocket connections Container Orchestration:\nAmazon ECS (Fargate): No server management required Auto Scaling based on CPU/memory utilization Task definitions for Next.js and Spring Boot containers Service discovery and load balancing integration Container images stored in Amazon ECR 3.4. Data Architecture The data layer uses a combination of relational and NoSQL databases, along with caching for optimal performance.\nPrimary Database:\nAmazon RDS PostgreSQL (Multi-AZ): Primary Instance: db.t3.medium in AZ-1 (active) Standby Instance: db.t3.medium in AZ-2 (passive, synchronous replication) Data: Users, blogs, study sessions, practice tests, flashcards metadata Backup: Automated daily backups with 7-day retention High Availability: Automatic failover to standby in \u0026lt; 60 seconds Encryption: At rest (AES-256) and in transit (SSL/TLS) Caching Layer:\nAmazon ElastiCache (Redis): Session management and user authentication tokens Frequently accessed data caching (blog posts, user profiles) Real-time leaderboards and statistics Rate limiting and API throttling Cache invalidation strategies for data consistency Object Storage:\nAmazon S3: User-uploaded files (images, documents, audio recordings) Static assets (before CloudFront distribution) Backup storage for RDS snapshots Lifecycle policies for cost optimization (move to Glacier after 90 days) Versioning enabled for critical files NoSQL Database (AI Services):\nAmazon DynamoDB: AI assessment results (Writing and Speaking scores) Generated flashcards from RAG pipeline User progress tracking for AI features On-demand scaling for variable workloads Point-in-time recovery enabled Vector Store (RAG):\nAmazon OpenSearch Service (or Amazon Bedrock Knowledge Base): Document embeddings generated by Amazon Titan V2 Semantic search for relevant document chunks Vector similarity search for flashcard generation Index management and query optimization 3.5. Security Architecture Multi-layer security ensures data protection and system integrity at every level.\nNetwork Security:\nAWS WAF: Protection against SQL injection, XSS, DDoS attacks Security Groups: Restrictive firewall rules (least privilege principle) Network ACLs: Subnet-level traffic filtering VPC Isolation: Private subnets with no direct internet access VPN/Bastion Hosts: Secure administrative access (optional) Data Security:\nEncryption at Rest: RDS: AES-256 encryption S3: Server-side encryption (SSE-S3) DynamoDB: Encryption at rest enabled ElastiCache: Encryption in transit and at rest Encryption in Transit: HTTPS/TLS 1.2+ for all external communications SSL/TLS for database connections ACM certificates for all domains Identity \u0026amp; Access Management:\nAWS IAM: Role-based access control for AWS services ECS tasks use IAM roles (no hardcoded credentials) Lambda functions have minimal required permissions S3 bucket policies for access control Spring Security: Application-level authentication JWT tokens for stateless authentication OAuth 2.0 for social login (Google, Facebook) Role-based access control (Guest, Member, Premium, Admin) AWS Secrets Manager: Secure storage of API keys and credentials Database passwords Third-party API keys (Gemini, dictionary APIs) OAuth client secrets Application Security:\nInput Validation: All user inputs validated and sanitized SQL Injection Prevention: Parameterized queries with Spring Data JPA XSS Protection: Content Security Policy (CSP) headers CSRF Protection: Spring Security CSRF tokens Rate Limiting: API throttling with Redis Audit Logging: CloudWatch Logs for security events 3.6. CI/CD Pipeline Automated deployment pipeline ensures consistent and reliable releases.\nSource Control:\nGit Repository: Code versioning and collaboration Branch Strategy: Main branch for production, feature branches for development CI/CD Components:\nGitLab Webhook (or GitHub Actions): Triggers pipeline on code push AWS CodePipeline: Orchestrates the deployment workflow AWS CodeBuild: Builds and tests application code Frontend: npm install \u0026amp;\u0026amp; npm run build Backend: mvn clean package Docker image creation and tagging Amazon ECR: Container registry for Docker images Stores Next.js and Spring Boot container images Image versioning and lifecycle policies Deployment Flow:\nDeveloper pushes code to repository Webhook triggers CodePipeline CodeBuild compiles and tests code CodeBuild creates Docker images and pushes to ECR ECS service updates with new task definitions Rolling deployment: New tasks start, health checks pass, old tasks terminate CloudWatch monitors deployment success/failure Infrastructure as Code:\nAWS CloudFormation (or Terraform): Infrastructure provisioning VPC, subnets, security groups ECS clusters, services, task definitions RDS instances, ElastiCache clusters Lambda functions, API Gateway, SQS queues IAM roles and policies 3.7. Monitoring \u0026amp; Observability Comprehensive monitoring ensures system health and performance.\nApplication Monitoring:\nAmazon CloudWatch: Metrics: CPU, memory, network utilization for ECS tasks Logs: Application logs from Next.js and Spring Boot Alarms: Automated alerts for errors, high latency, resource exhaustion Dashboards: Custom dashboards for key metrics AWS X-Ray (optional): Distributed tracing for request flow analysis Infrastructure Monitoring:\nCloudWatch Metrics: RDS: CPU, memory, connection count, read/write latency ElastiCache: CPU, memory, cache hit ratio ALB: Request count, response time, error rates Lambda: Invocations, duration, errors, throttles SQS: Queue depth, message age, DLQ size API Gateway: Request count, latency, 4xx/5xx errors Logging:\nCloudWatch Logs: Application logs (structured JSON format) Access logs from ALB API Gateway execution logs Lambda function logs Log retention: 30 days (configurable) Alerting:\nCloudWatch Alarms: High error rate (\u0026gt; 5% 5xx errors) High latency (\u0026gt; 2 seconds p95) Low availability (\u0026lt; 99% uptime) Resource exhaustion (CPU \u0026gt; 80%, Memory \u0026gt; 85%) Database connection pool exhaustion SQS queue depth exceeding threshold SNS Notifications: Email/SMS alerts for critical alarms Performance Optimization:\nCloudFront CDN: Caching static assets at edge locations ElastiCache: Reducing database load with intelligent caching Database Query Optimization: Indexed queries, connection pooling Auto Scaling: ECS tasks scale based on CPU/memory metrics Technology Stack Frontend:\nNext.js 14+: Modern React framework for responsive web application TypeScript: Type-safe development TailwindCSS: Utility-first styling WebRTC: Real-time video/voice communication Socket.io Client: Real-time messaging and collaboration Backend:\nSpring Boot 3.x: Monolithic RESTful API architecture Java 17+: Backend programming language Spring Security: Authentication and authorization Spring WebSocket: Real-time communication JWT: Secure token-based authentication OAuth 2.0: Social login integration (Google, Facebook) Database:\nPostgreSQL 14+: Primary relational database for all data (users, tests, blogs, flashcards, study sessions) Amazon ElastiCache (Redis): Caching and session management Cloud Infrastructure (AWS):\nAmazon ECS (Elastic Container Service): Container orchestration with active-passive Multi-AZ deployment Application Load Balancer: Routes traffic to active AZ with automatic failover Amazon RDS for PostgreSQL: Multi-AZ deployment (active primary in AZ-1, passive standby in AZ-2) Amazon S3: Media and file storage Amazon CloudFront: CDN for static assets Amazon CloudWatch: Monitoring and logging Amazon API Gateway: RESTful API endpoint for AI service requests Amazon SQS: Message queue for asynchronous AI processing with Dead Letter Queue (DLQ) AWS Lambda: Serverless functions for AI-powered assessments and flashcard generation Amazon DynamoDB: NoSQL database for storing AI assessment results and generated flashcards Amazon Bedrock: AI model service for embeddings (Titan V2) and content generation Amazon OpenSearch Service (optional): Vector store for RAG-based document embeddings and semantic search Third-party Services:\nGoogle Gemini Flash API (Free Tier): AI-powered Speaking/Writing assessment and smart query generation for RAG Amazon Bedrock (GPT-OSS): Alternative AI model for assessments and content generation Amazon Titan V2 Embeddings: Vector embeddings for document chunks in RAG-based flashcard generation Free Dictionary APIs: Word definitions and examples Open-source translation libraries: Context-aware translation (alternative to paid APIs) 3.9. Component Design The system consists of five major functional modules, each integrated with the core architecture described above. Each module leverages the shared infrastructure (ECS, RDS, ElastiCache, S3) while maintaining clear separation of concerns.\n1. User Management Module:\nMulti-tier authentication system (Guest, Member, Premium, Admin) Social OAuth integration (Google, Facebook) User profile management with learning statistics Real-time messaging system Password recovery and email verification 2. Blog Platform Module:\nCRUD operations for blog posts Genre and tag management system Advanced search and filtering (by tags, genres, date, popularity) Comment system with nested replies Content reporting and moderation Favorite/bookmark functionality SEO-optimized content delivery 3. Study Room Module:\nVirtual study room creation and management Study session scheduling with calendar integration WebRTC-based voice and video calls Pomodoro timer with customizable intervals Background music library with focus playlists Integrated dictionary with free dictionary APIs Translation support using open-source libraries Screen sharing for collaborative study Real-time participant management 4. IELTS Practice Test Module:\nTest bank management (Reading, Listening, Speaking, Writing) - stored in PostgreSQL Timed test simulations with auto-submission Automatic vocabulary extraction from Reading passages to flashcards AI-powered Speaking assessment (see Section 3.8, Lambda Function 2): Audio recordings uploaded to S3 Processed asynchronously via API Gateway → SQS → Lambda Function 2 Uses Amazon Transcribe for speech-to-text Evaluates pronunciation, fluency, coherence, lexical resource Results stored in DynamoDB and displayed to user AI-powered Writing assessment (see Section 3.8, Lambda Function 1): Writing samples submitted via API Gateway → SQS → Lambda Function 1 Evaluates grammar, vocabulary, task achievement, coherence Results stored in DynamoDB with detailed feedback Dictation exercises for Listening practice Detailed score reports and analytics (aggregated from DynamoDB) Progress tracking across test types (stored in PostgreSQL, cached in Redis) 5. Quizlet Flashcard Module:\nCRUD operations for flashcard sets (stored in PostgreSQL) Multiple study modes (flashcards, learn, test, match, write) Spaced repetition algorithm (SRS) for optimized learning Automatic vocabulary extraction from text passages AI-generated flashcards from uploaded documents using RAG (see Section 3.8) Documents uploaded to S3 Processed asynchronously via API Gateway → SQS → Lambda Function 3 Generated flashcards stored in DynamoDB and linked to user\u0026rsquo;s sets Collaborative flashcard sets with sharing Study statistics and mastery tracking (cached in Redis) Import/export functionality 3.8. AI Service Architecture (Serverless) The AI service is implemented as a fully serverless architecture using AWS services to handle AI-powered assessments and content generation. This architecture follows an asynchronous processing pattern for scalability, cost-efficiency, and reliability.\nArchitecture Components:\n1. API Gateway (Entry Point)\nPurpose: RESTful API endpoint for AI service requests Configuration: REST API with custom domain (optional) API keys for rate limiting and access control CORS configuration for frontend integration Request/response transformation Integration with SQS for asynchronous processing Endpoints: POST /ai/writing-assessment - Submit writing samples POST /ai/speaking-assessment - Submit audio recordings POST /ai/generate-flashcards - Upload documents for flashcard generation Security: IAM authentication or API keys 2. Amazon SQS (Message Queue)\nPurpose: Decouples API Gateway from Lambda functions for asynchronous processing Configuration: Standard queue for high throughput Message retention: 14 days Visibility timeout: 5 minutes (adjustable per function) Dead Letter Queue (DLQ) for failed messages after 3 retries Message Format: JSON payloads containing: User ID and request metadata File references (S3 URIs for documents/audio) Processing parameters Benefits: Handles traffic spikes without overwhelming Lambda Automatic retry mechanism for transient failures Cost-effective message queuing 3. AWS Lambda Functions (Processing Layer) Three specialized Lambda functions process different types of AI tasks, each optimized for its specific workload:\nLambda Function 1: Writing Assessment\nTrigger: SQS queue messages for writing assessments Runtime: Python 3.11 or Node.js 18.x Memory: 512 MB - 1 GB (configurable) Timeout: 5 minutes Processing Flow: Receives writing sample from SQS message Retrieves document from S3 if needed Calls Amazon Bedrock (GPT-OSS) or Google Gemini Flash API with prompt: Task description and IELTS band descriptors Writing sample text Evaluation criteria (grammar, vocabulary, task achievement, coherence) Parses AI response for structured assessment: Overall band score (0-9) Detailed scores per criterion Feedback comments and suggestions Stores results in DynamoDB with: User ID, timestamp, writing sample reference Assessment scores and feedback Processing metadata Sends notification (optional) via SNS or updates user via WebSocket Lambda Function 2: Speaking Assessment\nTrigger: SQS queue messages for speaking assessments Runtime: Python 3.11 or Node.js 18.x Memory: 1 GB - 2 GB (for audio processing) Timeout: 15 minutes (for longer audio files) Processing Flow: Receives audio file reference from SQS message Retrieves audio file from S3 Audio Transcription (if needed): Uses Amazon Transcribe for speech-to-text conversion Supports multiple languages and accents Generates transcript with timestamps Calls Amazon Bedrock (GPT-OSS) or Google Gemini Flash API with: Transcript text Audio metadata (duration, sample rate) IELTS speaking band descriptors Evaluation criteria (pronunciation, fluency, coherence, lexical resource) Parses AI response for structured assessment: Overall band score (0-9) Detailed scores per criterion Pronunciation analysis Fluency and coherence feedback Stores results in DynamoDB with: User ID, timestamp, audio file reference Transcript and assessment scores Detailed feedback Sends notification or updates user status Lambda Function 3: Flashcard Generation (RAG-based)\nTrigger: SQS queue messages for document uploads\nRuntime: Python 3.11 (for ML/AI libraries)\nMemory: 2 GB - 3 GB (for document processing)\nTimeout: 15 minutes (for large documents)\nProcessing Flow:\nStep 1: Document Processing \u0026amp; Embedding\nReceives document file reference from SQS Retrieves document from S3 (PDF, DOCX, TXT formats) Document Chunking: Splits document into semantic chunks (500-1000 tokens each) Preserves context and structure Handles tables, lists, and formatted content Vector Embedding Generation: Uses Amazon Titan V2 Embeddings model via Bedrock Generates 1024-dimensional vectors for each chunk Maintains chunk metadata (position, section, page number) Vector Storage: Stores embeddings in Amazon OpenSearch Service (vector index) Alternative: Amazon Bedrock Knowledge Base (managed service) Index structure: Document ID, chunk ID, embedding vector, metadata Step 2: Smart Query Generation\nUses Google Gemini Flash API to analyze document content Query Generation Process: Analyzes document structure and content Identifies key concepts, important facts, learning points Generates 10-20 intelligent, context-aware queries Queries designed to extract: Definitions and explanations Important dates, facts, and figures Concepts and relationships Examples and use cases Query Optimization: Ensures queries are specific and answerable Covers different difficulty levels Balances factual and conceptual questions Step 3: RAG-based Flashcard Generation\nRetrieval Phase: For each generated query, performs semantic search in vector store Retrieves top 3-5 most relevant document chunks Uses cosine similarity for vector matching Augmentation Phase: Combines query with retrieved chunks Adds context and metadata Formats prompt for flashcard generation Generation Phase: Sends query-context pairs to Google Gemini Flash or Amazon Bedrock Prompt includes: Query/question to answer Relevant document chunks Flashcard format requirements (question-answer pairs) Educational guidelines Model generates flashcards that are: Relevant to document content Contextually accurate Educationally valuable Properly formatted (question-answer pairs) Appropriate difficulty level Post-processing: Validates flashcard quality Removes duplicates Formats for storage Storage: Stores generated flashcards in DynamoDB Associates with user\u0026rsquo;s flashcard sets Links to original document Stores metadata (generation date, source chunks) 4. Amazon DynamoDB (Data Storage)\nPurpose: Stores AI assessment results and generated flashcards Table Design: WritingAssessments Table: Partition Key: userId (String) Sort Key: timestamp (Number) Attributes: writingSample, scores, feedback, metadata SpeakingAssessments Table: Partition Key: userId (String) Sort Key: timestamp (Number) Attributes: audioFile, transcript, scores, feedback, metadata GeneratedFlashcards Table: Partition Key: userId (String) Sort Key: flashcardId (String) Attributes: question, answer, sourceDocument, sourceChunks, metadata Features: On-demand scaling for variable workloads Point-in-time recovery enabled Encryption at rest TTL for automatic cleanup of old data 5. Amazon Bedrock (AI Model Service)\nModels Used: Amazon Titan V2 Embeddings: Vector embeddings for document chunks GPT-OSS (Open Source): Alternative model for assessments and generation Integration: Direct API calls from Lambda functions Cost: Pay-per-use pricing model 6. Amazon OpenSearch Service (Vector Store)\nPurpose: Semantic search and retrieval for RAG pipeline Configuration: Vector index for document embeddings KNN (k-nearest neighbors) search capability Index management and optimization Alternative: Amazon Bedrock Knowledge Base (fully managed) 7. Amazon CloudWatch (Monitoring)\nLambda Metrics: Invocations, duration, errors, throttles Memory utilization, concurrent executions SQS Metrics: Queue depth, message age DLQ message count API Gateway Metrics: Request count, latency, 4xx/5xx errors Alarms: Automated alerts for: High error rates Queue depth exceeding threshold Lambda timeouts API Gateway throttling Data Flow Summary:\nUser submits request → API Gateway (validates and queues) Request queued in SQS (decoupled processing) Lambda function triggered by SQS message Lambda processes request using AI services (Bedrock/Gemini) Results stored in DynamoDB User notified or results retrieved via API CloudWatch monitors entire flow Benefits of Serverless Architecture:\nScalability: Auto-scales with request volume Cost Efficiency: Pay only for actual usage Reliability: Built-in retry mechanisms and DLQ Maintenance: No server management required Performance: Low latency with optimized cold starts 4. Technical Implementation Implementation Phases\nPhase 1: Planning and Design (Weeks 1-2)\nRequirements gathering and user story definition System architecture design and AWS infrastructure planning Database schema design for PostgreSQL UI/UX wireframes and mockups API endpoint design for Spring Boot Third-party service evaluation and integration planning Multi-AZ architecture setup on AWS ECS Phase 2: Core Development (Weeks 3-6)\nSpring Boot application setup with monolithic architecture User authentication and authorization with Spring Security PostgreSQL database setup on Amazon RDS (Multi-AZ: active-passive) JWT and OAuth 2.0 integration (Google, Facebook) Next.js frontend setup with TypeScript Frontend component library creation Basic CRUD operations for all modules AWS ECS cluster configuration with active-passive failover Phase 3: Feature Development (Weeks 7-10)\nBlog platform with advanced filtering and search Study room creation with WebRTC integration Practice test module with automatic grading logic Flashcard system with spaced repetition algorithm Real-time messaging with Spring WebSocket Dictionary and Google Translate API integration Amazon S3 integration for file uploads ElastiCache Redis for session management and caching Phase 4: AI Integration \u0026amp; Deploying (Weeks 11-12)\nAI Service Infrastructure Setup: Amazon API Gateway configuration for AI endpoints Amazon SQS queue setup with Dead Letter Queue (DLQ) AWS Lambda function development (3 functions) Amazon DynamoDB table design for AI results storage Lambda Function 1: Writing Assessment Integration with Google Gemini Flash API or Amazon Bedrock Writing evaluation logic (grammar, vocabulary, task achievement, coherence) Result storage in DynamoDB Lambda Function 2: Speaking Assessment Audio transcription integration Integration with Google Gemini Flash API or Amazon Bedrock Speaking evaluation logic (pronunciation, fluency, coherence, lexical resource) Result storage in DynamoDB Lambda Function 3: RAG-based Flashcard Generation Document chunking algorithm implementation Amazon Titan V2 Embeddings integration for vector generation Vector store setup (Amazon OpenSearch Service or Bedrock Knowledge Base) Google Gemini integration for smart query generation RAG pipeline: Query generation → Document retrieval → Flashcard generation Flashcard storage in DynamoDB Automatic vocabulary extraction algorithms Comprehensive testing (unit, integration, end-to-end) Performance optimization and load testing Security audit and bug fixes Production deployment to AWS ECS with Multi-AZ Monitoring setup with CloudWatch for Lambda, SQS, and API Gateway Technical Requirements\nDevelopment Environment:\nJava 17+ for Spring Boot backend Node.js 18+ for Next.js frontend PostgreSQL 14+ for database Docker for local containerization Git for version control Maven for Java dependency management Frontend Requirements:\nNext.js 14+ with App Router and TypeScript WebRTC for real-time video/voice communication Socket.io client for real-time features Form validation (React Hook Form + Zod) Backend Requirements:\nSpring Boot 3.x with Java 17+ Spring Data JPA for database operations Spring Security for authentication/authorization Spring WebSocket for real-time features Spring Web for RESTful APIs JWT for token-based authentication OAuth 2.0 for social login Multipart file upload handling AWS Infrastructure:\nAmazon ECS: Fargate launch type for containerized applications Application Load Balancer: Routes traffic to active AZ with health checks Amazon RDS PostgreSQL: Multi-AZ active-passive deployment for high availability Amazon ElastiCache (Redis): Session and cache management Amazon S3: Media file storage Amazon CloudFront: CDN for static assets Amazon CloudWatch: Logging and monitoring Amazon VPC: Network isolation with public/private subnets across 2 AZs AWS Certificate Manager: SSL/TLS certificates Amazon API Gateway: RESTful API for AI service endpoints Amazon SQS: Message queue for asynchronous AI processing with DLQ AWS Lambda: Serverless compute for AI functions (3 functions: Writing, Speaking, Flashcard Generation) Amazon DynamoDB: NoSQL database for AI assessment results and flashcards Amazon Bedrock: AI model service (Titan V2 Embeddings, GPT-OSS) Amazon OpenSearch Service (optional): Vector store for RAG document embeddings AI/ML Integration:\nGoogle Gemini Flash API (Free Tier): Writing assessment (grammar, vocabulary, task achievement) Speaking assessment (pronunciation, fluency, coherence) Smart query generation for RAG-based flashcard creation Amazon Bedrock (GPT-OSS): Alternative AI model for assessments Flashcard generation from query-context pairs Amazon Titan V2 Embeddings: Vector embeddings for document chunks Semantic search and retrieval for RAG RAG (Retrieval-Augmented Generation) Architecture: Document chunking and embedding generation Vector store for semantic search (OpenSearch Service or Bedrock Knowledge Base) Context-aware flashcard generation using smart queries + document chunks 5. Timeline \u0026amp; Milestones Project Timeline: 3 Months (12 Weeks)\nWeeks 1-2: Planning \u0026amp; Design\n✓ Requirements analysis and documentation ✓ AWS Multi-AZ architecture design ✓ PostgreSQL database schema design ✓ UI/UX mockups completion ✓ Spring Boot project structure setup Deliverable: Complete technical specification document and AWS infrastructure plan Weeks 3-6: Core Development\n✓ Spring Boot monolithic backend setup ✓ Spring Security implementation (JWT, OAuth 2.0) ✓ User management and role-based access control ✓ Amazon RDS PostgreSQL Multi-AZ deployment ✓ Next.js frontend framework and component library ✓ AWS ECS cluster setup with Application Load Balancer ✓ ElastiCache Redis configuration Deliverable: Working authentication system and AWS infrastructure Weeks 7-10: Feature Development\n✓ Blog platform with full CRUD functionality ✓ Study room creation and management ✓ WebRTC integration for video/voice calls Practice test module (Reading \u0026amp; Listening) Flashcard system with study modes Amazon S3 integration for media storage Free dictionary API and open-source translation integration Spring WebSocket for real-time features Deliverable: All five core features operational (without AI) Weeks 11-12: AI Integration \u0026amp; Deployment\n✓ Amazon API Gateway and SQS setup for AI service architecture ✓ AWS Lambda Function 1: Writing Assessment with Gemini/Bedrock integration ✓ AWS Lambda Function 2: Speaking Assessment with Gemini/Bedrock integration ✓ AWS Lambda Function 3: RAG-based Flashcard Generation Document chunking and Amazon Titan V2 Embeddings integration Google Gemini smart query generation RAG pipeline with vector retrieval and flashcard generation ✓ Amazon DynamoDB setup for AI results storage ✓ Amazon OpenSearch Service or Bedrock Knowledge Base for vector store ✓ Vocabulary extraction algorithms ✓ Comprehensive testing (unit, integration, E2E) ✓ Performance optimization and security audit ✓ Production deployment to AWS ECS Multi-AZ ✓ CloudWatch monitoring and alerting setup for Lambda, SQS, API Gateway ✓ Final bug fixes and documentation Deliverable: Production-ready application deployed on AWS with complete AI service architecture Key Milestones:\nWeek 2: Technical specification and AWS architecture approved Week 6: Core backend and infrastructure deployed Week 10: All features complete (beta version) Week 12: Production launch with AI integration Weekly Sprint Goals:\nSprint 1-2: Architecture and setup Sprint 3-4: Authentication and database Sprint 5-6: Core API and AWS deployment Sprint 7-8: Blog and study room features Sprint 9-10: Practice tests and flashcards Sprint 11: AI integration and testing Sprint 12: Production deployment and launch 6. Budget Estimation Development Costs (One-time)\nSoftware \u0026amp; Tools:\nDevelopment tools and licenses: $0 (using free/open-source tools) Design tools (Figma Free): $0 Project management tools: $0 (using free tier) Total Software: $0 Initial Setup:\nDomain registration: $1/year SSL certificate: $0 (AWS Certificate Manager - Free) Development servers: $0 (local development) Total Initial Setup: $1 Operating Costs (Monthly)\nInfrastructure \u0026amp; Hosting (AWS):\nAWS Infrastructure \u0026amp; Hosting Third-party Services:\nGoogle Gemini Flash API: Free tier Subtotal Services: $0/month Other Operating Costs:\nMonitoring \u0026amp; analytics: $10/month (integrated with CloudWatch) Backup storage (RDS automated backups): $5/month Domain \u0026amp; SSL renewal: $0.08/month (amortized from $1/year, SSL via AWS Certificate Manager - Free) Subtotal Other: $15.08/month Total Monthly Operating Costs: $103.66/month\nAnnual Budget Summary\nDevelopment Phase (3 Months):\nDevelopment costs: $1 (one-time, domain only) Operating costs (3 months): $310.98 ($103.66 × 3 months) Total Development Phase: $311.98 Year 1 (After Launch):\nDevelopment costs: $1 (one-time, domain only) Operating costs: $1,243.92 ($103.66 × 12 months) Total Year 1: $1,244.92 Year 2+ (Annual Recurring):\nOperating costs: $1,243.92/year Domain renewal: $1/year Total Annual: $1,244.92 Revenue Projections (Subscription Model)\nMembership Tiers:\nGuest: Free (limited features) Member: $5/month (basic features) Premium: $15/month (all features + AI assessments) Conservative Revenue Estimate (Year 1):\nMonth 6-12: Average 100 Premium + 200 Members Revenue: (100 × $15 + 200 × $5) × 7 months = $17,500 Operating costs (Year 1): $1,243.92 Net Profit Year 1: $16,256.08 Break-even: Month 1 after launch Optimistic Revenue Estimate (Year 2):\n500 Premium + 1,000 Members Monthly revenue: $12,500 Annual revenue: $150,000 Operating costs: $1,243.92 Net Profit Year 2: $148,756.08 Profit margin: ~97.6% after operating costs Cost Optimization Strategies:\nZero personnel costs (self-developed project) Active-passive Multi-AZ deployment reduces costs (standby resources only used during failover) Use AWS ECS Fargate Spot for development environments (70% cost savings) Implement caching with ElastiCache to reduce database queries Use CloudFront CDN to minimize data transfer costs Leverage free tier of Google Gemini Flash API for AI features Use AWS Lambda for cost-effective serverless AI processing (pay per request) Amazon SQS provides reliable message queuing with minimal cost Amazon Bedrock Titan V2 Embeddings offers competitive pricing for vector generation DynamoDB on-demand pricing scales with usage Utilize free dictionary APIs and open-source translation libraries Free development tools and design software (VS Code, Figma Free, etc.) Leverage AWS Free Tier during initial development RDS automated backups included (7-day retention) Use AWS Certificate Manager for free SSL certificates Implement S3 lifecycle policies to move old data to cheaper storage tiers No email service costs by deferring email features to later phase Standby ECS tasks in AZ-2 kept minimal until failover needed 7. Risk Assessment Risk Matrix High Priority Risks:\nAI API Cost Overruns\nImpact: Low | Probability: Low Description: Google Gemini Flash API free tier has usage limits that may be exceeded Mitigation: Implement usage quotas and rate limiting; monitor API usage through dashboard Contingency: Upgrade to paid tier if free limits are consistently exceeded, or implement queuing system Data Privacy \u0026amp; Security Breaches\nImpact: Critical | Probability: Low Description: User data exposure or unauthorized access Mitigation: Implement encryption, regular security audits, GDPR compliance Contingency: Incident response plan, insurance, legal consultation Scalability Issues\nImpact: High | Probability: Medium Description: System performance degradation with user growth Mitigation: AWS ECS Auto Scaling in active AZ, RDS Multi-AZ active-passive for high availability, ElastiCache for performance Contingency: Scale ECS tasks in active AZ, activate additional tasks in passive AZ if needed, upgrade RDS instance class Medium Priority Risks:\nThird-party Service Downtime\nImpact: Medium | Probability: Low Description: Dependency on external APIs (Gemini Flash free tier, free dictionary APIs) Mitigation: Graceful degradation, inform users when AI features are unavailable Contingency: Cached responses for dictionary lookups, manual grading option for tests during outages User Acquisition Challenges\nImpact: High | Probability: Medium Description: Difficulty attracting and retaining users Mitigation: Marketing strategy, SEO optimization, referral programs Contingency: Pivot features based on feedback, partnerships with schools Content Moderation Issues\nImpact: Medium | Probability: High Description: Inappropriate content in blogs, comments, or study rooms Mitigation: Automated content filtering, reporting system, moderator team Contingency: Community guidelines, user bans, legal disclaimer Low Priority Risks:\nTechnology Stack Obsolescence\nImpact: Low | Probability: Medium Description: Chosen technologies become outdated Mitigation: Regular dependency updates, modular architecture Contingency: Gradual migration plan, refactoring budget Competition from Established Platforms\nImpact: Medium | Probability: High Description: Competing with Duolingo, IELTS.org, etc. Mitigation: Unique features (study rooms, AI assessment), niche targeting Contingency: Differentiation strategy, feature innovation Mitigation Strategies Technical Mitigations:\nImplement comprehensive error handling and logging with CloudWatch Set up CloudWatch alarms for resource utilization and errors Regular automated backups with RDS Multi-AZ and point-in-time recovery Use CloudFront CDN for static assets to reduce ECS load Implement API rate limiting to prevent abuse Code reviews and automated testing in CI/CD pipeline (AWS CodePipeline/GitHub Actions) AWS WAF for application security and DDoS protection Business Mitigations:\nStart with freemium model to build user base Beta testing phase to identify critical issues Gradual feature rollout to manage costs Build community through social media and content marketing Establish partnerships with IELTS teachers and institutions Legal \u0026amp; Compliance Mitigations:\nTerms of Service and Privacy Policy GDPR and data protection compliance Content licensing agreements User consent for data processing Regular compliance audits Contingency Plans Technical Failures:\nDatabase failure: Automatic failover to RDS Multi-AZ standby instance in AZ-2 (passive) ECS task failure in AZ-1: Auto Scaling replaces unhealthy tasks; critical failure triggers AZ-2 activation API downtime: Serve cached content from ElastiCache and queue requests Security breach: AWS Security Hub immediate alerts, lockdown, and investigation Active-passive Multi-AZ ensures high availability with automatic failover to passive AZ-2 Business Failures:\nLow user adoption: Pivot to B2B model (schools, tutors) High churn rate: User interviews, feature improvements Revenue shortfall: Cost optimization, seek investment Legal Issues:\nCopyright claims: Content takedown procedure Privacy complaints: Data deletion and compliance review Terms violations: User suspension and investigation 8. Expected Outcomes Technical Improvements Platform Capabilities:\nFully functional web application with 5 integrated modules Real-time collaboration features (video/voice calls, messaging) Serverless AI service architecture with asynchronous processing AI-powered Writing assessment via Lambda Function 1 (grammar, vocabulary, task achievement, coherence) AI-powered Speaking assessment via Lambda Function 2 (pronunciation, fluency, coherence, lexical resource) RAG-based flashcard generation via Lambda Function 3 with smart query generation and document understanding Scalable Multi-AZ architecture on AWS ECS supporting 10,000+ concurrent users Mobile-responsive design for learning on-the-go Robust RESTful API built with Spring Boot monolith High availability with 99.9% uptime through active-passive Multi-AZ deployment Vector embeddings and semantic search for intelligent content retrieval Technical Achievements:\nModern full-stack web development with Next.js and Spring Boot Real-time communication implementation (WebRTC, Spring WebSocket) Serverless AI service architecture with API Gateway, SQS, and Lambda Three specialized Lambda functions for Writing/Speaking assessment and RAG-based flashcard generation RAG (Retrieval-Augmented Generation) implementation with Amazon Titan V2 Embeddings Vector store integration for semantic search and document retrieval Google Gemini Flash AI/ML integration for assessments and smart query generation Amazon Bedrock integration for alternative AI models and embeddings AWS cloud infrastructure and active-passive Multi-AZ architecture implementation PostgreSQL database design and optimization DynamoDB for AI results and flashcard storage Container orchestration with Amazon ECS Fargate Security best practices with Spring Security and AWS services DevOps practices with CloudWatch monitoring and automated deployments Educational Impact For Students:\nAccessible, affordable IELTS preparation platform Personalized learning paths and progress tracking Immediate feedback on practice tests Community-driven learning environment 24/7 access to study materials and practice tests Estimated 30-40% cost reduction compared to traditional courses Learning Outcomes:\nImproved IELTS scores through consistent practice Better time management with Pomodoro integration Enhanced vocabulary through flashcard system Intelligent flashcard generation from documents using RAG technology Speaking confidence through AI feedback with detailed pronunciation and fluency analysis Writing skills improvement with detailed grammar, vocabulary, and task achievement analysis Context-aware learning through RAG-based flashcard generation that understands document content Business Value Market Position:\nCompetitive alternative to expensive IELTS prep courses Unique combination of features (study rooms + AI + community) Scalable SaaS business model Potential for B2C and B2B markets (schools, tutoring centers) User Growth Targets:\nWeek 12 (Launch): 100 registered users (beta testers) Month 6: 500 registered users Month 12: 2,000 registered users Year 2: 10,000 registered users Premium conversion rate: 10-15% Revenue Potential:\nYear 1: $17,500 (after launch in Month 6) Year 2: $150,000+ (with 500 premium, 1,000 regular members) Year 3: $500,000+ (with market expansion and partnerships) Long-term Value Platform Evolution:\nFoundation for other language learning modules (TOEFL, SAT, etc.) Data collection for improved AI models Community-generated content library Potential for gamification and achievement systems Mobile app development based on web platform success Social Impact:\nDemocratizing IELTS preparation for students worldwide Reducing language learning barriers Building a supportive learning community Enabling peer-to-peer knowledge sharing Creating opportunities for educational content creators Portfolio \u0026amp; Career Benefits:\nComprehensive full-stack project with Spring Boot and Next.js for developer portfolios Real-world experience with AWS cloud services (ECS, RDS, S3, CloudFront, etc.) Active-passive Multi-AZ architecture and high-availability system design experience AI integration experience with Google Gemini Flash Understanding of educational technology (EdTech) Container orchestration and failover strategies Potential startup opportunity or acquisition target Success Metrics:\nUser engagement: Average 3+ sessions per week Test completion rate: 70%+ of started tests User retention: 60%+ monthly active users NPS Score: 50+ (indicating strong user satisfaction) Average IELTS score improvement: 0.5-1.0 band increase Project Planning "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-ai-service/5.6.2-sqs-queues/","title":"SQS Queues","tags":[],"description":"","content":"Overview Create Amazon SQS queues for asynchronous AI processing with Dead Letter Queues for failed messages.\nCreate Writing Assessment Queue Setting Value Name ielts-ai-dev-writing-evaluation Type Standard Visibility timeout 5 minutes Message retention 14 days Dead-letter queue ielts-ai-dev-writing-evaluation-dlq Max receives 3 Create Speaking Assessment Queue Setting Value Name ielts-ai-dev-speaking-evaluation Visibility timeout 15 minutes Dead-letter queue ielts-ai-dev-speaking-evaluation-dlq Create Flashcard Generation Queue Setting Value Name ielts-ai-dev-flashcard-generation Visibility timeout 15 minutes Dead-letter queue ielts-ai-dev-flashcard-generation-dlq AWS CLI Commands # Create Dead Letter Queue aws sqs create-queue --queue-name ielts-writing-dlq # Create main queue with DLQ aws sqs create-queue \\ --queue-name ielts-writing-queue \\ --attributes \u0026#39;{ \u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;300\u0026#34;, \u0026#34;MessageRetentionPeriod\u0026#34;: \u0026#34;1209600\u0026#34;, \u0026#34;RedrivePolicy\u0026#34;: \u0026#34;{\\\u0026#34;deadLetterTargetArn\\\u0026#34;:\\\u0026#34;arn:aws:sqs:ap-southeast-1:{account}:ielts-writing-dlq\\\u0026#34;,\\\u0026#34;maxReceiveCount\\\u0026#34;:\\\u0026#34;3\\\u0026#34;}\u0026#34; }\u0026#39; # Repeat for speaking and flashcard queues aws sqs create-queue --queue-name ielts-speaking-dlq aws sqs create-queue --queue-name ielts-speaking-queue \\ --attributes \u0026#39;{\u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;900\u0026#34;}\u0026#39; aws sqs create-queue --queue-name ielts-flashcard-dlq aws sqs create-queue --queue-name ielts-flashcard-queue \\ --attributes \u0026#39;{\u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;900\u0026#34;}\u0026#39; Queue Summary Queue Visibility Timeout DLQ Max Receives ielts-writing-queue 5 min ielts-writing-dlq 3 ielts-speaking-queue 15 min ielts-speaking-dlq 3 ielts-flashcard-queue 15 min ielts-flashcard-dlq 3 Next Steps Proceed to Lambda Functions.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS Event Purpose Provide an overview of the AI/ML landscape in Vietnam. Introduce key AWS AI/ML services, especially Amazon SageMaker. Dive into Generative AI with Amazon Bedrock, covering foundation models and modern deployment techniques (RAG, Prompt Engineering). Highlights Morning: AWS AI/ML Services Overview Overview: context of AI/ML in Vietnam and the workshop objectives. Amazon SageMaker: AWS\u0026rsquo;s end-to-end ML platform. ML workflow: data preparation, labeling, training, tuning, and model deployment. Built-in MLOps capabilities. Live demo: walkthrough of SageMaker Studio. Afternoon: Generative AI with Amazon Bedrock Foundation Models: comparison and guidance for choosing models such as Claude, Llama, Titan, etc. Prompt Engineering: Advanced techniques: Chain-of-Thought reasoning, Few-shot learning. Retrieval-Augmented Generation (RAG): RAG architecture and integration with external knowledge bases. Bedrock Agents: building multi-step workflows and tool integrations. Guardrails: safety and content filtering principles. Live demo: building a Generative AI chatbot using Bedrock. Key Takeaways SageMaker: understood as a comprehensive platform that manages the entire ML lifecycle (data prep → training → deployment). Bedrock \u0026amp; GenAI: learned Bedrock\u0026rsquo;s role as a foundation-model management platform, how to compare FMs, and core techniques like Prompt Engineering and RAG. Project application: RAG and Bedrock Agents are useful for enhancing AI/chatbot features in the Travel-Guided project. Live demos provided practical insights into deployment flows and rapid prototyping with Bedrock. Event Experience Impressed by the live demos, especially the fast Bedrock-based chatbot build. Valuable networking and exchange opportunities with AI/ML experts in Vietnam. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives Resolve AWS account issues and create a new account if necessary. Master Hybrid DNS configuration with Route 53 Resolver. Implement and understand VPC Peering for inter-VPC communication. Discuss project plans and finalize programming language with the team. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Access Management with AWS Identity and Access Management (IAM). 21/09/2025 23/09/2025 AWS Identity and Access Management (IAM) Access Control 3 - Complete Lab 10: Route 53 and Hybrid DNS configuration.\n- Launch virtual servers to implement and test DNS setup.\n- Complete: Hybrid DNS Management with Amazon Route 53. 24/09/2025 25/09/2025 FCJ Playlist 4 - Implement VPC Peering for private communication between VPCs.\n- Create necessary resources for VPC Peering configuration.\n- Clean up resources after completion.\n- Complete: Network Integration with VPC Peering. 25/09/2025 26/09/2025 AWS VPC Peering 5 - Attend team meeting to discuss project plans and programming language selection.\n- Set deadlines for team members to study chosen technology stack. 28/09/2025 28/09/2025 Team Meeting AWS Skill Builder Courses Completed Course Category Status Hybrid DNS Management with Amazon Route 53 Networking ✅ Network Integration with VPC Peering Networking ✅ Networking on AWS Workshop Networking ✅ Infrastructure as Code with AWS CloudFormation DevOps ✅ Cloud Development with AWS Cloud9 Development ✅ Static Website Hosting with Amazon S3 Storage ✅ Week 3 Achievements Technical Skills Acquired:\nRoute 53 and Hybrid DNS:\nSuccessfully configured Hybrid DNS infrastructure with Route 53 Resolver Created and configured Outbound Endpoints for DNS query forwarding Set up Route 53 Resolver rules for conditional DNS resolution Implemented Inbound Endpoints for on-premises to AWS DNS queries Successfully connected to RD Gateway Server during practical exercises VPC Peering:\nMastered VPC Peering concepts for private inter-VPC communication without traversing public internet Enabled Cross-Zone and Cross-Region DNS Resolution in VPC Peering: EC2 instances can now resolve DNS of instances in peered VPCs to private IP addresses Understood that without this feature, DNS queries return public IPs, routing traffic through internet Learned resource cleanup procedures to avoid unnecessary costs Infrastructure as Code:\nLearned to provision AWS resources using CloudFormation templates Understood declarative infrastructure management principles Explored AWS Cloud9 as a cloud-based development environment Team Collaboration:\nParticipated in team meeting to finalize project direction Selected programming language for the project Established deadlines for team members to study the chosen technology stack Continued learning journey with FCJ team support Key Takeaways:\nHybrid DNS enables seamless DNS resolution between on-premises and AWS environments VPC Peering is cost-effective for connecting VPCs but has limitations (no transitive peering) CloudFormation templates ensure consistent, repeatable infrastructure deployments AWS Cloud9 eliminates local development environment setup complexity "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Multi-Region keys: A new approach to key replication in AWS Payment Cryptography","tags":[],"description":"","content":"Published: 2025‑09‑16 – Authors: Ruy Cavalcanti and Mark Cline in Announcements, Financial Services, Industries, Intermediate (200), Security, Identity, \u0026amp; Compliance, Technical How-to\nIn our previous post (Part 1 of the key replication series), Automatically replicate your card payment keys across AWS Regions, we explored an event-driven serverless architecture using AWS PrivateLink to securely replicate card payment keys between AWS Regions. That solution illustrated how to build a custom payment cryptography key replication framework.\nBased on customer feedback requesting a more automated, code-free approach, we are excited to announce an additional option with Multi-Region keys for AWS Payment Cryptography in Part 2 of the series.\nWith this new feature, you can automatically synchronize payment cryptography keys from a primary Region to other Regions you choose, improving fault tolerance and availability of payment applications. You can also choose between account-level or key-level replication, providing more flexibility in managing payment keys across Regions.\nMulti-Region keys: Overview and benefits The new Multi-Region key replication feature for AWS Payment Cryptography provides you with flexible control over key replication strategy through the following key capabilities:\nDecide whether keys are replicated Choose specific Regions to replicate keys to Manage replication configuration changes Configure replication at the account or key level based on business needs Multi-Region keys provide many benefits for global payment operations, including:\nIncreased availability: Access payment keys even when one Region is unavailable. Disaster recovery capability: Maintain business operations during incidents by replicating keys between Regions. Global operations: Support payment processing across multiple geographic regions. Simplified management: Centralized control with distributed capabilities. Consistent key IDs: Same key IDs across Regions simplify application development. Configuration options Payment Cryptography provides two distinct methods to configure Multi-Region key replication, giving you flexibility in implementing the strategy that best fits your organization: you can choose between a broad approach (account-level) or a more granular approach (key-level).\nAccount-level With account-level configuration, AWS automatically copies exportable symmetric keys created in your Payment Cryptography account from the primary Region you specify to other Regions you specify. This simplifies key management in multi-Region deployments, provides consistent key availability in your chosen Regions, and reduces operational costs for key management.\nTo configure account-level replication using the AWS Command Line Interface (AWS CLI), use the new API enable-default-key-replication-regions to set the Regions where AWS will replicate your keys. To remove a Region from the default replication list, use the API disable-default-key-replication-regions.\nNote: Only symmetric keys created after account-level replication is enabled will be replicated.\nKey-level replication By using key-level replication, you can achieve more granular control by:\nDesignating specific keys as multi-Region keys. Defining custom replication targets for each multi-Region key. Maintaining separate keys for each Region when needed. Note: Within each Region, Payment Cryptography maintains key redundancy across multiple Availability Zones to ensure high availability. Multi-Region key replication extends this geographically, enhancing resilience to inter-Region connectivity failures while maintaining control over key storage location.\nYou can specify replication Regions when creating keys using the --replication-regions parameter in AWS CLI, through the create-key or import-key APIs. For existing keys, you can use the new APIs add-key-replication-regions and remove-key-replication-regions to manage which Regions will receive key replicas.\nImportant: When you specify replication Regions during key creation, these settings take precedence over the default account-level replication configuration.\nHow it works Figure 1 describes the process when you replicate a key in Payment Cryptography:\nThe key is created in the primary Region you specify. Payment Cryptography automatically asynchronously replicates the key material to the Regions selected as replicas. Replicated keys maintain the same Key ID across all Regions — only the Region portion of the Amazon Resource Name (ARN) changes. The key in the primary Region is labeled MultiRegionKeyType: PRIMARY Keys in replica Regions are labeled MultiRegionKeyType: REPLICA and have a reference to the primary Region. When deleting a key, the deletion automatically cascades from the primary key to replicas in other Regions. | Figure 1: Illustration of key replication process from us-east-1 to us-west-2\nExample: Creating a multi-Region key at key level The following example illustrates creating a card verification key (CVK) in the primary Region (us-east-1) with replication to us-west-2:\naws payment-cryptography create-key \\\n--exportable \\\n--key-attributes KeyAlgorithm=TDES_2KEY,\\\nKeyUsage=TR31_C0_CARD_VERIFICATION_KEY,\\\nKeyClass=SYMMETRIC_KEY,KeyModesOfUse=\u0026rsquo;{Generate=true,Verify=true}\u0026rsquo; \\\n--region us-east-1 \\\n--replication-regions us-west-2\nThe response shows the key is being created and replication is in progress:\n{ \u0026ldquo;Key\u0026rdquo;: { \u0026ldquo;KeyArn\u0026rdquo;: \u0026ldquo;arn:aws:payment-cryptography:us-east-1:111122223333:key/qs6643jl4ohibtqk\u0026rdquo;, \u0026ldquo;KeyAttributes\u0026rdquo;: { \u0026ldquo;KeyUsage\u0026rdquo;: \u0026ldquo;TR31_C0_CARD_VERIFICATION_KEY\u0026rdquo;, \u0026ldquo;KeyClass\u0026rdquo;: \u0026ldquo;SYMMETRIC_KEY\u0026rdquo;, \u0026ldquo;KeyAlgorithm\u0026rdquo;: \u0026ldquo;TDES_2KEY\u0026rdquo;, \u0026ldquo;KeyModesOfUse\u0026rdquo;: { \u0026ldquo;Encrypt\u0026rdquo;: false, \u0026ldquo;Decrypt\u0026rdquo;: false, \u0026ldquo;Wrap\u0026rdquo;: false, \u0026ldquo;Unwrap\u0026rdquo;: false, \u0026ldquo;Generate\u0026rdquo;: true, \u0026ldquo;Sign\u0026rdquo;: false, \u0026ldquo;Verify\u0026rdquo;: true, \u0026ldquo;DeriveKey\u0026rdquo;: false, \u0026ldquo;NoRestrictions\u0026rdquo;: false } }, \u0026ldquo;KeyCheckValue\u0026rdquo;: \u0026ldquo;CC5EE2\u0026rdquo;, \u0026ldquo;KeyCheckValueAlgorithm\u0026rdquo;: \u0026ldquo;ANSI_X9_24\u0026rdquo;, \u0026ldquo;Enabled\u0026rdquo;: true, \u0026ldquo;Exportable\u0026rdquo;: true, \u0026ldquo;KeyState\u0026rdquo;: \u0026ldquo;CREATE_COMPLETE\u0026rdquo;, \u0026ldquo;KeyOrigin\u0026rdquo;: \u0026ldquo;AWS_PAYMENT_CRYPTOGRAPHY\u0026rdquo;, \u0026ldquo;CreateTimestamp\u0026rdquo;: \u0026ldquo;2025-08-21T15:25:54.475000-03:00\u0026rdquo;, \u0026ldquo;UsageStartTimestamp\u0026rdquo;: \u0026ldquo;2025-08-21T15:25:54.287000-03:00\u0026rdquo;, \u0026ldquo;MultiRegionKeyType\u0026rdquo;: \u0026ldquo;PRIMARY\u0026rdquo;, \u0026ldquo;ReplicationStatus\u0026rdquo;: { \u0026ldquo;us-west-2\u0026rdquo;: { \u0026ldquo;Status\u0026rdquo;: \u0026ldquo;IN_PROGRESS\u0026rdquo; } }, \u0026ldquo;UsingDefaultReplicationRegions\u0026rdquo;: false } }\nWhen replication completes, the status will be updated to SYNCHRONIZED:\naws payment-cryptography get-key \\\n--key-identifier arn:aws:payment-cryptography:us-east-1:111122223333:key/qs6643jl4ohibtqk \\\n--region us-east-1\nThe response shows the primary key has completed synchronization:\n\u0026ldquo;ReplicationStatus\u0026rdquo;: { \u0026ldquo;us-west-2\u0026rdquo;: { \u0026ldquo;Status\u0026rdquo;: \u0026ldquo;SYNCHRONIZED\u0026rdquo; } }\nYou can access the key in the replica Region (us-west-2) using the same Key ID, only changing the Region name:\naws payment-cryptography get-key \\\n--key-identifier arn:aws:payment-cryptography:us-west-2:111122223333:key/qs6643jl4ohibtqk \\\n--region us-west-2\nThe response displays the replica key with a reference to the primary Region (PrimaryRegion: us-east-1).\nImportant considerations When using multi-Region keys, there are several important points to consider:\nOnly exportable symmetric keys are supported, asymmetric keys are not supported. Costs are calculated separately for each Region — for example, replicating to 3 Regions will incur costs for the primary key plus costs for each replica key. Key aliases and tags need to be managed separately in each Region as they are not automatically replicated. Primary keys can be edited or updated, while replica keys are read-only and support cryptographic operations. All changes must be made to the primary key and Payment Cryptography will automatically synchronize to replica Regions. Monitor replication status to ensure changes have been successfully synchronized. Key deletion behavior When scheduling deletion of a primary key:\nAll replica keys will be deleted immediately. The primary key will transition to pending deletion status for a minimum of 3 days, during which time deletion can be canceled. If you restore the primary key, you need to re-enable replication to recreate replicas in desired Regions. After the 3-day period passes, the primary key will be permanently deleted and cannot be recovered. Deleting a replica key only affects that Region, does not affect the primary key or other replicas. Eventual consistency model Multi-Region key replication operates on an eventual consistency model — meaning changes may not appear immediately across all Regions.\nTherefore, applications should be designed to handle this model and not assume that keys or changes will be immediately available in replica Regions.\nIf your application requires strong consistency, implement a polling mechanism using the GetKey API to verify that changes have been synchronized before proceeding with cryptographic operations.\nLogging and monitoring Payment Cryptography records API activity through AWS CloudTrail, which now includes new events and attributes specifically for the Multi-Region key replication feature.\nNew CloudTrail events The service records a new event type called SynchronizeMultiRegionKey, which appears in both the primary and replica Regions.\nEvents in the primary Region: Each defined replica Region generates two SynchronizeMultiRegionKey events in the primary Region:\nAn event related to the key export process.\n\u0026ldquo;serviceEventDetails\u0026rdquo;: {\n\u0026ldquo;keyArn\u0026rdquo;: \u0026ldquo;arn:aws:payment-cryptography:us-east-1:111122223333:key/qs6643jl4ohibtqk\u0026rdquo;,\n\u0026ldquo;replicationRegion\u0026rdquo;: \u0026ldquo;us-west-2\u0026rdquo;,\n\u0026ldquo;replicationType\u0026rdquo;: \u0026ldquo;ExportKeyReplica\u0026rdquo;\n},\nAn event related to the key import process.\n\u0026ldquo;serviceEventDetails\u0026rdquo;: {\n\u0026ldquo;keyArn\u0026rdquo;: \u0026ldquo;arn:aws:payment-cryptography:us-east-1:111122223333:key/qs6643jl4ohibtqk\u0026rdquo;,\n\u0026ldquo;replicationRegion\u0026rdquo;: \u0026ldquo;us-west-2\u0026rdquo;,\n\u0026ldquo;replicationType\u0026rdquo;: \u0026ldquo;ImportKeyReplica\u0026rdquo;\n},\nEvents in replica Regions: Each replica Region records a SynchronizeMultiRegionKey event corresponding to the key import process:\n\u0026ldquo;eventName\u0026rdquo;: \u0026ldquo;SynchronizeMultiRegionKey\u0026rdquo;,\n\u0026ldquo;awsRegion\u0026rdquo;: \u0026ldquo;us-west-2\u0026rdquo;,\n\u0026ldquo;serviceEventDetails\u0026rdquo;: {\n\u0026quot;keyArn\u0026quot;: \u0026quot;arn:aws:payment-cryptography:us-west-2:111122223333:key/qs6643jl4ohibtqk\u0026quot;, \u0026quot;replicationRegion\u0026quot;: \u0026quot;us-west-2\u0026quot;, **\u0026quot;replicationType\u0026quot;: \u0026quot;ImportKeyReplica\u0026quot;** }, New CloudTrail attributes Key management APIs now include new attributes to reflect Multi-Region replication activity.\nFor example, the CreateKey event in the primary Region (us-east-1) now includes the attribute:\n\u0026ldquo;requestParameters\u0026rdquo;: { \u0026ldquo;replicationRegions\u0026rdquo;: [\u0026ldquo;us-west-2\u0026rdquo;] }\nThe CreateKey event in the replica Region (us-west-2) will reflect the corresponding replica key initialization process, with similar information about KeyUsage, Algorithm, and KeyCheckValue.\nGetting started To get started with Multi-Region key replication in Payment Cryptography, follow these steps:\nIdentify the primary Region.\nIdentify replica Regions and decide whether you will use account-level or key-level configuration.\nCreate new exportable symmetric keys or update existing keys to enable Multi-Region replication.\nUpdate your application to use consistent Key IDs across Regions.\nConclusion The new Multi-Region key replication feature in AWS Payment Cryptography enhances automatic key replication capabilities, improving fault tolerance, availability, and simplifying global payment key management. This feature ensures that your payment cryptography keys are always available anytime, anywhere, while providing flexibility in choosing replication strategies between account-level and key-level, helping organizations optimize performance, security, and costs in multi-Region environments.\nRuy Cavalcanti Ruy is a Senior Security Architect specializing in Latin American financial services at AWS. He has worked in IT and Security for over 19 years, helping customers build security architectures and solve data protection and compliance challenges. When not designing security solutions, he enjoys playing guitar, cooking Brazilian barbecue, and spending time with family and friends. Mark Cline Mark is Head of Product Management at AWS Payments, where he has over 15 years of experience in financial services across multiple use cases and industries. He collaborates with leading banks, financial institutions, and technology providers to reduce the burden on payment systems, enabling customers to focus on innovation. When not busy simplifying payments, you can find him coaching little league baseball teams or running. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup-fe/5.4.3-sg/","title":"Configure Security Group","tags":[],"description":"","content":"Our Frontend containers run in Private Subnets. To allow them to receive traffic, we must configure a Security Group that acts as a firewall.\nFor security best practices, we will only allow traffic from the Application Load Balancer (ALB) on port 3000. Direct access from the internet or other sources will be blocked.\n1. Create Security Group Navigate to EC2 Dashboard \u0026gt; Security Groups \u0026gt; Create security group. Basic details: Security group name: ecs-private-sg. Description: security group for ecs. VPC: Select band-up-vpc. 2. Configure Inbound Rules This is the most critical step. We need to allow the ALB to talk to our Next.js application.\nInbound rules: Click Add rule. Type: Custom TCP. Port range: 3000 (The port our Next.js app listens on). Source: Select Custom and choose the Security Group ID of your ALB (e.g., alb-sg). Note: By selecting the ALB\u0026rsquo;s Security Group ID instead of an IP range, we ensure that only traffic originating from our Load Balancer is accepted. Outbound rules: Leave the default settings (Allow all traffic) to enable the container to download packages or communicate with external APIs. Click Create security group. The Security Group is now ready to be attached to our ECS Tasks in the next step.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-setup-be/5.5.3-redis/","title":"Create ElastiCache (Redis/Valkey)","tags":[],"description":"","content":"In this step, we provision an in-memory data store to handle session management and caching for the backend. We will use Amazon ElastiCache with the Valkey engine (a high-performance, open-source fork of Redis supported by AWS).\n1. Configure Security Group First, create a Security Group to allow the backend to communicate with the cache cluster.\nNavigate to EC2 \u0026gt; Security Groups \u0026gt; Create security group. Name: redis-sg. Inbound rules: Allow Custom TCP traffic on port 6379 from the ecs-backend-sg. (Note: Ensure this is created before proceeding to the ElastiCache console).\n2. Create Subnet Group We need to define which subnets the cache nodes will reside in.\nNavigate to Amazon ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group. Name: bandup-cached-subnet-group. VPC: Select band-up-vpc. Subnets: Select private-database-subnet-1 and private-database-subnet-2 (Availability Zones ap-southeast-1a and 1b). 3. Create ElastiCache Cluster Now we provision the cache cluster.\nNavigate to ElastiCache \u0026gt; Caches \u0026gt; Create cache. Engine: Select Valkey - recommended (Compatible with Redis OSS). Deployment option: Select Node-based cluster (Gives more control over instance types). Creation method: Cluster cache. Cluster settings: Cluster mode: Disabled (Simple primary-replica structure is sufficient). Name: bandup-redis. Description: in memory db for bandup. Node configuration: Node type: cache.t3.micro (Cost-effective for testing). Number of replicas: 0 (Standalone node for this workshop). Connectivity: Network type: IPv4. Subnet groups: Select bandup-cached-subnet-group. Security \u0026amp; Encryption: Encryption at rest: Enabled (Default key). Encryption in transit: Enabled. Access control: No access control (We rely on Security Groups). Security groups: Select redis-sg created earlier. Backup: Enable automatic backups (Retention: 1 day). Click Create. The cluster status will change to Creating. Once Available, note down the Primary Endpoint (ending in ...cache.amazonaws.com:6379) to use in the backend configuration.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-network/5.3.3-iam/","title":"IAM Roles for ECS","tags":[],"description":"","content":"To allow Amazon ECS to manage your containers, it needs specific permissions. We must create an IAM Role that authorizes the ECS agent to pull container images from Amazon ECR and send logs to Amazon CloudWatch on your behalf.\nCreate ecsTaskExecutionRole Navigate to the IAM Dashboard. In the left navigation pane, choose Roles. Click Create role. Step 1: Trusted Entity\nTrusted entity type: Select AWS service. Service or use case: Choose Elastic Container Service. Select Elastic Container Service Task from the options below. Click Next. Step 2: Add Permissions\nIn the search bar, type AmazonECSTaskExecutionRolePolicy. Check the box next to the policy name AmazonECSTaskExecutionRolePolicy. Note: This managed policy grants permissions to pull images from ECR and upload logs to CloudWatch. Click Next. Step 3: Name and Review\nRole name: Enter ecsTaskExecutionRole. Review the configuration and click Create role. Once created, this role is ready to be assigned to our ECS Task Definitions in the upcoming sections.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-network/","title":"Network &amp; Security Infrastructure","tags":[],"description":"","content":"Overview In this section, we will establish the foundational network layer and security boundaries for IELTS BandUp.\nA robust network architecture is critical for protecting sensitive user data and ensuring high availability. Instead of using the default network settings, we will construct a custom Virtual Private Cloud (VPC) designed for a production-grade environment. This setup allows us to strictly control traffic flow between our application components (Frontend, Backend, Database) and the internet.\nFurthermore, we will configure VPC Endpoints to allow our private containers to communicate securely with AWS services (like ECR and S3) without traversing the public internet, enhancing both security and network performance.\nImplementation Steps We will break down the infrastructure setup into the following key tasks:\nVPC \u0026amp; Connectivity: Create the isolated network environment, partition it into Public and Private subnets, and configure Internet Gateways (IGW) for external connectivity. Load Balancing (ALB): Set up the Application Load Balancer and Target Groups to distribute incoming traffic efficiently to our future ECS tasks. IAM Security: Provision the ecsTaskExecutionRole to grant our Fargate containers the necessary permissions to pull images and push logs. VPC Endpoints: Establish private connections to AWS services (ECR, CloudWatch, S3) to secure internal traffic. Content VPC, Subnets \u0026amp; Routing Application Load Balancer (ALB) IAM Roles for ECS VPC Endpoints Setup "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-ai-service/5.6.3-lambda-functions/","title":"Lambda Functions","tags":[],"description":"","content":"Overview The AI Service layer consists of four Lambda functions that power the IELTS learning platform. These functions process requests asynchronously via SQS queues and integrate with Google Gemini API and Amazon Bedrock for AI-powered evaluations.\nLambda Function 1: Writing Evaluator Evaluates IELTS Writing Task 1 and Task 2 essays using Gemini API with detailed band scoring.\nSetting Value Function name bandup-writing-evaluator Runtime Python 3.11 Memory 1024 MB Timeout 5 minutes Trigger SQS (bandup-writing-queue) AI Model Google Gemini 2.0 Flash Core Implementation:\nimport json import os import boto3 import logging from typing import Dict, Any logger = logging.getLogger() logger.setLevel(logging.INFO) # Import from Lambda layer from lambda_shared.gemini_client import GeminiClient from secrets_helper import get_gemini_api_key def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Evaluate IELTS Writing essays using Gemini API.\u0026#34;\u0026#34;\u0026#34; # Parse SQS message or API Gateway request if is_sqs_event(event): request_data, job_id = parse_sqs_message(event) update_job_status(job_id, \u0026#39;processing\u0026#39;, \u0026#39;writing\u0026#39;) else: request_data = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) # Get API key securely from Secrets Manager gemini_api_key = get_gemini_api_key() # Retrieved from AWS Secrets Manager gemini_client = GeminiClient(api_key=gemini_api_key) # Extract request parameters user_id = request_data.get(\u0026#39;user_id\u0026#39;) essay_content = request_data.get(\u0026#39;essay_content\u0026#39;) task_type = request_data.get(\u0026#39;task_type\u0026#39;, \u0026#39;TASK_2\u0026#39;) # Build evaluation prompt prompt = build_writing_prompt(essay_content, task_type) # Call Gemini API for evaluation response = gemini_client.generate_evaluation( prompt=prompt, feature=\u0026#39;writing_task2\u0026#39;, max_retries=3, timeout=60 ) # Parse and validate band scores evaluation = parse_gemini_response(response[\u0026#39;content\u0026#39;]) # Build result with IELTS criteria result = { \u0026#39;session_id\u0026#39;: request_data.get(\u0026#39;session_id\u0026#39;), \u0026#39;overall_band\u0026#39;: evaluation.get(\u0026#39;overall_band\u0026#39;), \u0026#39;task_achievement_band\u0026#39;: evaluation[\u0026#39;task_achievement\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;coherence_band\u0026#39;: evaluation[\u0026#39;coherence_cohesion\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;lexical_band\u0026#39;: evaluation[\u0026#39;lexical_resource\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;grammar_band\u0026#39;: evaluation[\u0026#39;grammatical_range_accuracy\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;feedback\u0026#39;: evaluation } # Save to DynamoDB dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ.get(\u0026#39;DYNAMODB_EVALUATIONS\u0026#39;)) table.put_item(Item={ \u0026#39;evaluation_id\u0026#39;: result[\u0026#39;session_id\u0026#39;], \u0026#39;user_id\u0026#39;: user_id, \u0026#39;evaluation_type\u0026#39;: \u0026#39;writing\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, **result }) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Gemini Prompt Template:\ndef build_writing_prompt(essay_content: str, task_type: str) -\u0026gt; str: return f\u0026#34;\u0026#34;\u0026#34;You are an experienced IELTS examiner. Evaluate this essay: Task Type: {task_type} ESSAY: {essay_content} Evaluate using IELTS band descriptors (1-9, 0.5 increments): 1. Task Achievement - Addresses all parts of task 2. Coherence and Cohesion - Logical organization 3. Lexical Resource - Vocabulary range and accuracy 4. Grammatical Range and Accuracy - Sentence structures RESPOND IN JSON FORMAT: {{ \u0026#34;overall_band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;task_achievement\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;coherence_cohesion\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;lexical_resource\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;grammatical_range_accuracy\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;quoted_examples\u0026#34;: [{{\u0026#34;quote\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;...\u0026#34;}}] }}\u0026#34;\u0026#34;\u0026#34; Lambda Function 2: Speaking Evaluator Evaluates IELTS Speaking using Gemini native audio processing - 72% cheaper and 2x faster than AWS Transcribe alternatives.\nSetting Value Function name bandup-speaking-evaluator Runtime Python 3.11 Memory 2048 MB Timeout 5 minutes Trigger SQS (bandup-speaking-queue) AI Model Gemini 2.5 Flash (Native Audio) Core Implementation:\nimport json import os import boto3 import logging from typing import Dict, Any, Tuple logger = logging.getLogger() # Import from Lambda layer from lambda_shared.gemini_client import GeminiClient from secrets_helper import get_gemini_api_key def download_audio_from_s3(audio_url: str) -\u0026gt; Tuple[bytes, str]: \u0026#34;\u0026#34;\u0026#34;Download audio file from S3 and determine MIME type.\u0026#34;\u0026#34;\u0026#34; s3_client = boto3.client(\u0026#39;s3\u0026#39;) # Parse S3 URL: s3://bucket-name/path/to/file.mp3 parts = audio_url.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) bucket, key = parts[0], parts[1] response = s3_client.get_object(Bucket=bucket, Key=key) audio_bytes = response[\u0026#39;Body\u0026#39;].read() # Determine MIME type from extension mime_types = {\u0026#39;.mp3\u0026#39;: \u0026#39;audio/mp3\u0026#39;, \u0026#39;.wav\u0026#39;: \u0026#39;audio/wav\u0026#39;, \u0026#39;.m4a\u0026#39;: \u0026#39;audio/m4a\u0026#39;} ext = \u0026#39;.\u0026#39; + key.split(\u0026#39;.\u0026#39;)[-1].lower() mime_type = mime_types.get(ext, \u0026#39;audio/mp3\u0026#39;) return audio_bytes, mime_type def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Evaluate IELTS Speaking using Gemini native audio.\u0026#34;\u0026#34;\u0026#34; # Parse request request_data = parse_request(event) # Get API key from Secrets Manager gemini_api_key = get_gemini_api_key() gemini_client = GeminiClient(api_key=gemini_api_key) # Extract parameters audio_url = request_data.get(\u0026#39;audio_url\u0026#39;) part = request_data.get(\u0026#39;part\u0026#39;, \u0026#39;PART_1\u0026#39;) questions = request_data.get(\u0026#39;questions\u0026#39;, []) # Step 1: Download audio from S3 audio_bytes, mime_type = download_audio_from_s3(audio_url) logger.info(f\u0026#34;Downloaded {len(audio_bytes)} bytes, MIME: {mime_type}\u0026#34;) # Step 2: Send audio directly to Gemini (ONE API call) # No AWS Transcribe needed - Gemini processes audio natively evaluation = gemini_client.evaluate_audio( audio_bytes=audio_bytes, part=part, questions=questions, mime_type=mime_type, max_retries=3, timeout=120 ) # Step 3: Build response with IELTS Speaking criteria result = { \u0026#39;session_id\u0026#39;: request_data.get(\u0026#39;session_id\u0026#39;), \u0026#39;transcript\u0026#39;: evaluation.get(\u0026#39;transcript\u0026#39;), \u0026#39;duration\u0026#39;: evaluation.get(\u0026#39;duration_seconds\u0026#39;), \u0026#39;overall_band\u0026#39;: evaluation.get(\u0026#39;overall_band\u0026#39;), \u0026#39;fluency_band\u0026#39;: evaluation[\u0026#39;fluency_coherence\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;lexical_band\u0026#39;: evaluation[\u0026#39;lexical_resource\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;grammar_band\u0026#39;: evaluation[\u0026#39;grammatical_range_accuracy\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;pronunciation_band\u0026#39;: evaluation[\u0026#39;pronunciation\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;model_used\u0026#39;: \u0026#39;gemini-2.5-flash-audio\u0026#39;, \u0026#39;estimated_cost\u0026#39;: evaluation[\u0026#39;usage\u0026#39;][\u0026#39;cost\u0026#39;] } # Save to DynamoDB save_evaluation(result, request_data.get(\u0026#39;user_id\u0026#39;)) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Cost Comparison:\nApproach Cost per 3-min Audio Latency Gemini Native Audio ~$0.021 30-45s AWS Transcribe + LLM ~$0.076 60-90s Savings 72% 2x faster Lambda Function 3: Flashcard Generator (RAG) Generates flashcards from PDF documents using lightweight RAG pipeline with Titan Embeddings (in-memory vector store, optimized for \u0026lt;50MB Lambda package).\nSetting Value Function name bandup-flashcard-generator Runtime Python 3.11 Memory 1024 MB Timeout 10 minutes Trigger SQS (bandup-flashcard-queue) AI Model Gemini + Amazon Titan Embeddings V2 RAG Pipeline Flow:\n┌─────────────┐ ┌──────────────┐ ┌─────────────────┐ │ PDF Upload │ ──▶ │ Chunking │ ──▶ │ Titan Embeddings│ │ (S3) │ │ (3000 chars) │ │ (Bedrock) │ └─────────────┘ └──────────────┘ └────────┬────────┘ │ ▼ ┌─────────────┐ ┌──────────────┐ ┌─────────────────┐ │ Flashcards │ ◀── │ Gemini │ ◀── │ In-Memory Store │ │ (JSON) │ │ Generation │ │ (Cosine Sim) │ └─────────────┘ └──────────────┘ └─────────────────┘ Core Implementation:\nimport json import os import boto3 import time import google.generativeai as genai from typing import Dict, Any, List from concurrent.futures import ThreadPoolExecutor, as_completed logger = logging.getLogger() # Global instance for warm starts (Lambda optimization) _rag_instance = None _s3_client = None def get_s3_client(): \u0026#34;\u0026#34;\u0026#34;Get cached S3 client.\u0026#34;\u0026#34;\u0026#34; global _s3_client if _s3_client is None: _s3_client = boto3.client(\u0026#39;s3\u0026#39;) return _s3_client def get_rag_instance(api_key: str): \u0026#34;\u0026#34;\u0026#34;Get cached RAG instance for warm starts.\u0026#34;\u0026#34;\u0026#34; global _rag_instance if _rag_instance is None: _rag_instance = RAG( api_key=api_key, chunk_size=int(os.environ.get(\u0026#39;RAG_CHUNK_SIZE\u0026#39;, \u0026#39;500\u0026#39;)), chunk_overlap=int(os.environ.get(\u0026#39;RAG_CHUNK_OVERLAP\u0026#39;, \u0026#39;100\u0026#39;)) ) logger.info(\u0026#34;Cold start: RAG instance created\u0026#34;) else: logger.info(\u0026#34;Warm start: Reusing RAG instance\u0026#34;) return _rag_instance def download_pdf_from_s3(bucket: str, key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Download PDF from S3 to /tmp.\u0026#34;\u0026#34;\u0026#34; s3 = get_s3_client() local_path = f\u0026#34;/tmp/{key.split(\u0026#39;/\u0026#39;)[-1]}\u0026#34; s3.download_file(bucket, key, local_path) return local_path def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;RAG-based flashcard generation .\u0026#34;\u0026#34;\u0026#34; start_time = time.time() is_async = is_sqs_event(event) # Parse request if is_async: request, job_id = parse_sqs_message(event) update_job_status(job_id, \u0026#39;processing\u0026#39;) else: request = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) if isinstance(event.get(\u0026#39;body\u0026#39;), str) else event # Get S3 location pdf_url = request.get(\u0026#39;pdf_url\u0026#39;) s3_bucket, s3_key = parse_s3_url(pdf_url) # Get API key from Secrets Manager secret_arn = os.environ.get(\u0026#39;GEMINI_API_KEY_SECRET_ARN\u0026#39;) secrets_client = boto3.client(\u0026#39;secretsmanager\u0026#39;) api_key = secrets_client.get_secret_value(SecretId=secret_arn)[\u0026#39;SecretString\u0026#39;] # Get parameters num_cards = int(request.get(\u0026#39;num_cards\u0026#39;, 10)) difficulty = request.get(\u0026#39;difficulty\u0026#39;, \u0026#39;MEDIUM\u0026#39;) question_types = request.get(\u0026#39;question_types\u0026#39;, [\u0026#39;DEFINITION\u0026#39;, \u0026#39;VOCABULARY\u0026#39;, \u0026#39;COMPREHENSION\u0026#39;]) # Step 1: Download PDF from S3 local_pdf = download_pdf_from_s3(s3_bucket, s3_key) # Step 2: Index document with RAG (Titan Embeddings + in-memory store) rag = get_rag_instance(api_key) rag._vector_store = None # Reset for new document rag._chunks = [] index_result = rag.index_document(local_pdf, document_id=s3_key) logger.info(f\u0026#34;Indexed {index_result[\u0026#39;chunk_count\u0026#39;]} chunks from {index_result[\u0026#39;page_count\u0026#39;]} pages\u0026#34;) # Step 3: Retrieve relevant chunks (hybrid approach) if index_result[\u0026#39;chunk_count\u0026#39;] \u0026lt;= 15: # Small document: use representative chunks chunks = rag.get_representative_chunks(num_chunks=min(10, index_result[\u0026#39;chunk_count\u0026#39;])) retrieval_method = \u0026#34;representative\u0026#34; else: # Large document: use smart keyword-based queries chunks = rag.retrieve_with_smart_queries(top_k_per_query=3) retrieval_method = \u0026#34;smart_queries\u0026#34; # Step 4: Generate flashcards with Gemini prompt = generate_flashcards_prompt(chunks, num_cards, difficulty, question_types) flashcard_result = call_gemini(prompt, api_key) # Clean up os.remove(local_pdf) # Build response total_time = time.time() - start_time response_body = { \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;set_id\u0026#39;: request.get(\u0026#39;set_id\u0026#39;), \u0026#39;user_id\u0026#39;: request.get(\u0026#39;user_id\u0026#39;), \u0026#39;document\u0026#39;: { \u0026#39;s3_bucket\u0026#39;: s3_bucket, \u0026#39;s3_key\u0026#39;: s3_key, \u0026#39;page_count\u0026#39;: index_result[\u0026#39;page_count\u0026#39;], \u0026#39;chunk_count\u0026#39;: index_result[\u0026#39;chunk_count\u0026#39;] }, \u0026#39;retrieval\u0026#39;: { \u0026#39;method\u0026#39;: retrieval_method, \u0026#39;chunks_used\u0026#39;: len(chunks), \u0026#39;keywords\u0026#39;: index_result.get(\u0026#39;keywords\u0026#39;, [])[:5] }, \u0026#39;flashcards\u0026#39;: flashcard_result.get(\u0026#39;flashcards\u0026#39;, []), \u0026#39;total_cards\u0026#39;: len(flashcard_result.get(\u0026#39;flashcards\u0026#39;, [])), \u0026#39;metrics\u0026#39;: { \u0026#39;total_time_ms\u0026#39;: round(total_time * 1000) } } # Save to DynamoDB (bandup-flashcard-sets table) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ.get(\u0026#39;DYNAMODB_FLASHCARD_SETS\u0026#39;)) table.put_item(Item={ \u0026#39;set_id\u0026#39;: request.get(\u0026#39;set_id\u0026#39;), \u0026#39;user_id\u0026#39;: request.get(\u0026#39;user_id\u0026#39;), \u0026#39;document_id\u0026#39;: s3_key, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;flashcards\u0026#39;: json.dumps(response_body[\u0026#39;flashcards\u0026#39;]), \u0026#39;total_cards\u0026#39;: response_body[\u0026#39;total_cards\u0026#39;], \u0026#39;page_count\u0026#39;: index_result[\u0026#39;page_count\u0026#39;], \u0026#39;chunk_count\u0026#39;: index_result[\u0026#39;chunk_count\u0026#39;], \u0026#39;created_at\u0026#39;: int(time.time()) }) if is_async: return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;OK\u0026#39;} return create_response(200, response_body) Titan Embeddings with Parallel Processing:\nclass TitanEmbeddings: \u0026#34;\u0026#34;\u0026#34;Amazon Titan Text Embeddings V2 via Bedrock with parallel processing.\u0026#34;\u0026#34;\u0026#34; MODEL_ID = \u0026#34;amazon.titan-embed-text-v2:0\u0026#34; def __init__(self, region: str = None): self.region = region or os.environ.get(\u0026#39;BEDROCK_REGION\u0026#39;, \u0026#39;us-east-1\u0026#39;) self._client = None @property def client(self): if self._client is None: self._client = boto3.client(\u0026#39;bedrock-runtime\u0026#39;, region_name=self.region) return self._client def embed(self, text: str) -\u0026gt; List[float]: \u0026#34;\u0026#34;\u0026#34;Get embedding for single text using Titan V2.\u0026#34;\u0026#34;\u0026#34; response = self.client.invoke_model( modelId=self.MODEL_ID, body=json.dumps({ \u0026#34;inputText\u0026#34;: text[:8000], # Max input length \u0026#34;dimensions\u0026#34;: 512, \u0026#34;normalize\u0026#34;: True }), contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34; ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;embedding\u0026#39;] def embed_batch_parallel(self, texts: List[str], max_workers: int = 10) -\u0026gt; List[List[float]]: \u0026#34;\u0026#34;\u0026#34;Embed multiple texts in PARALLEL using ThreadPoolExecutor.\u0026#34;\u0026#34;\u0026#34; embeddings = [None] * len(texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: futures = {executor.submit(self.embed, t): i for i, t in enumerate(texts)} for future in as_completed(futures): idx = futures[future] embeddings[idx] = future.result() return embeddings RAG Pipeline (In-Memory):\nimport math import fitz # PyMuPDF class RAG: \u0026#34;\u0026#34;\u0026#34;Lightweight RAG using Titan Embeddings + in-memory cosine similarity.\u0026#34;\u0026#34;\u0026#34; def __init__(self, api_key: str, chunk_size: int = 3000, chunk_overlap: int = 300): self.api_key = api_key self.chunk_size = chunk_size self.chunk_overlap = chunk_overlap self._chunks = [] self._embeddings = [] self._titan = TitanEmbeddings() self._keywords = [] def index_document(self, pdf_path: str, document_id: str = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Index PDF with Titan V2 embeddings (parallel processing).\u0026#34;\u0026#34;\u0026#34; # Load PDF pages pages = [] with fitz.open(pdf_path) as doc: for page_num, page in enumerate(doc): text = page.get_text() if text.strip(): pages.append({\u0026#39;content\u0026#39;: text, \u0026#39;page\u0026#39;: page_num + 1}) # Chunk text with overlap self._chunks = [] for page in pages: chunks = self._chunk_text(page[\u0026#39;content\u0026#39;]) for chunk in chunks: self._chunks.append({ \u0026#39;text\u0026#39;: chunk, \u0026#39;page\u0026#39;: page[\u0026#39;page\u0026#39;] }) # Extract keywords for smart query generation all_text = \u0026#34; \u0026#34;.join([c[\u0026#39;text\u0026#39;] for c in self._chunks]) self._keywords = self._extract_keywords(all_text, top_n=20) # Generate embeddings in parallel (10 concurrent Bedrock calls) texts = [c[\u0026#39;text\u0026#39;] for c in self._chunks] self._embeddings = self._titan.embed_batch_parallel(texts, max_workers=10) return { \u0026#39;page_count\u0026#39;: len(pages), \u0026#39;chunk_count\u0026#39;: len(self._chunks), \u0026#39;keywords\u0026#39;: self._keywords[:10] } def _cosine_similarity(self, a: List[float], b: List[float]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Calculate cosine similarity between two vectors.\u0026#34;\u0026#34;\u0026#34; dot_product = sum(x * y for x, y in zip(a, b)) norm_a = math.sqrt(sum(x * x for x in a)) norm_b = math.sqrt(sum(x * x for x in b)) if norm_a == 0 or norm_b == 0: return 0.0 return dot_product / (norm_a * norm_b) def similarity_search(self, query: str, top_k: int = 5) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for similar chunks using in-memory cosine similarity.\u0026#34;\u0026#34;\u0026#34; query_embedding = self._titan.embed(query) # Calculate similarities similarities = [] for i, embedding in enumerate(self._embeddings): score = self._cosine_similarity(query_embedding, embedding) similarities.append((i, score)) # Sort by similarity (descending) and return top-k similarities.sort(key=lambda x: x[1], reverse=True) results = [] for rank, (idx, score) in enumerate(similarities[:top_k]): chunk = self._chunks[idx] results.append({ \u0026#39;text\u0026#39;: chunk[\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: chunk[\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: score, \u0026#39;rank\u0026#39;: rank + 1 }) return results def generate_smart_queries(self, num_queries: int = 5) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Generate document-specific queries using extracted keywords.\u0026#34;\u0026#34;\u0026#34; kw = self._keywords queries = [] if len(kw) \u0026gt;= 2: queries.append(f\u0026#34;definition and explanation of {kw[0]} and {kw[1]}\u0026#34;) if len(kw) \u0026gt;= 4: queries.append(f\u0026#34;key concepts about {kw[2]} {kw[3]}\u0026#34;) if len(kw) \u0026gt;= 6: queries.append(f\u0026#34;important information regarding {kw[4]} {kw[5]}\u0026#34;) return queries[:num_queries] def retrieve_with_smart_queries(self, top_k_per_query: int = 3) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Retrieve chunks using multiple smart queries for better coverage.\u0026#34;\u0026#34;\u0026#34; queries = self.generate_smart_queries() seen_texts = set() all_results = [] for query in queries: results = self.similarity_search(query, top_k=top_k_per_query) for r in results: if r[\u0026#39;text\u0026#39;] not in seen_texts: seen_texts.add(r[\u0026#39;text\u0026#39;]) all_results.append(r) return sorted(all_results, key=lambda x: x[\u0026#39;score\u0026#39;], reverse=True) def get_representative_chunks(self, num_chunks: int = 10) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Get evenly distributed chunks across document.\u0026#34;\u0026#34;\u0026#34; if len(self._chunks) \u0026lt;= num_chunks: return [{\u0026#39;text\u0026#39;: c[\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: c[\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: 1.0} for c in self._chunks] step = len(self._chunks) // num_chunks return [{\u0026#39;text\u0026#39;: self._chunks[i * step][\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: self._chunks[i * step][\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: 1.0} for i in range(num_chunks)] Flashcard Generation Prompt:\ndef generate_flashcards_prompt(chunks: List[Dict], num_cards: int, difficulty: str, question_types: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Build prompt for Gemini flashcard generation.\u0026#34;\u0026#34;\u0026#34; context = \u0026#34;\\n\\n\u0026#34;.join([ f\u0026#34;[Chunk {i+1}] (Page {c.get(\u0026#39;page\u0026#39;, \u0026#39;?\u0026#39;)}):\\n{c[\u0026#39;text\u0026#39;]}\u0026#34; for i, c in enumerate(chunks) ]) return f\u0026#34;\u0026#34;\u0026#34;Based on the following document excerpts, generate {num_cards} flashcards. CONTEXT: {context} REQUIREMENTS: - Difficulty: {difficulty} - Generate exactly {num_cards} flashcards - Each flashcard should have a clear question and concise answer - Focus on key concepts, definitions, and important facts - Use these question types: {\u0026#34;, \u0026#34;.join(question_types)} OUTPUT FORMAT (JSON): {{ \u0026#34;flashcards\u0026#34;: [ {{ \u0026#34;question\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DEFINITION\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;{difficulty}\u0026#34;, \u0026#34;source_chunk\u0026#34;: 1 }} ] }} Return ONLY valid JSON.\u0026#34;\u0026#34;\u0026#34; def call_gemini(prompt: str, api_key: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Call Gemini API for flashcard generation.\u0026#34;\u0026#34;\u0026#34; import google.generativeai as genai genai.configure(api_key=api_key) model = genai.GenerativeModel( model_name=os.environ.get(\u0026#39;GEMINI_MODEL\u0026#39;, \u0026#39;gemini-2.0-flash\u0026#39;), generation_config={ \u0026#39;temperature\u0026#39;: 0.3, \u0026#39;max_output_tokens\u0026#39;: 4096 } ) response = model.generate_content(prompt) text = response.text # Extract JSON if wrapped in markdown if \u0026#39;```json\u0026#39; in text: text = text.split(\u0026#39;```json\u0026#39;)[1].split(\u0026#39;```\u0026#39;)[0] return json.loads(text.strip()) Lambda Function 4: S3 Upload Handler Generates presigned URLs for secure file uploads to S3.\nSetting Value Function name bandup-s3-upload Runtime Python 3.11 Memory 256 MB Timeout 30 seconds Trigger API Gateway (sync) Core Implementation:\nimport json import os import boto3 from datetime import datetime from typing import Dict, Any s3_client = boto3.client(\u0026#39;s3\u0026#39;) def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Generate presigned URL for S3 upload.\u0026#34;\u0026#34;\u0026#34; request = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) user_id = request.get(\u0026#39;user_id\u0026#39;) filename = request.get(\u0026#39;filename\u0026#39;) content_type = request.get(\u0026#39;content_type\u0026#39;, \u0026#39;application/octet-stream\u0026#39;) upload_type = request.get(\u0026#39;upload_type\u0026#39;, \u0026#39;general\u0026#39;) # Determine bucket based on upload type bucket_map = { \u0026#39;speaking_audio\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_AUDIO\u0026#39;), \u0026#39;flashcard_pdf\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_DOCUMENTS\u0026#39;), \u0026#39;writing_essay\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_DOCUMENTS\u0026#39;), } bucket = bucket_map.get(upload_type) # Generate organized S3 key timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) key = f\u0026#34;uploads/{upload_type}/{user_id}/{timestamp}_{filename}\u0026#34; # Generate presigned PUT URL (15 min expiry) upload_url = s3_client.generate_presigned_url( \u0026#39;put_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Key\u0026#39;: key, \u0026#39;ContentType\u0026#39;: content_type}, ExpiresIn=900 ) # Generate presigned GET URL (1 hour expiry) get_url = s3_client.generate_presigned_url( \u0026#39;get_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Key\u0026#39;: key}, ExpiresIn=3600 ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;upload_url\u0026#39;: upload_url, \u0026#39;get_url\u0026#39;: get_url, \u0026#39;file_url\u0026#39;: f\u0026#34;s3://{bucket}/{key}\u0026#34;, \u0026#39;expires_in\u0026#39;: 900 }) } Secure Secrets Management All Lambda functions use AWS Secrets Manager to retrieve API keys:\n# secrets_helper.py (in Lambda Layer) import boto3 import os from functools import lru_cache @lru_cache(maxsize=1) def get_gemini_api_key() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Retrieve Gemini API key from Secrets Manager (cached).\u0026#34;\u0026#34;\u0026#34; client = boto3.client(\u0026#39;secretsmanager\u0026#39;) secret_arn = os.environ.get(\u0026#39;GEMINI_API_KEY_SECRET_ARN\u0026#39;) response = client.get_secret_value(SecretId=secret_arn) return response[\u0026#39;SecretString\u0026#39;] Security Best Practices:\nNever hardcode API keys in Lambda code Use AWS Secrets Manager for all sensitive credentials Rotate secrets regularly using automatic rotation Use IAM roles with least-privilege permissions IAM Role for Lambda Functions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;BedrockAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;bedrock:InvokeModel\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:*:*:foundation-model/amazon.titan-embed-text-v2*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:*:*:table/bandup-evaluations\u0026#34;, \u0026#34;arn:aws:dynamodb:*:*:table/bandup-flashcard-sets\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3Access\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::bandup-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SQSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sqs:ReceiveMessage\u0026#34;, \u0026#34;sqs:DeleteMessage\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:*:*:bandup-*-queue\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;secretsmanager:GetSecretValue\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:*:*:secret:bandup/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:log-group:/aws/lambda/bandup-*\u0026#34; } ] } DynamoDB Tables Lambda functions store results in two DynamoDB tables:\nTable Used By Purpose bandup-evaluations Writing + Speaking Evaluators Stores IELTS band scores, feedback, transcripts bandup-flashcard-sets Flashcard Generator Stores generated flashcards and document metadata Evaluations Table Schema (Writing \u0026amp; Speaking):\n# Used by Writing Evaluator table.put_item(Item={ \u0026#39;evaluation_id\u0026#39;: session_id, # Partition Key \u0026#39;user_id\u0026#39;: user_id, # Sort Key \u0026#39;evaluation_type\u0026#39;: \u0026#39;writing\u0026#39;, # \u0026#39;writing\u0026#39; or \u0026#39;speaking\u0026#39; \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;overall_band\u0026#39;: \u0026#39;7.0\u0026#39;, \u0026#39;task_achievement_band\u0026#39;: \u0026#39;7.0\u0026#39;, # Writing only \u0026#39;fluency_band\u0026#39;: \u0026#39;6.5\u0026#39;, # Speaking only \u0026#39;pronunciation_band\u0026#39;: \u0026#39;7.0\u0026#39;, # Speaking only \u0026#39;transcript\u0026#39;: \u0026#39;...\u0026#39;, # Speaking only \u0026#39;feedback\u0026#39;: json.dumps(feedback), \u0026#39;created_at\u0026#39;: timestamp }) Flashcard Sets Table Schema:\n# Used by Flashcard Generator table.put_item(Item={ \u0026#39;set_id\u0026#39;: set_id, # Partition Key \u0026#39;user_id\u0026#39;: user_id, # Sort Key \u0026#39;document_id\u0026#39;: document_id, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;flashcards\u0026#39;: json.dumps(flashcards), \u0026#39;total_cards\u0026#39;: 10, \u0026#39;page_count\u0026#39;: 5, \u0026#39;chunk_count\u0026#39;: 12, \u0026#39;created_at\u0026#39;: timestamp }) Environment Variables Variable Description Example GEMINI_API_KEY_SECRET_ARN Secrets Manager ARN arn:aws:secretsmanager:...:secret:bandup/gemini-api-key DYNAMODB_EVALUATIONS Evaluations table (Writing + Speaking) bandup-evaluations DYNAMODB_FLASHCARD_SETS Flashcard sets table bandup-flashcard-sets S3_BUCKET_AUDIO Audio bucket bandup-audio-bucket S3_BUCKET_DOCUMENTS Documents bucket bandup-documents-bucket BEDROCK_REGION Bedrock region for Titan us-east-1 RAG_CHUNK_SIZE Chunk size for RAG 3000 RAG_CHUNK_OVERLAP Chunk overlap 300 GEMINI_MODEL Gemini model name gemini-2.0-flash Deploy Lambda Functions cd rag_flashcard pip install -r requirements.txt -t package/ cp lambda_handler.py rag_pipeline.py package/ cd package \u0026amp;\u0026amp; zip -r ../function.zip . \u0026amp;\u0026amp; cd .. # Create Lambda function aws lambda create-function \\ --function-name bandup-flashcard-generator \\ --runtime python3.11 \\ --handler lambda_handler.lambda_handler \\ --role arn:aws:iam::${AWS_ACCOUNT_ID}:role/bandup-lambda-role \\ --timeout 600 \\ --memory-size 1024 \\ --zip-file fileb://function.zip \\ --environment Variables=\u0026#34;{ GEMINI_API_KEY_SECRET_ARN=arn:aws:secretsmanager:${AWS_REGION}:${AWS_ACCOUNT_ID}:secret:bandup/gemini-api-key, BEDROCK_REGION=us-east-1, RAG_CHUNK_SIZE=3000 }\u0026#34; # Add SQS trigger aws lambda create-event-source-mapping \\ --function-name bandup-flashcard-generator \\ --event-source-arn arn:aws:sqs:${AWS_REGION}:${AWS_ACCOUNT_ID}:bandup-flashcard-queue \\ --batch-size 1 Next Steps Proceed to DynamoDB to configure the database tables.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #2 — DevOps on AWS Event Purpose Reinforce the DevOps mindset and cultural principles. Present AWS services that support CI/CD: CodeCommit, CodeBuild, CodeDeploy, CodePipeline. Deep-dive into Infrastructure as Code (IaC) with CloudFormation and AWS CDK. Explore container services (ECS, EKS, App Runner) and observability tools (CloudWatch, X-Ray). Highlights Morning: CI/CD Pipeline \u0026amp; Infrastructure as Code (08:30 – 12:00) DevOps mindset: emphasize core principles and DORA metrics (Deployment Frequency, MTTR, etc.). CI/CD services: Source control: AWS CodeCommit and Git strategies (GitFlow vs. trunk-based). Build \u0026amp; test: AWS CodeBuild. Deployment: AWS CodeDeploy (Blue/Green, Canary, Rolling strategies). Orchestration: automate pipelines with AWS CodePipeline. Demo: walkthrough of a full CI/CD pipeline. IaC: AWS CloudFormation: templates, stacks, and drift detection. AWS CDK: constructs, reusable patterns, and multi-language support. Demo \u0026amp; discussion: deploying with both CloudFormation and CDK; discussion about tool choices. Afternoon: Containers, Observability \u0026amp; Best Practices (13:00 – 17:00) Container services: Docker: fundamentals of containerization. Amazon ECR: image storage, scanning, and lifecycle management. ECS \u0026amp; EKS: deployment strategies, scaling, orchestration. App Runner: an alternative PaaS for containers. Monitoring \u0026amp; observability: AWS CloudWatch: metrics, logs, alarms, dashboards. AWS X-Ray: distributed tracing for microservices. Best practices: Feature flags, A/B testing. Incident management and postmortems. Demo \u0026amp; case study: comparing deployment strategies for microservices. Key Takeaways DevOps culture: understand DORA metrics and the automation mindset. CI/CD: how AWS Code services integrate; advanced deployment strategies (Blue/Green, Canary). IaC: foundational knowledge of CloudFormation and CDK—useful for optimizing templates and multi-stack architectures. Observability: the importance of full-stack monitoring with CloudWatch and X-Ray to debug issues (CORS, Lambda, etc.). Container services (ECS/EKS) provide guidance for more complex future architectures. Practical demos on CI/CD \u0026amp; IaC helped the team see how to optimize deployment workflows. Event Experience The full-day workshop was detailed and practical, providing applicable knowledge. Demos and case studies were helpful for project application. Good networking opportunities with the community and experts. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives Keep pace with the team\u0026rsquo;s learning progress on AWS services. Master AWS Transit Gateway setup and configuration. Deepen understanding of Amazon EC2 and related compute services. Learn Git fundamentals for effective team collaboration. Workshop: Begin VPC \u0026amp; Network Setup for the Bandup IELTS infrastructure. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Explore AWS Transit Gateway: concepts, setup process, and required resources.\n- Compare differences between VPC Peering and Transit Gateway.\n- Complete: Centralized Network Management with AWS Transit Gateway. 29/09/2025 30/09/2025 AWS Transit Gateway 3 - Deep dive into Amazon EC2 through Module 3 lectures.\n- Study EC2 Auto Scaling for automated resource management.\n- Complete: Scaling Applications with EC2 Auto Scaling. 01/10/2025 02/10/2025 FCJ Playlist 4 - Learn and practice Git commands (commit, push, pull) for team collaboration.\n- Explore Amazon Lightsail for simplified compute solutions.\n- Complete: Simplified Computing with Amazon Lightsail. 03/10/2025 04/10/2025 Git Tutorial 5 - Propose ideas and assign tasks to team members for project proposal.\n- Study migration strategies for AWS.\n- Complete: VM Migration with AWS VM Import/Export.\n- Workshop Activity: Create VPC with CIDR 10.0.0.0/16 and configure DNS support. 05/10/2025 06/10/2025 Team Meeting, Workshop 5.3 AWS Skill Builder Courses Completed Course Category Status Centralized Network Management with AWS Transit Gateway Networking ✅ Scaling Applications with EC2 Auto Scaling Compute ✅ Simplified Computing with Amazon Lightsail Compute ✅ Container Deployment with Amazon Lightsail Containers Containers ✅ VM Migration with AWS VM Import/Export Migration ✅ Database Migration with AWS DMS and SCT Migration ✅ Disaster Recovery with AWS Elastic Disaster Recovery Reliability ✅ Monitoring with Amazon CloudWatch Operations ✅ Week 4 Achievements Technical Skills Acquired:\nAWS Transit Gateway:\nMastered Transit Gateway setup and configuration Understood key advantages over VPC Peering: Supports complex multi-VPC topologies (hub-and-spoke model) Enables transitive routing between connected networks Simplifies network management at scale Supports VPN and Direct Connect attachments Learned Transit Gateway route table management Amazon EC2 Deep Dive:\nComprehensive understanding of EC2 key features: Elasticity: Scale resources up/down based on demand Flexible configurations: Multiple instance types for various workloads Cost optimization: On-Demand, Reserved, Spot instance pricing models Mastered EC2 Auto Scaling for automated resource adjustment Understood Instance Store as ephemeral block storage for EC2 Explored Amazon Lightsail as a simplified solution for small-scale applications Learned about Lightsail Containers for easy container deployment Migration Services:\nUnderstood AWS Application Migration Service (MGN) for server migration Learned VM Import/Export for virtual machine migration to AWS Explored Database Migration Service (DMS) and Schema Conversion Tool (SCT) Studied disaster recovery strategies with AWS Elastic Disaster Recovery DevOps and Monitoring:\nProficient in Git commands (commit, push, pull) and team workflows Learned CloudWatch fundamentals for monitoring AWS resources Team Collaboration:\nSuccessfully proposed ideas and assigned tasks for project proposal Team is prepared to begin implementation phase Established clear roles and responsibilities for each team member Workshop Progress - VPC \u0026amp; Network Setup:\nCreated VPC with CIDR 10.0.0.0/16 in ap-southeast-1 region Designed subnet architecture: Public subnets (10.0.1.0/24, 10.0.2.0/24) and Private subnets for App (10.0.11.0/24, 10.0.12.0/24) and DB (10.0.21.0/24, 10.0.22.0/24) across two AZs Configured Internet Gateway for public subnet internet access Set up route tables for proper traffic routing Began security group configuration for multi-tier architecture Key Takeaways:\nTransit Gateway is essential for managing complex multi-VPC architectures EC2 Auto Scaling ensures applications can handle variable load efficiently Lightsail is perfect for simple workloads without AWS complexity Migration services provide multiple paths for moving workloads to AWS VPC design with separate public/private subnets provides security isolation Multi-AZ deployment ensures high availability from the network layer "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-setup-be/5.5.4-task/","title":"Create Service &amp; Task","tags":[],"description":"","content":"In the final step of the backend deployment, we define the runtime configuration for the Spring Boot application and launch it as a stable ECS Service.\n1. Create Task Definition Navigate to Amazon ECS \u0026gt; Task definitions \u0026gt; Create new task definition. Task definition configuration: Family: bandup-backend. Launch type: AWS Fargate. OS/Architecture: Linux/X86_64. Task size: 1 vCPU and 2 GB Memory. Note: Java applications (Spring Boot) generally require more memory than Node.js apps to handle the JVM heap effectively. Task Role \u0026amp; Execution Role: Select ecsTaskExecutionRole. Container Details:\nName: bandup-be-container. Image URI: Enter the ECR URI (.../band-up-backend:v1.0.0). Container Port: 8080 (Default Spring Boot port). Environment Configuration (Best Practice): Instead of manually entering sensitive variables (Database URL, User, Password) in plain text, we load them from a secure file stored in S3.\nEnvironment files: Add the S3 ARN of your .env file (e.g., arn:aws:s3:::bandup2025-fcj/.env). Requirement: Ensure your ecsTaskExecutionRole has permission to read this S3 object. 2. Create ECS Service Deploy the task definition to the cluster.\nNavigate to bandup-cluster \u0026gt; Services \u0026gt; Create. Deployment configuration: Compute options: FARGATE. Family: bandup-backend (Revision 7 or latest). Service name: bandup-backend-service. Desired tasks: 1. Networking: VPC: band-up-vpc. Subnets: Select Private Subnets (private-subnet-1, private-subnet-2). Security group: Select ecs-backend-sg (Created in step 5.5.2). Load Balancing: Load balancer: Select bandup-public-alb. Listener: Use existing listener 80:HTTP. Target group: Create a new target group target-bandup-be. Container info: Ensure traffic is routed to port 8080. Click Create. The service will provision the Fargate tasks, pull the image, load the environment variables from S3, and register with the ALB. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup-fe/5.4.4-task/","title":"Create Task Definition &amp; Service","tags":[],"description":"","content":"In this final step for the frontend, we define how our application container should run (Task Definition) and deploy it as a scalable service (ECS Service) connected to our Load Balancer.\n1. Create Task Definition The Task Definition serves as a blueprint for our application.\nNavigate to Amazon ECS \u0026gt; Task definitions \u0026gt; Create new task definition. Task definition configuration: Task definition family: bandup-frontend. Launch type: AWS Fargate. OS/Architecture: Linux/X86_64. Task size: .5 vCPU and 1 GB Memory (Sufficient for our Next.js frontend). Task Role \u0026amp; Task Execution Role: Select ecsTaskExecutionRole (Created in section 5.3.3). Container details: Name: bandup-fe-container. Image URI: Enter the ECR URI we pushed earlier (e.g., .../band-up-frontend:v1.0.0). Container port: 3000. Click Create. 2. Create ECS Service Now we deploy this blueprint into our Cluster.\nNavigate to Clusters \u0026gt; Select bandup-cluster. In the Services tab, click Create. Step 1: Environment\nCompute options: Launch type -\u0026gt; FARGATE. Task definition: bandup-frontend (Revision 1). Service name: bandup-frontend-service. Desired tasks: 1. Step 2: Networking\nVPC: band-up-vpc. Subnets: Select the Private Subnets (private-subnet-1, private-subnet-2). Security group: Select ecs-private-sg (This allows traffic from ALB). Step 3: Load Balancing\nLoad balancer type: Application Load Balancer. Load balancer: Select bandup-public-alb. Container to load balance: bandup-fe-container 3000:3000. Listener: Create a new listener on Port 80 (HTTP). Target group: Use an existing target group -\u0026gt; target-bandup-fe. Click Create. The service will start deploying your container. Wait until the status changes to Active and the Task status is Running. 3. Verify Deployment Once the service is stable, open your web browser and navigate to the DNS name of your Application Load Balancer.\nYou should see the IELTS BandUp landing page loading successfully, served from your container in the private subnet.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup-fe/","title":"Frontend Deployment (ECS Fargate)","tags":[],"description":"","content":"Overview In this section, we will deploy the IELTS BandUp Frontend (Next.js application) to the AWS Cloud.\nWe will utilize Amazon Elastic Container Service (ECS) with the Fargate launch type. This serverless approach allows us to run containers without managing the underlying EC2 instances. The frontend service will be placed in Private Subnets for security but will be accessible to users via the Application Load Balancer (ALB) we configured in the previous section.\nImplementation Steps To successfully deploy the frontend, we will follow this workflow:\nContainer Registry (ECR): Create a repository to store our Docker images and push the local application code to AWS. Security Configuration: Define specific security group rules allowing the Frontend container to receive traffic only from the ALB. ECS Task \u0026amp; Service: Define the blueprint (Task Definition) for our container (CPU, Memory, Environment Variables) and launch it as a stable Service. Content Dockerize Application Setup ECR \u0026amp; Push Image Configure Security Group Create Task Definition \u0026amp; Service "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-network/5.3.4-endpoints/","title":"VPC Endpoints Setup","tags":[],"description":"","content":"To ensure security, our backend services running in Private Subnets should not access key AWS services over the public internet. Instead, we use AWS PrivateLink (VPC Endpoints) to keep this traffic within the AWS network.\nWe will create 4 Endpoints:\nInterface Endpoints: For ECR (Docker \u0026amp; API) and CloudWatch Logs. Gateway Endpoint: For Amazon S3. 1. Create Interface Endpoints (ECR \u0026amp; CloudWatch) We will start by creating the endpoint for ECR Docker (ecr.dkr). The process is identical for ECR API (ecr.api) and CloudWatch (logs).\nStep 1: Service Selection\nNavigate to VPC Dashboard \u0026gt; Endpoints \u0026gt; Create endpoint. Name tag: ecr-endpoint (for Docker). Service category: Select AWS services. Services: Search for ecr.dkr and select com.amazonaws.ap-southeast-1.ecr.dkr. Step 2: VPC \u0026amp; Subnets\nVPC: Select band-up-vpc. Subnets: Select the Availability Zones and choose the Private Subnets (private-app-subnet-1 and private-app-subnet-2). Note: This creates Elastic Network Interfaces (ENIs) in your private subnets to serve as entry points. Step 3: Security Group\nSecurity groups: Select the Security Group that allows HTTPS (Port 443) traffic from your VPC. For this workshop, you can use the default security group if it allows inbound traffic from within the VPC. Click Create endpoint. Step 4: Repeat for ECR API and CloudWatch Repeat the steps above to create two more Interface Endpoints:\nECR API: Search for ecr.api -\u0026gt; Name: ecr-api-endpoint. CloudWatch Logs: Search for logs -\u0026gt; Name: cloudwatch-endpoint. 2. Create Gateway Endpoint (S3) For Amazon S3, we use a Gateway Endpoint, which is cost-effective and uses routing tables instead of network interfaces.\nClick Create endpoint. Name tag: s3-endpoint. Services: Search for s3 and select com.amazonaws.ap-southeast-1.s3 (Type: Gateway). VPC: Select band-up-vpc. Route tables: Select the Route Tables associated with your Private Subnets. Click Create endpoint. 3. Verify All Endpoints Once completed, navigate to the Endpoints list. You should see 4 active endpoints ensuring secure connectivity for your infrastructure.\necr-endpoint (Interface) ecr-api-endpoint (Interface) cloudwatch-endpoint (Interface) s3-endpoint (Gateway) "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 — Kick-off: The First Cloud Journey (FCJ) Date: 06/09/2025 Location: Online Role: Attendee Summary: Official kick-off for the 12-week FCJ program: introductions, program vision, roadmap overview, team formation guidance, and AWS account security setup (2FA, budgets). Outcome / Lessons Learned: Clear understanding of program expectations, the importance of discipline and commitment, and initial AWS security best practices. Event 2 — DX Talk#7: Reinventing DevSecOps with AWS Generative AI Date: 16/10/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: Strategic and practical perspectives on integrating Generative AI into DevSecOps; case studies from CMC Global and AWS; an overview of CI/CD and security tooling (Jenkins, SonarQube, OWASP ZAP, Terraform) and Amazon Q Developer. Outcome / Lessons Learned: Insights into AI-enabled DevSecOps workflows, automated security checks, and tools to integrate into pipelines. Event 3 — AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS Date: 15/11/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: Overview of AI/ML in Vietnam and AWS services (Amazon SageMaker); introduction to Generative AI via Amazon Bedrock, prompt engineering and Retrieval-Augmented Generation (RAG); live demo building a GenAI chatbot. Outcome / Lessons Learned: Gained practical knowledge of SageMaker and Bedrock, RAG techniques, and GenAI prototyping. Event 4 — AWS Cloud Mastery Series #2: DevOps on AWS Date: 17/11/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: DevOps culture and practices; AWS CI/CD services; Infrastructure as Code (CloudFormation, CDK); container services (ECS, EKS, App Runner); observability (CloudWatch, X-Ray); and pipeline demos. Outcome / Lessons Learned: Practical experience with CI/CD patterns, IaC best practices, container strategies, and monitoring for cloud applications. Event 5 — AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar Date: 29/11/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: Security Pillar deep dive: Identity \u0026amp; Access Management, detection (GuardDuty, CloudTrail), infrastructure protection, data protection (KMS, Secrets Manager), and incident response automation. Outcome / Lessons Learned: Reinforced security fundamentals, IAM best practices, data encryption, and incident response playbooks and automation. Event 6 — Vietnam Cloud Day HCMC Connect Edition Date: 18/09/2025 Location: Ho Chi Minh City (main hall) / Online Role: Attendee Summary: Track 1 — AI, Data \u0026amp; Infrastructure Modernization: building data foundations for AI, GenAI adoption roadmap on AWS, AI-Driven Development Lifecycle (AI-DLC), GenAI application security, and AI Agents. Outcome / Lessons Learned: Practical guidance for designing AI-ready data platforms, an understanding of GenAI tools and services (e.g., Bedrock), security considerations for GenAI workloads, and architectural trade-offs (Serverless vs Container). Event 7 — AI-Driven Development Life Cycle: Reimagining Software Engineering Date: 18/09/2025 Location: Ho Chi Minh City / Online Role: Attendee Summary: Introduction to AI-Driven Development Life Cycle (AI-DLC), a transformative methodology that positions AI as a central collaborator throughout software development. The event covered the three phases (Inception, Construction, Operations), AI-powered execution with human oversight, and introduced Kiro—an AI-native IDE featuring spec-driven development, hooks for automated oversight, and synchronization between specs and code. Outcome / Lessons Learned: Understood how AI-DLC reimagines software engineering by placing AI at the center of development, learned about the three-phase structure with AI-human collaboration, recognized potential for development cycles to shift from weeks to hours/days, and gained insights into Kiro\u0026rsquo;s production-ready tooling that bridges prototyping and production systems. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-ai-service/5.6.4-dynamodb/","title":"DynamoDB","tags":[],"description":"","content":"Overview Create two DynamoDB tables to store Lambda function results:\nTable Used By Purpose bandup-evaluations Writing + Speaking Evaluators Stores IELTS band scores, feedback, transcripts bandup-flashcard-sets Flashcard Generator Stores generated flashcards and document metadata Table 1: Evaluations Table Stores results from both Writing Evaluator and Speaking Evaluator Lambda functions.\nSetting Value Table name bandup-evaluations Partition key evaluation_id (String) Sort key user_id (String) Billing mode On-demand (PAY_PER_REQUEST) Table Schema:\nAttribute Type Description evaluation_id String Unique session ID (PK) user_id String User identifier (SK) evaluation_type String writing or speaking status String processing, completed, failed overall_band String Overall IELTS band score (e.g., \u0026ldquo;7.0\u0026rdquo;) task_achievement_band String Writing only coherence_band String Writing only lexical_band String Both Writing and Speaking grammar_band String Both Writing and Speaking fluency_band String Speaking only pronunciation_band String Speaking only transcript String Speaking only - transcribed audio feedback String JSON-encoded detailed feedback model_used String AI model used for evaluation created_at Number Unix timestamp Example Item (Writing):\n{ \u0026#34;evaluation_id\u0026#34;: \u0026#34;eval-abc123\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;evaluation_type\u0026#34;: \u0026#34;writing\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;overall_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;task_achievement_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;coherence_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;lexical_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;grammar_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;feedback\u0026#34;: \u0026#34;{\\\u0026#34;strengths\\\u0026#34;:[...],\\\u0026#34;weaknesses\\\u0026#34;:[...]}\u0026#34;, \u0026#34;model_used\u0026#34;: \u0026#34;gemini-writing_task2\u0026#34;, \u0026#34;created_at\u0026#34;: 1733644800 } Example Item (Speaking):\n{ \u0026#34;evaluation_id\u0026#34;: \u0026#34;speak-xyz789\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;evaluation_type\u0026#34;: \u0026#34;speaking\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;overall_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;fluency_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;lexical_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;grammar_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;pronunciation_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;transcript\u0026#34;: \u0026#34;Well, I\u0026#39;d like to talk about...\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;120.5\u0026#34;, \u0026#34;word_count\u0026#34;: 185, \u0026#34;feedback\u0026#34;: \u0026#34;{\\\u0026#34;fluency\\\u0026#34;:{...},\\\u0026#34;pronunciation\\\u0026#34;:{...}}\u0026#34;, \u0026#34;model_used\u0026#34;: \u0026#34;gemini-2.5-flash-audio\u0026#34;, \u0026#34;created_at\u0026#34;: 1733644800 } Table 2: Flashcard Sets Table Stores results from Flashcard Generator Lambda function.\nSetting Value Table name bandup-flashcard-sets Partition key set_id (String) Sort key user_id (String) Billing mode On-demand (PAY_PER_REQUEST) Table Schema:\nAttribute Type Description set_id String Unique flashcard set ID (PK) user_id String User identifier (SK) document_id String Source document S3 key status String processing, completed, failed flashcards String JSON-encoded array of flashcards total_cards Number Number of flashcards generated page_count Number Number of pages in source PDF chunk_count Number Number of text chunks indexed created_at Number Unix timestamp Example Item:\n{ \u0026#34;set_id\u0026#34;: \u0026#34;flashcard-set-123\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;document_id\u0026#34;: \u0026#34;uploads/documents/user-456/vocab-guide.pdf\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;flashcards\u0026#34;: \u0026#34;[{\\\u0026#34;question\\\u0026#34;:\\\u0026#34;What is...\\\u0026#34;,\\\u0026#34;answer\\\u0026#34;:\\\u0026#34;...\\\u0026#34;}]\u0026#34;, \u0026#34;total_cards\u0026#34;: 15, \u0026#34;page_count\u0026#34;: 8, \u0026#34;chunk_count\u0026#34;: 24, \u0026#34;created_at\u0026#34;: 1733644800 } Create Tables with AWS CLI # Create evaluations table (Writing + Speaking) aws dynamodb create-table \\ --table-name bandup-evaluations \\ --attribute-definitions \\ AttributeName=evaluation_id,AttributeType=S \\ AttributeName=user_id,AttributeType=S \\ --key-schema \\ AttributeName=evaluation_id,KeyType=HASH \\ AttributeName=user_id,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --tags Key=Project,Value=bandup Key=Environment,Value=production # Create flashcard sets table aws dynamodb create-table \\ --table-name bandup-flashcard-sets \\ --attribute-definitions \\ AttributeName=set_id,AttributeType=S \\ AttributeName=user_id,AttributeType=S \\ --key-schema \\ AttributeName=set_id,KeyType=HASH \\ AttributeName=user_id,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --tags Key=Project,Value=bandup Key=Environment,Value=production Enable Point-in-Time Recovery Enable PITR for data protection:\n# Enable PITR for evaluations table aws dynamodb update-continuous-backups \\ --table-name bandup-evaluations \\ --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true # Enable PITR for flashcard sets table aws dynamodb update-continuous-backups \\ --table-name bandup-flashcard-sets \\ --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true Query Patterns Get user\u0026rsquo;s evaluation history:\nresponse = table.query( IndexName=\u0026#39;user_id-created_at-index\u0026#39;, # If GSI exists KeyConditionExpression=Key(\u0026#39;user_id\u0026#39;).eq(\u0026#39;user-456\u0026#39;), ScanIndexForward=False, # Most recent first Limit=10 ) Get specific evaluation by ID:\nresponse = table.get_item( Key={ \u0026#39;evaluation_id\u0026#39;: \u0026#39;eval-abc123\u0026#39;, \u0026#39;user_id\u0026#39;: \u0026#39;user-456\u0026#39; } ) Get user\u0026rsquo;s flashcard sets:\nresponse = table.query( KeyConditionExpression=Key(\u0026#39;user_id\u0026#39;).eq(\u0026#39;user-456\u0026#39;), FilterExpression=Attr(\u0026#39;status\u0026#39;).eq(\u0026#39;completed\u0026#39;) ) Lambda Environment Variables Configure Lambda functions to use these tables:\nLambda Function Environment Variable Value Writing Evaluator DYNAMODB_EVALUATIONS bandup-evaluations Speaking Evaluator DYNAMODB_EVALUATIONS bandup-evaluations Flashcard Generator DYNAMODB_FLASHCARD_SETS bandup-flashcard-sets IAM Policy for Lambda Access { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/bandup-evaluations\u0026#34;, \u0026#34;arn:aws:dynamodb:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/bandup-flashcard-sets\u0026#34; ] } ] } Next Steps Proceed to Bedrock Integration to configure Amazon Titan Embeddings.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #3 — AWS Well-Architected Security Pillar Event Purpose Introduce the role of the Security Pillar within the AWS Well-Architected Framework. Present the five core pillars of cloud security: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. Provide best practices and practical playbooks to protect cloud applications. Highlights Pillar 1 — Identity \u0026amp; Access Management (08:50 – 09:30) Principles: Least Privilege, Zero Trust, Defense in Depth. Modern IAM: avoid long-term credentials; prefer Roles and Policies. IAM Identity Center: SSO and management of Permission Sets. Multi-account security: SCPs (Service Control Policies) and Permission Boundaries. Mini demo: validate IAM policies and simulate access. Pillar 2 — Detection (09:30 – 09:55) Continuous monitoring: CloudTrail (organization-level), GuardDuty, Security Hub. Logging at all layers: VPC Flow Logs, ALB/S3 logs. Automated alerting: using EventBridge. Pillar 3 — Infrastructure Protection (10:10 – 10:40) Network security: VPC segmentation (private vs. public). Defenses: Security Groups vs. NACLs; using WAF, Shield, Network Firewall. Workload security: securing EC2, basics for ECS/EKS. Pillar 4 — Data Protection (10:40 – 11:10) Encryption: encryption at rest \u0026amp; in transit (S3, EBS, RDS, DynamoDB). Key and secret management: KMS, Secrets Manager, Parameter Store. Data classification and access guardrails. Pillar 5 — Incident Response (11:10 – 11:40) IR lifecycle: AWS-recommended incident response processes. IR playbook \u0026amp; automation. Sample scenarios: compromised IAM key, public S3 exposure, EC2 malware detection. Automated response using Lambda / Step Functions. What I Learned Understand the five Security Pillars and the Shared Responsibility Model. Advanced IAM: using IAM Identity Center, SCPs, and avoiding long-term credentials. Data security: the importance of KMS and managing secrets. Incident Response: building playbooks and automating responses with serverless. Event Experience The workshop served as the final summary session in the series, providing essential security knowledge before project completion. The IAM Identity Center and Secrets Manager presentations helped address Sub ID authentication issues and API key management for the team. IR scenarios (e.g., S3 public exposure) were valuable for reinforcing project security policies. The final Q\u0026amp;A helped outline the next learning path (Security Specialty). "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives Identify and resolve abnormal AWS costs on the account. Design and partition infrastructure architecture for the project. Begin initial project configuration and assign team roles. Explore AWS Skill Builder and advance learning on optimization topics. Workshop: Complete VPC Network Setup and begin Database \u0026amp; Storage Setup. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Analyze and identify causes of abnormal costs on AWS account.\n- Complete: Cost and Usage Management and Managing Quotas with Service Quotas. 07/10/2025 08/10/2025 AWS Cost Explorer 3 - Design and partition project infrastructure architecture.\n- Propose basic architecture templates for team reference.\n- Complete: Building Highly Available Web Applications.\n- Workshop Activity: Configure NAT Gateways in public subnets and complete Security Groups setup. 09/10/2025 10/10/2025 FCJ Community, Workshop 5.3 4 - Build code skeleton and configure initial project files.\n- Set up development environment.\n- Complete: Development Environment with AWS Toolkit for VS Code.\n- Workshop Activity: Begin Database \u0026amp; Storage Setup - plan RDS PostgreSQL and ElastiCache Redis configurations. 11/10/2025 13/10/2025 VS Code + AWS Toolkit, Workshop 5.6 5 - Register for AWS Skill Builder and explore courses.\n- Study EC2 optimization techniques.\n- Complete: Right-Sizing with EC2 Resource Optimization. 11/10/2025 12/10/2025 AWS Skill Builder AWS Skill Builder Courses Completed Course Category Status Cost and Usage Management Cost Optimization ✅ Managing Quotas with Service Quotas Operations ✅ Billing Console Delegation Cost Management ✅ Right-Sizing with EC2 Resource Optimization Cost Optimization ✅ Development Environment with AWS Toolkit for VS Code Development ✅ Building Highly Available Web Applications Architecture ✅ Database Essentials with Amazon RDS Database ✅ NoSQL Database Essentials with Amazon DynamoDB Database ✅ In-Memory Caching with Amazon ElastiCache Database ✅ Command Line Operations with AWS CLI Operations ✅ Week 5 Achievements Technical Skills Acquired:\nCost Optimization:\nIdentified causes of abnormal AWS costs: Incomplete deletion of EC2 resources (EBS volumes, Elastic IPs) Lack of control over user accounts and IAM permissions Resources left running in unused regions Learned AWS cost management best practices: AWS Budgets for proactive cost alerts Cost Explorer for analyzing spending patterns Service Quotas for managing account limits Billing Console Delegation for team cost visibility Proposed cost optimization measures for the team Architecture Design:\nSuccessfully designed project infrastructure architecture Created reference architecture templates for team adoption Applied High Availability principles: Multi-AZ deployments Load balancing strategies Database replication patterns Fault-tolerant design patterns Development Environment:\nSet up AWS Toolkit for VS Code for streamlined development Mastered AWS CLI for command-line operations Built robust code skeleton with initial configuration files Established project foundation for team collaboration Database Services:\nUnderstood Amazon RDS for relational database needs Learned DynamoDB for NoSQL workloads Explored ElastiCache for in-memory caching (Redis/Memcached) Applied database selection criteria based on use cases Project Progress:\nRegistered and activated AWS Skill Builder account Began exploring advanced courses and learning paths Infrastructure architecture finalized and documented Development environment configured and ready for coding Workshop Progress - Network \u0026amp; Database Setup:\nCompleted NAT Gateway deployment in both public subnets for private subnet internet access Configured Security Groups for ALB, ECS, RDS, and ElastiCache tiers with least-privilege access Set up route tables: Public routes to Internet Gateway, Private routes to NAT Gateways Designed RDS PostgreSQL Multi-AZ configuration for high availability Planned ElastiCache Redis cluster for session management and caching Configured S3 buckets for static assets and document storage Key Takeaways:\nCost optimization starts with visibility - use Cost Explorer daily Right-sizing EC2 instances can reduce costs by 30-50% High availability requires planning across multiple AZs AWS Toolkit for VS Code significantly improves developer productivity Database selection depends on data model, scale, and access patterns Service Quotas prevent unexpected capacity limitations NAT Gateways enable private subnet resources to access internet securely Security Groups provide defense-in-depth with multiple layers of protection "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-setup-be/","title":"Backend Deployment (ECS Fargate)","tags":[],"description":"","content":"Overview In this section, we will deploy the IELTS BandUp Backend, a Spring Boot application that serves as the core logic layer of the platform.\nUnlike the Frontend, the Backend requires persistent storage and caching mechanisms to function effectively. Therefore, before launching the application containers on ECS Fargate, we must provision the data infrastructure (PostgreSQL and Redis). The Backend service will reside in Private Subnets, strictly protected by Security Groups, and will communicate with the AI services via AWS SDK.\nImplementation Steps To deploy the fully functional backend system, we will follow this sequence:\nContainer Registry (ECR): Build the Spring Boot application and push the Docker image to a private ECR repository. Relational Database (RDS): Provision an Amazon RDS for PostgreSQL instance to store user data, test results, and content. In-Memory Cache (ElastiCache): Set up an Amazon ElastiCache (Redis) cluster for session management and high-speed data retrieval. ECS Task \u0026amp; Service: Define the backend task configuration (including environment variables for DB connections) and launch the service. Content Setup ECR \u0026amp; Push Image Create PostgreSQL RDS Create ElastiCache (Redis) Create Service \u0026amp; Task "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"IELTS Self-Learning Web System - AWS Infrastructure Workshop Overview This comprehensive workshop guides you through building a production-ready AWS infrastructure for the IELTS Self-Learning Web System. You will learn how to deploy a highly available, scalable, and secure web application using modern AWS services and best practices.\nThe architecture implements an active-passive Multi-AZ deployment pattern on Amazon ECS, with a serverless AI service layer for intelligent assessment and content generation.\nWhat You Will Build By completing this workshop, you will have deployed:\nComponent AWS Service Purpose Network Layer VPC, Subnets, NAT Gateway Isolated, secure network infrastructure Container Platform ECS Fargate, ECR Serverless container orchestration Load Balancing ALB, Route 53, ACM Traffic distribution and SSL termination Data Layer RDS PostgreSQL, ElastiCache, S3 Relational database, caching, object storage AI Services API Gateway, SQS, Lambda, DynamoDB Serverless AI processing pipeline CI/CD CodePipeline, CodeBuild Automated deployment pipeline Security IAM, Secrets Manager, WAF Identity management and protection Monitoring CloudWatch Logs, Alarms Observability and alerting Architecture Highlights High Availability Design:\nMulti-AZ deployment across two Availability Zones Active-passive failover for ECS services RDS Multi-AZ with automatic failover Application Load Balancer with health checks Serverless AI Architecture:\nAPI Gateway for RESTful AI endpoints SQS for asynchronous message processing Lambda functions for Writing Assessment, Speaking Assessment, and RAG-based Flashcard Generation DynamoDB for storing AI results Amazon Bedrock integration for AI models (Gemma 3 12B, Titan Embeddings) Google Gemini API for smart query generation Security Best Practices:\nPrivate subnets for application and database tiers Security groups with least-privilege access AWS WAF for application-level protection Secrets Manager for credential management IAM roles with minimal required permissions Prerequisites Before starting this workshop, ensure you have:\nAn AWS account with appropriate permissions AWS CLI installed and configured Basic understanding of AWS services (VPC, EC2, ECS) Docker installed locally for container builds Git for version control Time to Complete Section Estimated Time Prerequisites 15 minutes VPC \u0026amp; Network Setup 30 minutes ECS \u0026amp; Container Setup 45 minutes Load Balancer Configuration 30 minutes Database \u0026amp; Storage Setup 45 minutes AI Service Architecture 60 minutes CI/CD Pipeline 30 minutes Security \u0026amp; IAM 30 minutes Monitoring Setup 20 minutes Total ~5 hours Content Workshop Overview Prerequisites VPC \u0026amp; Network Setup ECS \u0026amp; Container Setup Load Balancer Configuration Database \u0026amp; Storage Setup AI Service Architecture CI/CD Pipeline Security \u0026amp; IAM Monitoring \u0026amp; Logging Clean Up "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-ai-service/5.6.5-bedrock-integration/","title":"Bedrock Integration","tags":[],"description":"","content":"Overview Configure Amazon Bedrock for AI model access including Gemma 3 12B and Titan Embeddings.\nEnable Model Access Navigate to Amazon Bedrock → Model access Request access to: Amazon Titan Text Express (for assessments) Amazon Titan Embeddings V2 (for RAG) Meta Llama 3 or Anthropic Claude (optional) Test Bedrock API import boto3 import json bedrock = boto3.client(\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # Test Titan Text response = bedrock.invoke_model( modelId=\u0026#39;amazon.titan-text-express-v1\u0026#39;, body=json.dumps({ \u0026#39;inputText\u0026#39;: \u0026#39;Hello, how are you?\u0026#39;, \u0026#39;textGenerationConfig\u0026#39;: { \u0026#39;maxTokenCount\u0026#39;: 100, \u0026#39;temperature\u0026#39;: 0.7 } }) ) print(json.loads(response[\u0026#39;body\u0026#39;].read())) Titan Embeddings for RAG # Generate embeddings response = bedrock.invoke_model( modelId=\u0026#39;amazon.titan-embed-text-v2:0\u0026#39;, body=json.dumps({ \u0026#39;inputText\u0026#39;: \u0026#39;Document chunk text here\u0026#39; }) ) embedding = json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embedding\u0026#39;] # Store in OpenSearch or use for similarity search Google Gemini Integration For smart query generation:\nimport google.generativeai as genai genai.configure(api_key=os.environ[\u0026#39;GEMINI_API_KEY\u0026#39;]) model = genai.GenerativeModel(\u0026#39;gemini-2.5-flash\u0026#39;) response = model.generate_content( f\u0026#34;\u0026#34;\u0026#34;Analyze this document and generate 10 intelligent questions: {document_text} \u0026#34;\u0026#34;\u0026#34; ) Cost Optimization Use Titan Text Express for assessments (lower cost) Batch embeddings generation where possible Implement caching for repeated queries Use Google Gemini free tier for query generation Next Steps Proceed to CI/CD Pipeline.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives Master fundamental AWS storage services and their use cases. Enhance Python programming skills through practical exercises. Design and refine the project\u0026rsquo;s infrastructure architecture. Attend the \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; webinar to explore DevSecOps practices and Amazon Q Developer. Workshop: Complete Database \u0026amp; Storage Setup and configure S3 for static assets. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Study Amazon S3 fundamentals: Bucket architecture, durability guarantees, and static website hosting capabilities.\n- Explore S3 Storage Classes (Standard, Standard-IA) and Amazon Glacier for cold storage solutions.\n- Complete: Static Website Hosting with Amazon S3. 14/10/2025 15/10/2025 AWS S3 Documentation 3 - Learn AWS Storage Gateway types (File, Volume, Tape Gateway) and their integration patterns.\n- Understand Object Lifecycle Management policies for cost optimization.\n- Practice Python fundamentals: data structures, functions, and error handling.\n- Attend webinar: \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; featuring Hoàng Kha. 16/10/2025 17/10/2025 AWS Storage Gateway\nAWS Events 4 - Study disaster recovery concepts: RTO, RPO, and Backup \u0026amp; Restore strategies.\n- Explore AWS Backup service for centralized backup management.\n- Hands-on: Create S3 buckets, upload files, configure static website hosting, and test lifecycle policies.\n- Research DevSecOps methodologies: CI/CD pipelines, SAST/DAST tools, Infrastructure as Code.\n- Workshop Activity: Deploy RDS PostgreSQL Multi-AZ instance and ElastiCache Redis cluster. 18/10/2025 19/10/2025 AWS Backup, Workshop 5.6 5 - Finalize project infrastructure architecture diagram with detailed component relationships.\n- Restructure code skeleton to align with the updated architecture design.\n- Standardize programming language and framework selection for team consistency.\n- Explore Amazon Q Developer capabilities: AI-powered code generation, testing, and vulnerability scanning. 20/10/2025 21/10/2025 Amazon Q Developer AWS Skill Builder Courses Completed Course Category Status Static Website Hosting with Amazon S3 Storage ✅ Data Protection with AWS Backup Reliability ✅ Content Delivery with Amazon CloudFront Networking ✅ Week 6 Achievements Storage Services Mastery:\nComprehensive understanding of Amazon S3 architecture: Buckets, durability (99.999999999%), and static website hosting Mastered S3 Storage Classes: Standard, Standard-IA, Glacier for different access patterns Learned AWS Storage Gateway integration patterns for hybrid cloud storage Understood Object Lifecycle Management for automated data tiering and cost optimization Disaster Recovery \u0026amp; Backup:\nGrasped disaster recovery fundamentals: RTO (Recovery Time Objective) and RPO (Recovery Point Objective) Learned AWS Backup service for centralized backup management across services Understood Backup \u0026amp; Restore strategies for business continuity Development Skills:\nEnhanced Python programming through practical exercises Successfully created S3 buckets, configured static websites, and tested lifecycle policies Improved understanding of data structures and error handling Project Planning:\nFinalized comprehensive infrastructure architecture diagram Restructured code skeleton with proper directory structure Standardized technology stack for team collaboration DevSecOps Insights:\nAttended \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; webinar (October 16, 2025) Learned DevSecOps integration: Security in SDLC using Jenkins (CI/CD), SonarQube (SAST), OWASP ZAP (DAST), Terraform (IaC) Explored Amazon Q Developer: AI assistant for code generation, testing, vulnerability scanning, and AWS optimization Workshop Progress - Database \u0026amp; Storage:\nDeployed RDS PostgreSQL Multi-AZ instance in private DB subnets for high availability Configured ElastiCache Redis cluster for session management and application caching Set up S3 buckets for static website assets, user uploads, and document storage Configured S3 lifecycle policies for cost optimization (transitioning to Glacier) Established database connection strings and caching endpoints for application integration Implemented database backup strategies using AWS Backup service Key Takeaways:\nS3 is the foundation for object storage in AWS - understanding storage classes is crucial for cost optimization Lifecycle policies automate data management and reduce storage costs significantly AWS Backup provides unified backup management across multiple AWS services DevSecOps integrates security throughout the development lifecycle, not as an afterthought RDS Multi-AZ provides automatic failover for database high availability ElastiCache significantly improves application performance through in-memory caching Private subnets for databases provide additional security layer "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Post-Event Report — Vietnam Cloud Day HCMC Connect Edition Event Objective Provide a strategic vision for Digital Transformation and the role of the Cloud in modernizing infrastructure. Focus on Track 1: AI, Data \u0026amp; Infrastructure Modernization — introducing AWS methodologies and services to build AI and data foundations. Create networking and learning opportunities with top industry experts. Speaker List (afternoon session) Jun Kai Loke — AI/ML Specialist SA, AWS Kien Nguyen — Solutions Architect, AWS Tamelly Lim — Storage Specialist SA, AWS Binh Tran — Senior Solutions Architect, AWS Taiki Dang — Solutions Architect, AWS Michael Armentano — Principal WW GTM Specialist, AWS Key Highlights (Track 1 — Afternoon Session) Building a Unified Data Foundation for AI and Analytics Topic: Building a Unified Data Foundation on AWS for AI and Analytics Workloads Content: Strategies and best practices for constructing a unified, scalable data foundation on AWS — covering Data Ingestion, Storage, Processing, and Governance. Generative AI (GenAI) Outlook and Future on AWS Topic: Building the Future: Gen AI Adoption and Roadmap on AWS Content: AWS\u0026rsquo;s comprehensive vision, emerging trends, and strategic roadmap for Generative AI adoption; services supporting innovation via GenAI. AI-Driven Development Lifecycle (AI-DLC) Topic: AI-Driven Development Lifecycle (AI-DLC) — Shaping the future of Software Implementation Content: Introduction to AI-DLC — integrating AI as a central collaborator in the software development lifecycle (SDLC) to significantly improve speed, quality, and innovation. Securing Generative AI Applications Topic: Securing Generative AI Applications with AWS: Fundamentals and Best Practices Content: Exploring unique security challenges at every layer of the GenAI stack; strategies including encryption, zero-trust architecture, continuous monitoring, and fine-grained access controls. AI Agents: Your Ultimate Productivity Multipliers Topic: Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers Content: Discussing the paradigm shift where AI agents act as intelligent partners that learn, adapt, and autonomously execute complex tasks to exponentially multiply productivity. Key Takeaways / Value Gained AI / Data Foundation: Understood how to construct a robust Data Foundation to prepare data for AI projects, including storage and processing services. GenAI Vision: Gained insight into the AWS GenAI roadmap and core services like Bedrock. AI-Driven Development: Recognized the potential of integrating AI into the SDLC to accelerate development speed and quality. GenAI Security: Learned specialized security strategies for the GenAI stack (zero-trust, encryption, fine-grained access control). Network Modernization: Reinforced knowledge of Transit Gateway and network topology simplification, useful for the VPC Peering lab. Compute Decision: Learned criteria for choosing between Serverless and Container — helping validate the team\u0026rsquo;s choice of Serverless (SAM). Event Experience Cutting-Edge Knowledge: Direct access to experts in GenAI and AI-DLC, providing insight into the latest technology trends. High Applicability: Sessions on Data Foundation and Securing GenAI are directly applicable to the project (AI image processing flow, data security). Energy and Inspiration: The large-scale event provided significant motivation and helped clarify market needs and the Cloud sector\u0026rsquo;s development potential. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-ai-service/","title":"AI Service Architecture","tags":[],"description":"","content":"Overview This section covers the serverless AI service architecture using API Gateway, SQS, Lambda, DynamoDB, and Amazon Bedrock for intelligent assessment and content generation.\nAI Service Architecture The AI service implements a fully serverless pattern:\nUser → API Gateway → SQS Queue → Lambda → AI Model → DynamoDB → Response Three Lambda Functions:\nFunction Purpose AI Model Writing Evaluate IELTS writing assessment Gemma 3 12B / Gemini Speaking Evaluate Audio transcription + assessment Transcribe + Gemma 3 12B Flashcard Generate RAG-based flashcard creation Titan Embeddings + Gemini Content API Gateway SQS Queues Lambda Functions DynamoDB Bedrock Integration Request Flow User submits request (writing sample, audio, document) API Gateway validates and enqueues message to SQS SQS triggers appropriate Lambda function Lambda processes with AI model (Bedrock/Gemini) Results stored in DynamoDB User retrieves results via API Estimated Time: ~60 minutes Cost Estimate Monthly Cost Summary:\nUpfront Cost Monthly Cost Total 12 Months Cost Currency $0.00 $23.61 $283.32 USD Note: Includes upfront cost\nDetailed Cost Breakdown:\nService Description Region Monthly Cost (USD) Annual Cost (USD) AWS Lambda Writing Evaluator Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda Speaking Evaluator Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda Evaluation Status Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda S3 Upload Asia Pacific (Singapore) $0.00 $0.00 Amazon API Gateway HTTP API Asia Pacific (Singapore) $0.42 $5.04 S3 Standard Audio bucket Asia Pacific (Singapore) $0.51 $6.12 S3 Standard Documents Bucket Asia Pacific (Singapore) $0.53 $6.36 DynamoDB Evaluations Table Asia Pacific (Singapore) $0.37 $4.44 DynamoDB Flashcard Sets Table Asia Pacific (Singapore) $0.52 $6.24 Amazon SQS Writing/Speaking/Flashcard queues Asia Pacific (Singapore) $0.00 $0.00 AWS Secrets Manager Secrets management Asia Pacific (Singapore) $0.45 $5.40 Amazon CloudWatch RAG Lambda logs Asia Pacific (Singapore) $0.01 $0.08 Amazon Bedrock Bedrock inference US East (N. Virginia) $0.50 $6.00 OpenAI GPT inference US East (N. Virginia) $20.30 $243.65 Total $23.61 $283.32 AWS Pricing Calculator provides only an estimate of your AWS fees and doesn\u0026rsquo;t include any taxes that might apply. Your actual fees depend on a variety of factors, including your actual usage of AWS services.\nView detailed cost breakdown: AWS Pricing Calculator\n"},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Internship Overview During my internship at Amazon Web Services (AWS) from September 8 to December 9, 2024, I had the invaluable opportunity to transform from a traditional backend developer into a cloud-native engineer.\nI served as a Backend Developer for the Bandup IELTS Learning Platform - a comprehensive AI-powered application. My primary focus was building a robust, scalable backend infrastructure using Java Spring Boot, integrating it deeply with AWS services to handle automated Writing and Speaking evaluations and intelligent study material generation.\nKey Achievements Through this project, I significantly advanced my expertise in:\nJava Spring Boot \u0026amp; Cloud-Native Development:\nRESTful API Design: Developed and documented complex REST APIs using Spring Web to serve frontend requests and orchestrate AI workflows. Data Persistence: Implemented efficient data access layers using Spring Data JPA/Hibernate with Amazon RDS and DynamoDB. Security: Integrated Spring Security with AWS services for robust authentication and authorization. Asynchronous Processing: Built event-driven features using Spring Boot Messaging to consume and produce messages via Amazon SQS, ensuring decoupling between user requests and heavy AI processing tasks. Cloud Architecture \u0026amp; AWS Integration:\nContainerization: Dockerized Spring Boot applications and managed deployments using Amazon ECS (Elastic Container Service) and AWS Fargate for serverless container execution. AWS SDK for Java: Successfully integrated the AWS SDK to programmatically interact with services like Amazon S3 (storage), ElastiCache (Redis) (caching), and Amazon Bedrock. Serverless Integration: Orchestrated workflows where the Java backend triggers AWS Lambda functions for specific micro-tasks. AI/ML Integration (Backend Perspective):\nOrchestration: Designed the backend logic to act as the central hub, routing user audio/text data to Google Gemini API and Amazon Bedrock and processing the returned analysis. Cost Optimization Logic: Implemented backend logic to optimize token usage and API calls (achieving ~72% cost savings). Vector Database Interaction: Managed integration with Vector Stores for RAG pipelines using Java clients. Work Ethics Throughout my internship, I consistently strived to:\nWrite clean, maintainable, and well-documented Java code following industry standards. Follow AWS Well-Architected Framework principles for security and reliability. Actively communicate with mentors to bridge the gap between application logic and infrastructure. Ensure thorough testing (Unit Testing with JUnit/Mockito) before deployment. Self-Evaluation Criteria To objectively reflect on my internship period, I evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Proficiency in Java Spring Boot, REST APIs, Microservices, and AWS SDK integration ✅ ☐ ☐ 2 Ability to learn Quickly adapting to Cloud-Native Java patterns and new AWS services (ECS, Bedrock) ✅ ☐ ☐ 3 Proactiveness researching best practices for deploying Spring Boot on AWS without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Delivering stable APIs, handling database migrations, and meeting deployment deadlines ✅ ☐ ☐ 5 Discipline Adhering to coding standards (Clean Code), commit conventions, and work schedules ☐ ✅ ☐ 6 Progressive mindset Willingness to refactor code based on reviews and optimize performance ✅ ☐ ☐ 7 Communication Explaining backend architecture and API contracts clearly to frontend and mobile teams ☐ ✅ ☐ 8 Teamwork Collaborating effectively with AI engineers to integrate models into the Java backend ✅ ☐ ☐ 9 Professional conduct Respecting data privacy best practices and maintaining API security ✅ ☐ ☐ 10 Problem-solving skills Troubleshooting JVM memory issues in containers, optimizing database queries ✅ ☐ ☐ 11 Contribution to project/team Delivered the core backend system, handled critical AI integrations, and ensured system stability ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Key Learnings Technical Skills Gained:\nSpring Cloud AWS: Deep understanding of how to build Java applications that run natively on the cloud. Container Orchestration: Practical experience with Docker and Amazon ECS/Fargate. Hybrid Architectures: Connecting stateful backend services (Spring Boot) with stateless serverless functions (Lambda). System Design: Designing scalable database schemas and API patterns for high-traffic applications. Soft Skills Developed:\nTechnical documentation writing (API Swagger/OpenAPI) Problem decomposition and systematic debugging in distributed systems Communicating technical constraints to non-technical stakeholders Working in an agile enterprise environment Areas for Improvement Discipline: Strengthen time management and adhere strictly to development sprints. Communication: Improve ability to explain complex backend concepts (like concurrency or eventual consistency) to the frontend team. Testing: Increase code coverage by implementing more comprehensive integration tests. Networking: Build stronger professional relationships within the AWS and Java developer communities. Gratitude I would like to express my sincere gratitude to:\nMy mentors at AWS for guiding me on how to build enterprise-grade cloud applications. The team for their support in integrating the backend with AI services. AWS for providing the infrastructure to learn and practice modern cloud development. This internship has been a transformative experience, bridging the gap between my academic Java knowledge and real-world Cloud Native development. I am confident that these skills will be the foundation of my future career as a Backend Cloud Engineer.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.7-event7/","title":"Event 7","tags":[],"description":"","content":"AI-Driven Development Life Cycle: Reimagining Software Engineering Event Objective Introduces the AI-Driven Development Life Cycle (AI-DLC), a transformative approach to software development that positions AI as a central collaborator throughout the software development process. The methodology aims to transform software development by leveraging AI to handle routine tasks while maintaining human oversight, ultimately freeing developers to focus on critical problem-solving and creative solutions. Introduces Kiro, an AI IDE that helps developer deliver from concept to production through a simplified developer experience for working with AI agents. Kiro is great at ‘vibe coding’ but goes way beyond that—Kiro’s strength is getting those prototypes into production systems with features such as specs and hooks. Speaker List (afternoon session) Toan Huynh\nMy Nguyen\nKey Highlights Part 1: AI-Driven Development Life Cycle (AI-DLC) Core Philosophy AI-Powered Execution with Human Oversight: AI-DLC proposes a middle ground between \u0026ldquo;AI-assisted\u0026rdquo; development (improving specific tasks) and \u0026ldquo;AI-autonomous\u0026rdquo; development (attempting to remove humans entirely). Mental Model: AI initiates workflows by creating plans, seeking clarification, and implementing solutions. Humans focus on providing context, validating plans, and making critical decisions. Dynamic Team Collaboration: Routine tasks are offloaded to AI, allowing human teams to focus on creative problem-solving and rapid decision-making in collaborative spaces. The Three Phases of AI-DLC The methodology restructures the traditional Software Development Life Cycle (SDLC) into three highly interactive phases:\nInception (Mob Elaboration):\nAI transforms business intent into detailed requirements and user stories. The entire team actively validates the AI\u0026rsquo;s proposals and answers clarifying questions to ensure alignment with business goals. Construction (Mob Construction):\nAI proposes logical architectures, domain models, code solutions, and tests. The team provides real-time feedback on technical decisions and architectural choices. Operations:\nAI uses the accumulated context to manage infrastructure-as-code and deployments. Humans provide oversight to ensure security and stability. Key Benefits Velocity: Development cycles shift from weeks to \u0026ldquo;bolts\u0026rdquo; (hours or days). Quality: Continuous clarification ensures the product matches business intent, while AI enforces organizational standards (security, design patterns). Context Retention: AI maintains persistent context across all phases, ensuring no knowledge is lost between planning and coding. Part 2: Kiro – The Agentic IDE for AI-DLC Kiro is a new AI-native Integrated Development Environment (IDE) designed to bridge the gap between \u0026ldquo;vibe coding\u0026rdquo; (prototyping) and building production-ready software. It creates the tooling necessary to implement methodologies like AI-DLC.\nCore Concept: Spec-Driven Development Kiro addresses the \u0026ldquo;black box\u0026rdquo; problem of AI coding where models make undocumented assumptions. It forces a structured approach where AI builds based on explicit specifications (\u0026ldquo;specs\u0026rdquo;) rather than vague prompts.\nKey Features Specs (The \u0026ldquo;Brain\u0026rdquo;):\nRequirements Generation: Kiro unpacks a single prompt into detailed user stories with acceptance criteria (using EARS notation). Technical Design: It analyzes the codebase to generate data flow diagrams, TypeScript interfaces, and database schemas before writing code. Task Implementation: It breaks down development into sequenced tasks (with dependencies, unit tests, and accessibility requirements) for humans to approve. Hooks (The \u0026ldquo;Guardian\u0026rdquo;):\nEvent-driven automations that act like a senior developer looking over your shoulder. Examples: Automatically updating tests when a component is saved, refreshing READMEs when APIs change, or scanning for security leaks before commits. Hooks enforce team-wide consistency and coding standards automatically. Synchronization: Kiro keeps specs and code in sync. If code changes, Kiro can update the specs, preventing documentation drift.\nCompatibility \u0026amp; Ecosystem VS Code Compatible: Built on Code OSS, allowing developers to keep their existing settings and extensions. Model Context Protocol (MCP): Supports connecting specialized tools and context providers. Key Takeaways / Value Gained AI-DLC Methodology Transformative Approach: Understood how AI-DLC reimagines software engineering by placing AI at the center of the development process rather than treating it merely as an assistant. Three-Phase Structure: Learned how the methodology restructures SDLC into Inception, Construction, and Operations phases with AI-human collaboration at each stage. Velocity and Quality: Recognized the potential for development cycles to shift from weeks to hours/days while maintaining quality through continuous clarification and AI-enforced standards. Context Retention: Appreciated how AI maintains persistent context across all phases, ensuring no knowledge is lost between planning and coding. Kiro IDE Spec-Driven Development: Understood how Kiro addresses the \u0026ldquo;black box\u0026rdquo; problem by forcing AI to build based on explicit specifications rather than vague prompts. Production-Ready Tooling: Learned how Kiro bridges the gap between prototyping (\u0026ldquo;vibe coding\u0026rdquo;) and production-ready software through features like specs and hooks. Automated Oversight: Recognized how hooks act as automated guardians, enforcing team-wide consistency and coding standards. Ecosystem Integration: Appreciated Kiro\u0026rsquo;s compatibility with VS Code and support for Model Context Protocol (MCP). Connection Between AI-DLC and Kiro Methodology and Tool: Understood that while AI-DLC is the methodology (the \u0026ldquo;how\u0026rdquo; and \u0026ldquo;why\u0026rdquo; of organizing teams and workflows around AI), Kiro is a tool (the \u0026ldquo;what\u0026rdquo;) that enables this shift. Operationalization: Recognized how Kiro\u0026rsquo;s \u0026ldquo;Specs\u0026rdquo; feature operationalizes the AI-DLC \u0026ldquo;Inception\u0026rdquo; and \u0026ldquo;Construction\u0026rdquo; phases by forcing AI to plan and seek validation before execution, while \u0026ldquo;Hooks\u0026rdquo; automate the oversight required in the \u0026ldquo;Operations\u0026rdquo; phase. Event Experience The event provided a comprehensive introduction to the future of software engineering through AI-DLC and Kiro:\nRevolutionary Methodology Paradigm Shift: The presentation clearly demonstrated how AI-DLC represents a fundamental shift from traditional AI-assisted development to a collaborative model where AI and humans work as a unified team. Practical Structure: The three-phase approach (Inception, Construction, Operations) provided a clear framework for understanding how AI can be integrated throughout the entire development lifecycle. Innovative Tooling Production Focus: Kiro\u0026rsquo;s emphasis on moving beyond prototyping to production-ready systems was particularly valuable, addressing a common gap in AI coding tools. Spec-Driven Approach: The concept of spec-driven development resonated strongly, as it addresses the critical issue of AI making undocumented assumptions. Real-World Application Team Collaboration: The emphasis on dynamic team collaboration and offloading routine tasks to AI while maintaining human oversight provided practical insights for improving development workflows. Quality Assurance: The integration of hooks for automated oversight and synchronization between specs and code demonstrated how AI can enforce standards while maintaining flexibility. Conclusion The event successfully introduced both the strategic vision (AI-DLC) and the practical tooling (Kiro) needed to transform software development. The combination of methodology and implementation provides a clear path forward for teams looking to leverage AI while maintaining human control and quality standards.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives Focus on comprehensive review and knowledge consolidation in preparation for the mid-term exam. Practice hands-on labs and multiple-choice questions on AWS Builders and AWSboy platforms to familiarize with exam format. Systematize fundamental AWS services learned: EC2, S3, VPC, IAM, RDS, Lambda, DynamoDB. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - Systematize knowledge of Compute services (EC2, Lambda). - Hands-on: Complete exercises/labs on creating, configuring, and managing the lifecycle of EC2 Instances. - Review Lambda function creation, triggers, and execution models. 22/10/2024 22/10/2024 AWS Builders, AWSboy Tuesday - Review knowledge of Storage services (S3, EBS, EFS). - Hands-on: Complete exercises on S3 storage classes (Standard, IA, Glacier), EBS volume types, and EFS use cases. - Practice S3 bucket policies and access control. 23/10/2024 23/10/2024 AWS Builders, AWSboy Wednesday - Consolidate knowledge of Networking (VPC, Subnets, Route Tables, Internet Gateway, Security Groups, NACLs). - Hands-on: Practice questions on VPC configuration, Security Group rules vs NACL rules, and routing principles. - Review VPC Peering and Transit Gateway concepts. 24/10/2024 24/10/2024 AWS Builders, AWSboy Thursday - Review Database services (RDS, DynamoDB) and Security/Identity (IAM). - Hands-on: Focus on fundamental IAM Policies, IAM Roles, and IAM Users concepts. - Practice DynamoDB table design and RDS instance configuration. 25/10/2024 25/10/2024 AWS Builders, AWSboy Friday - Summary and Mock Exams: Take comprehensive practice tests on AWS Builders and AWSboy platforms. - Review weak areas identified during practice tests for further study. - Create summary notes for quick reference before exam. 26/10/2024 26/10/2024 AWS Builders, AWSboy Week 7 Achievements Successfully completed comprehensive review of core AWS service groups: Compute, Storage, Networking, Database, Security (IAM). Successfully practiced numerous labs and multiple-choice questions on AWS Builders and AWSboy platforms. Mastered fundamental parameters of EC2 (Instance Types, AMI, EBS volumes) and S3 operations (Storage Classes, Object/Bucket management). Clearly understood relationships and configuration of components within a VPC (Public/Private Subnets, Routing, Security Groups vs NACLs). Gained confidence in knowledge acquired, ready for the upcoming mid-term exam. Created comprehensive study notes covering all major AWS service categories. Identified and addressed knowledge gaps through targeted practice sessions. Key Takeaways:\nSecurity Groups are stateful (return traffic automatically allowed), NACLs are stateless (bidirectional rules required) EC2 instance types are optimized for different workloads (compute, memory, storage, GPU) S3 storage classes balance cost vs. access frequency requirements IAM policies follow explicit deny principle - most restrictive policy wins VPC routing follows most specific route match principle "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-cicd-pipeline/","title":"CI/CD with CodeBuild &amp; CodePipeline","tags":[],"description":"","content":"CI/CD Pipeline with AWS CodeBuild \u0026amp; CodePipeline This guideline describes how to implement a production-ready CI/CD pipeline with AWS CodePipeline and CodeBuild using GitLab as SCM. When a new Release is created in the GitLab repository, CodePipeline is triggered, CodeBuild runs the frontend and backend projects using the existing frontend-buildspec.yml and backend-buildspec.yml, and then CodePipeline deploys to ECS.\nWhat you’ll do 5.3.1 – Configure CodeBuild projects (frontend/backend) and trigger on GitLab Release 5.3.2 – Design CodePipeline for ECS deploy and integrate post-build artifacts Prerequisites An IAM user/role with permissions for CodeBuild, CodePipeline, S3, ECR (if needed for GitLab token), and IAM pass role. An S3 bucket for pipeline artifacts (will be created by CodePipeline wizard or you can pre-create). ECR repository created. Architecture Overview "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" This section shares my personal experience and feedback from participating in the First Cloud Journey (FCJ) program at AWS, hoping to help the FCJ team continue improving the program for future interns.\nOverall Evaluation 1. Working Environment\nThe working environment at AWS exceeded my expectations. The office is modern, well-equipped, and designed for productivity. What impressed me most was the open culture where everyone, regardless of seniority, is approachable and willing to help. The remote/hybrid work flexibility was also greatly appreciated, allowing me to balance learning and development effectively.\nThe FCJ team created a supportive atmosphere where asking questions was encouraged, not judged. Having access to AWS resources and services for hands-on learning was invaluable - I could experiment with real cloud infrastructure instead of just reading documentation.\n2. Support from Mentor / Team Admin\nThe mentorship I received was exceptional. My mentor didn\u0026rsquo;t just provide answers - they guided me through the problem-solving process, helping me develop critical thinking skills for cloud architecture decisions. Key highlights:\nWeekly 1:1 sessions to review progress and address challenges Code reviews with constructive feedback that improved my coding practices Architecture discussions that helped me understand the \u0026ldquo;why\u0026rdquo; behind design decisions Career guidance on AWS certifications and cloud engineering paths The admin team efficiently handled all logistics - from account access to resource provisioning - so I could focus entirely on learning and building.\n3. Relevance of Work to Academic Major\nThe Bandup IELTS project was perfectly aligned with my computer science background while pushing me into new territories:\nAcademic Knowledge Applied New Skills Learned Python programming AWS Lambda \u0026amp; Serverless architecture Data structures \u0026amp; algorithms RAG pipelines \u0026amp; Vector embeddings Database fundamentals DynamoDB \u0026amp; ElastiCache Networking basics VPC, Subnets, Security Groups Software engineering CI/CD, Infrastructure as Code The project challenged me to integrate AI services (Gemini API, Titan Embeddings) into a production-ready system - something far beyond classroom exercises.\n4. Learning \u0026amp; Skill Development Opportunities\nThe FCJ program provided exceptional learning opportunities:\nHands-on AWS experience with 15+ services (Lambda, API Gateway, SQS, DynamoDB, S3, ECS, Fargate, Bedrock, etc.) Real project ownership - I wasn\u0026rsquo;t just doing small tasks; I built complete Lambda functions from scratch Cost optimization skills - Learned to achieve 72% cost savings through smart architecture decisions Documentation practices - Created this comprehensive workshop guide as a knowledge-sharing artifact Exposure to AI/ML - Integrated cutting-edge AI models for practical applications The self-paced learning combined with mentor guidance struck the perfect balance for skill development.\n5. Company Culture \u0026amp; Team Spirit\nAWS\u0026rsquo;s culture of customer obsession and ownership was evident throughout my internship:\nTeam members genuinely cared about helping me succeed Failures were treated as learning opportunities, not criticism Innovation was encouraged - my suggestions for the Gemini native audio approach (saving 72% costs) were welcomed and implemented The \u0026ldquo;Day 1\u0026rdquo; mentality kept everyone motivated and forward-thinking I felt like a valued contributor, not just \u0026ldquo;the intern.\u0026rdquo; My work mattered, and that made all the difference in my engagement and motivation.\n6. Internship Policies / Benefits\nThe FCJ program offered excellent support:\n✅ Competitive internship stipend ✅ Flexible working hours accommodating student schedules ✅ Access to AWS training resources and certifications ✅ Internal tech talks and learning sessions ✅ Networking opportunities with AWS professionals ✅ Real project experience for portfolio building Reflection Questions What did I find most satisfying during my internship?\nBuilding something real that works. Seeing the Bandup platform evaluate IELTS essays and generate flashcards using AI - knowing I built those Lambda functions - gave me immense satisfaction. The moment the Speaking Evaluator successfully processed audio through Gemini was a highlight I\u0026rsquo;ll never forget.\nWhat could be improved for future interns?\nEarlier AWS account access - Having full access from day 1 would accelerate the learning curve More structured onboarding - A checklist of \u0026ldquo;must-know\u0026rdquo; AWS concepts would help new interns ramp up faster Peer intern collaboration - Pairing interns on complementary projects could enhance learning More exposure to other teams - Shadowing opportunities with different AWS teams would broaden perspective Would I recommend this internship to friends?\nAbsolutely yes. For anyone interested in cloud computing, the FCJ program at AWS is unmatched. You get:\nHands-on experience with industry-leading cloud services Mentorship from experienced professionals Real project ownership and responsibility Skills that are highly valued in the job market The only prerequisite is a genuine willingness to learn and work hard.\nSuggestions \u0026amp; Future Expectations Suggestions to improve the internship experience:\nCreate an intern resource hub - Consolidated documentation, tutorials, and FAQs for common challenges Bi-weekly demo sessions - Interns present their work to the broader team for feedback and visibility AWS certification support - Vouchers or study groups for Cloud Practitioner/Solutions Architect exams Alumni network - Connect current interns with FCJ alumni for career advice Would I like to continue with this program?\nYes! I would love to:\nReturn as a full-time cloud engineer after graduation Mentor future FCJ interns, paying forward the guidance I received Continue contributing to FCJ workshops and documentation Final thoughts:\nThis internship transformed my understanding of cloud computing and AI integration. I arrived knowing basic programming; I leave with the ability to design and implement scalable serverless architectures. The FCJ team didn\u0026rsquo;t just teach me AWS - they showed me what it means to be a professional engineer.\nThank you, AWS and the FCJ team, for this incredible opportunity. 🙏\n\u0026ldquo;The best way to learn is to build. The best way to grow is to be challenged. FCJ provided both.\u0026rdquo;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives Complete the mid-term exam (October 31st) with strong performance. Begin implementing foundational CRUD (Create, Read, Update, Delete) functionalities for the Bandup IELTS project. Research and plan integration of AWS Serverless services (Lambda, API Gateway, DynamoDB) for the project architecture. Set up development environment and establish project structure. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - Final comprehensive review of knowledge in preparation for the mid-term exam. - Review challenging questions and commonly confused concepts (IAM Policies vs Roles, Security Groups vs NACLs, VPC routing). - Practice time management for exam completion. 28/10/2024 28/10/2024 Personal notes, AWS Builders Tuesday - Mental preparation and tool setup for the exam. - Hands-on: Begin setting up development environment for the Bandup IELTS project. - Install and configure Python development tools, AWS CLI, and IDE setup. 29/10/2024 30/10/2024 AWS CLI Documentation Wednesday - Mid-term Exam (October 31st) - Completion of the most important objective. - Post-exam reflection on performance and areas for improvement. 31/10/2024 31/10/2024 Exam Venue Thursday - Begin implementing first basic CRUD functionalities (Create operation: generating flashcard sets). - Research and trial deployment of AWS Lambda functions for serverless compute. - Study DynamoDB table design for storing flashcard data. 01/11/2024 01/11/2024 AWS Lambda \u0026amp; DynamoDB documentation Friday - Plan Serverless architecture integration: + Research API Gateway for RESTful API endpoints. + Design data flow: Frontend → API Gateway → Lambda → DynamoDB. + Define Lambda function structure and event handling patterns. - Implement basic Read functionality to retrieve flashcard sets from DynamoDB. 02/11/2024 02/11/2024 API Gateway documentation, Serverless patterns Week 8 Achievements Completed the mid-term exam (October 31st) successfully. Successfully set up basic development environment for the project with Python, AWS CLI, and IDE configuration. Started building initial Create/Read functionalities for the Bandup IELTS project using AWS Lambda and DynamoDB. Researched and designed serverless architecture pattern: API Gateway for HTTP endpoints Lambda functions for business logic DynamoDB for NoSQL data storage Reinforced knowledge of essential Serverless services (Lambda, DynamoDB, API Gateway) critical for project development. Created initial project structure with proper directory organization. Implemented first Lambda function handler for Create operation. Key Takeaways:\nServerless architecture eliminates server management overhead Lambda functions are event-driven and scale automatically DynamoDB provides single-digit millisecond latency for NoSQL workloads API Gateway acts as the entry point for serverless APIs Proper project structure from the start simplifies future development "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.8-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Overview This section guides you through cleaning up all AWS resources created during this workshop to avoid ongoing charges. Follow the steps in order as some resources depend on others.\nImportant: Deleting resources is irreversible. Make sure you have backed up any data you need before proceeding.\nEstimated Time: ~30 minutes Step 1: Delete CI/CD Pipeline Resources First, delete the CI/CD pipeline to stop any automated deployments.\nNavigate to AWS CodePipeline console Select your pipeline (e.g., ielts-pipeline) Click Delete pipeline → Confirm deletion Navigate to AWS CodeBuild console Delete all build projects associated with the workshop Delete any S3 buckets used for pipeline artifacts Step 2: Delete ECS Services and Cluster Stop and delete all ECS services before deleting the cluster.\nNavigate to Amazon ECS console Select your cluster (e.g., ielts-cluster) Go to Services tab For each service: Select the service Click Update → Set Desired tasks to 0 → Update Wait for running tasks to stop Click Delete service → Confirm After all services are deleted, go back to Clusters Select your cluster → Delete cluster → Confirm Step 3: Delete ECR Repositories Delete container images and repositories.\nNavigate to Amazon ECR console Select each repository: ielts-frontend ielts-backend Click Delete → Type the repository name to confirm Step 4: Delete AI Service Resources Delete serverless AI components in this order:\n4.1 Delete Lambda Functions Navigate to AWS Lambda console Delete each function: writing-evaluator speaking-evaluator flashcard-generator evaluation-status s3-upload Select function → Actions → Delete → Confirm 4.2 Delete API Gateway Navigate to Amazon API Gateway console Select your API (e.g., ielts-ai-api) Click Actions → Delete API → Confirm 4.3 Delete SQS Queues Navigate to Amazon SQS console Delete each queue: writing-evaluation-queue writing-evaluation-dlq speaking-evaluation-queue speaking-evaluation-dlq flashcard-generation-queue flashcard-generation-dlq Select queue → Delete → Confirm 4.4 Delete DynamoDB Tables Navigate to Amazon DynamoDB console Delete each table: evaluations flashcard-sets Select table → Delete → Confirm deletion Step 5: Delete Load Balancer Resources Navigate to EC2 console → Load Balancers Select your ALB (e.g., ielts-alb) Click Actions → Delete → Confirm Go to Target Groups Delete all target groups associated with the workshop Go to Listeners and delete any remaining listeners Step 6: Delete Database Resources 6.1 Delete RDS Instance Navigate to Amazon RDS console Select your database instance Click Actions → Delete Uncheck Create final snapshot (if not needed) Check I acknowledge\u0026hellip; → Delete RDS deletion may take 5-10 minutes to complete.\n6.2 Delete ElastiCache (Redis) Navigate to Amazon ElastiCache console Select your Redis cluster Click Delete → Confirm 6.3 Delete RDS Subnet Group In RDS console, go to Subnet groups Select your subnet group → Delete Step 7: Delete S3 Buckets Navigate to Amazon S3 console For each bucket created in the workshop: ielts-audio-bucket ielts-documents-bucket Any pipeline artifact buckets Select bucket → Empty → Confirm After emptying, select bucket → Delete → Confirm S3 buckets must be emptied before they can be deleted.\nStep 8: Delete Secrets Manager Secrets Navigate to AWS Secrets Manager console Select each secret created for the workshop Click Actions → Delete secret Set recovery window to 7 days (minimum) or choose immediate deletion Confirm deletion Step 9: Delete CloudWatch Resources Navigate to Amazon CloudWatch console Go to Log groups Delete log groups: /aws/lambda/writing-evaluator /aws/lambda/speaking-evaluator /aws/lambda/flashcard-generator /ecs/ielts-frontend /ecs/ielts-backend Go to Alarms and delete any created alarms Step 10: Delete VPC and Network Resources Delete network resources in this specific order:\n10.1 Delete NAT Gateway Navigate to VPC console → NAT Gateways Select your NAT Gateway Click Actions → Delete NAT gateway → Confirm Wait for status to change to Deleted 10.2 Release Elastic IPs Go to Elastic IPs Select any Elastic IPs associated with NAT Gateway Click Actions → Release Elastic IP addresses → Confirm 10.3 Delete VPC Endpoints Go to Endpoints Select all VPC endpoints created for the workshop Click Actions → Delete VPC endpoints → Confirm 10.4 Delete Security Groups Go to Security Groups Delete security groups in this order (due to dependencies): Application security groups first Database security groups Load balancer security groups Do not delete the default security group 10.5 Delete Subnets Go to Subnets Select all subnets in your workshop VPC Click Actions → Delete subnet → Confirm 10.6 Delete Route Tables Go to Route Tables Delete custom route tables (not the main route table) Select route table → Actions → Delete route table 10.7 Delete Internet Gateway Go to Internet Gateways Select your IGW → Actions → Detach from VPC → Confirm Select IGW again → Actions → Delete internet gateway → Confirm 10.8 Delete VPC Go to Your VPCs Select your workshop VPC Click Actions → Delete VPC → Confirm Step 11: Delete IAM Resources Navigate to IAM console Go to Roles and delete: ecsTaskExecutionRole (if created for this workshop) ielts-lambda-execution-role Any other workshop-specific roles Go to Policies and delete custom policies created for the workshop Be careful not to delete IAM resources used by other applications.\nVerification Checklist After completing the cleanup, verify all resources are deleted:\nResource Service Console Status CodePipeline CodePipeline ☐ Deleted ECS Cluster ECS ☐ Deleted ECR Repositories ECR ☐ Deleted Lambda Functions Lambda ☐ Deleted API Gateway API Gateway ☐ Deleted SQS Queues SQS ☐ Deleted DynamoDB Tables DynamoDB ☐ Deleted Load Balancer EC2 ☐ Deleted RDS Instance RDS ☐ Deleted ElastiCache ElastiCache ☐ Deleted S3 Buckets S3 ☐ Deleted Secrets Secrets Manager ☐ Deleted CloudWatch Logs CloudWatch ☐ Deleted NAT Gateway VPC ☐ Deleted VPC VPC ☐ Deleted IAM Roles IAM ☐ Deleted Cost Verification To ensure no unexpected charges:\nGo to AWS Billing console Check Bills for the current month Review Cost Explorer to verify no active resources Set up a Budget alert if you plan to continue using AWS Wait 24-48 hours and check your billing dashboard again to confirm all resources have been cleaned up and no charges are accruing.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives Complete transition to AWS SAM (Serverless Application Model) development framework. Refactor and re-implement CRUD functionalities following SAM architecture patterns. Resolve environment-related issues to achieve successful deployment status on AWS. Integrate Docker for standardized build environment and dependency management. Workshop: ECS \u0026amp; Container Setup - Deploy containerized Frontend and Backend services. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - In-depth research on AWS SAM: Understand template.yaml structure, SAM CLI commands, and how Serverless resources (Lambda, API Gateway) operate within SAM model. - Plan detailed migration strategy: Convert existing Lambda functions to SAM-compatible structure. - Study SAM local testing capabilities (sam local invoke, sam local start-api). 04/11/2024 04/11/2024 AWS SAM Documentation, AWS Study Group Tuesday - Source Code Refactoring: Rewrite CRUD functionalities (Create/Read operations) using SAM patterns (Lambda handlers and API Gateway events). - Docker Integration: Install and configure Docker to ensure consistent Python runtime environment for sam build process. - Create Dockerfile for Lambda layer dependencies. - Workshop Activity: Create ECR repositories for Frontend (Next.js) and Backend (Spring Boot) container images. 05/11/2024 06/11/2024 Docker Documentation, SAM CLI, Workshop 5.4 Wednesday - Local Debugging and Testing: Execute sam local invoke to test individual Lambda functions. - Encountered critical issues in Local environment: Dependency conflicts, Python version mismatches, DynamoDB local connection problems. - Attempt to resolve local testing barriers through configuration adjustments. 06/11/2024 07/11/2024 SAM CLI Error Reports, Stack Overflow Thursday - Strategic Decision: Backend Team decided to adopt deploy-then-test strategy on actual AWS environment to overcome local debugging limitations, accepting calculated risk. - Focus on fixing configuration errors in template.yaml (resource definitions, IAM permissions, environment variables). - Validate SAM template syntax and resource dependencies. 07/11/2024 08/11/2024 CloudFormation Template Validator Friday - Successful Deployment: Executed sam deploy --guided and successfully deployed project to AWS environment. - Basic Verification: Tested created API endpoints using Postman/curl, confirming CRUD functionality is operational. - Document deployment process and configuration for team reference. - Workshop Activity: Build and push Docker images to ECR, create ECS Task Definitions and deploy ECS Services with Fargate. 08/11/2024 08/11/2024 AWS CloudFormation Deployment Logs, Workshop 5.4 Week 9 Achievements Completed technology transition to AWS SAM development model for entire project. Successfully refactored CRUD functionalities into SAM Serverless structure with proper handler organization. Resolved environment issues by using Docker to ensure sam build process uses correct Python version and dependencies. Achieved critical milestone: Successfully deployed project to AWS environment, overcoming local debugging hurdles. The Bandup IELTS project now has working API version on real Cloud environment (though deeper testing still required). Established deployment workflow and best practices for team collaboration. Created comprehensive template.yaml with proper resource definitions and IAM permissions. Workshop Progress - ECS \u0026amp; Container Setup:\nCreated ECR repositories for Frontend (Next.js) and Backend (Spring Boot) container images Built and pushed Docker images to ECR with proper tagging strategy Created ECS Task Definitions with CPU/memory specifications (Frontend: 512 CPU/1024MB, Backend: 1024 CPU/2048MB) Set up ECS Cluster with Fargate capacity providers Deployed ECS Services in active-passive Multi-AZ pattern (2 replicas active, 1 standby) Configured Service Connect for internal service discovery between Frontend and Backend Implemented health checks for automatic task recovery Key Takeaways:\nSAM simplifies serverless application development with infrastructure as code Docker ensures consistent build environments across different development machines Deploy-then-test strategy can be viable when local testing is problematic SAM templates provide single source of truth for serverless infrastructure Proper IAM permissions in SAM templates are critical for Lambda function execution ECS Fargate eliminates server management overhead for containerized applications Multi-AZ deployment ensures high availability for container services Service Connect simplifies internal service communication without load balancers "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"How FINRA established real-time operational observability for big data workloads on Amazon EMR running on Amazon EC2 with Prometheus and Grafana","tags":[],"description":"","content":"Post date: Nov 15, 2024 – Authors: Sumalatha Bachu, PremKiran Bejjam, and Akhil Chalamalasetty in AWS Big Data, Customer Solutions, Monitoring and observability.\nThis is a guest post by FINRA (Financial Industry Regulatory Authority). FINRA is dedicated to protecting investors and safeguarding market integrity in a manner that facilitates vibrant capital markets.\nFINRA processes big data with massive volumes and workloads of varying sizes and instance types on Amazon EMR. Amazon EMR is a cloud big data environment designed for processing large amounts of data using open-source tools such as Hadoop, Spark, HBase, Flink, Hudi, and Presto.\nMonitoring EMR clusters is essential to detect critical issues with applications, infrastructure, or data in real time. A fine-tuned monitoring system helps quickly identify root causes, automate remediation, minimize manual tasks, and increase productivity. Additionally, observing cluster performance and utilization over time helps operations and engineering teams find potential performance bottlenecks and optimization opportunities to right-size their clusters. This, in turn, reduces manual tasks and improves service level agreement (SLA) compliance.\nIn this post, we discuss our challenges and show how we built an observability framework to provide insights into operational metrics for big data workloads on Amazon EMR clusters running on Amazon Elastic Compute Cloud (Amazon EC2).\nChallenges\nIn today’s data-centric world, organizations strive to extract valuable insights from massive amounts of data. The challenge we faced was finding an effective way to monitor and observe big data workloads on Amazon EMR due to their complexity. Monitoring and observing Amazon EMR solutions come with various challenges:\nComplexity and scale – EMR clusters often process massive amounts of data across multiple nodes. Monitoring such a distributed and complex system requires handling high data throughput and minimizing performance impact. Managing and interpreting the large volume of monitoring data generated by EMR clusters can be overwhelming, making it difficult to identify and troubleshoot issues promptly. Dynamic environments – EMR clusters are often ephemeral, created and terminated based on workload requirements. This dynamic nature makes it difficult to monitor, collect metrics, and maintain consistent observability over time. Data diversity – Monitoring cluster health and having a bird\u0026rsquo;s-eye view to detect bottlenecks, anomalous behavior in processing, data skew, and job performance is critical. Detailed observability of long-running clusters, nodes, tasks, potential data skew, stuck tasks, performance issues, and job-level metrics (such as Spark and JVM) is crucial to understand. Achieving comprehensive observability across these diverse data types is difficult. Resource optimization – EMR clusters consist of multiple components and services working together, making it challenging to effectively monitor all aspects of the system. Monitoring resource utilization (CPU, memory, disk I/O) across multiple nodes to prevent bottlenecks and inefficiencies is essential but complex, especially in a distributed environment. Latency and performance metrics – Collecting and analyzing latency and comprehensive performance metrics in real time to identify and resolve issues promptly is critical but challenging due to the distributed nature of Amazon EMR. Centralized observability dashboard – Having a single pane of glass for all aspects of EMR cluster metrics, including cluster health, resource utilization, job execution, logs, and security, to provide a complete picture of system performance and health, is a challenge. Alerting and incident management – Setting up effective centralized alerting and notification systems is difficult. Configuring alerts for critical events or performance thresholds requires careful consideration to avoid \u0026ldquo;alert fatigue\u0026rdquo; while ensuring critical issues are addressed promptly. Responding to incidents from performance slowdowns or interruptions takes time and effort to detect and fix without proper alerting mechanisms. Cost management – Finally, optimizing costs while maintaining effective monitoring is an ongoing challenge. Balancing the need for comprehensive monitoring with cost constraints requires careful planning and optimization strategies to avoid unnecessary expenses while ensuring adequate monitoring capabilities. Effective observability for Amazon EMR requires a combination of the right tools, practices, and strategies to address these challenges, thereby delivering big data processing that is reliable, efficient, and cost-effective.\nThe Ganglia system on Amazon EMR was designed to monitor the health of the entire cluster and all nodes, displaying various metrics such as Hadoop, Spark, and JVM. When viewing the Ganglia web interface in a browser, we see an overview of the EMR cluster\u0026rsquo;s performance, details on load, memory usage, CPU usage, and network traffic via various graphs.\nHowever, with AWS announcing the deprecation of Ganglia support for AWS for higher versions of Amazon EMR, it became critical for FINRA to build this solution.\nSolution Overview\nInsights drawn from the post Monitor and Optimize Analytic Workloads on Amazon EMR with Prometheus and Grafana inspired our approach. This article demonstrated how to set up a monitoring system using Amazon Managed Service for Prometheus and Amazon Managed Grafana to effectively monitor an EMR cluster, using Grafana dashboards to view metrics for troubleshooting and optimizing performance issues.\nBased on these insights, we completed a successful proof of concept. Next, we built a centralized monitoring solution for our enterprise with Managed Prometheus and Managed Grafana to simulate Ganglia-like metrics at FINRA. Managed Prometheus allows for real-time high-volume data collection, scalable ingestion, storage, and querying of operational metrics as workloads scale up or down. These metrics are forwarded to a Managed Grafana workspace for visualization.\nOur solution includes an ingestion layer for every cluster, configured to collect metrics via a custom script stored in Amazon Simple Storage Service. We also installed Managed Prometheus at launch for EC2 instances on Amazon EMR via a bootstrap script. Additionally, application-specific tags are defined in the configuration file to optimize the ingestion and collection of specific metrics.\nOnce Managed Prometheus (installed on EMR clusters) collects the metrics, they are sent to a remote Managed Prometheus workspace. Managed Prometheus workspaces are logical and isolated environments dedicated to Managed Prometheus servers, managing specific metrics. They also provide access control to authorize who or what can send and receive metrics from that workspace. You can create one or more workspaces by account or application depending on needs, facilitating better management.\nAfter the metrics are collected, we built a mechanism to display them on Managed Grafana dashboards, which are then consumed via an endpoint. We customized dashboards for task-level, node-level, and cluster-level metrics so they could be promoted from lower environments to higher ones. We also built several sample dashboards displaying node-level metrics such as OS-level metrics (CPU, memory, network, disk I/O), HDFS metrics, YARN metrics, Spark metrics, and job-level metrics (Spark and JVM), maximizing potential for each environment through automatic metric aggregation within each account.\nWe chose SAML-based authentication, allowing us to integrate with existing Active Directory (AD) groups, minimizing the effort required to manage user access and granting Grafana dashboard access based on user roles. We organized three main groups—admins, editors, and viewers—to authenticate Grafana users based on their roles.\nThrough complex monitoring automation, these desired metrics are pushed to Amazon CloudWatch. We use CloudWatch for alerting when necessary, whenever metrics exceed desired thresholds.\nSample Dashboards The following screenshots introduce the sample dashboards.\nConclusion\nIn this post, we shared how FINRA enhanced data-driven decision-making with comprehensive workload observability on EMR, aiming to optimize performance, maintain reliability, and gain critical insights into big data operations, leading to operational excellence.\nFINRA\u0026rsquo;s solution allowed operations and engineering teams to use a \u0026ldquo;single pane of glass\u0026rdquo; to monitor big data workloads and quickly detect any operational issues. This scalable solution significantly reduced troubleshooting time and improved our overall operational capabilities. The solution provided operations and engineering teams with comprehensive insights into various Amazon EMR metrics such as OS levels, Spark, JMX, HDFS, and YARN, all aggregated in a single place. We also extended this solution to use cases like Amazon Elastic Kubernetes Service (Amazon EKS) clusters, including EMR on EKS and other applications, making it a unified system for monitoring metrics across our entire infrastructure and applications.\nSumalatha Bachu Sumalatha is a Senior Director of Technology at FINRA. She oversees Big Data Operations, including managing petabyte-scale data and handling complex workloads on the cloud platform. Additionally, she is an expert in developing Enterprise Application Monitoring and Observability Solutions, Operational Data Analytics, and Machine Learning Model Governance processes. Outside of work, she enjoys yoga, singing practice, and teaching in her spare time. PremKiran Bejjam PremKiran is a Lead Engineer Consultant at FINRA. He specializes in developing resilient and scalable systems. With a strong focus on designing monitoring solutions to enhance infrastructure reliability, he is dedicated to optimizing system performance. Outside of work, he enjoys spending quality time with family and constantly seeking new learning opportunities. Akhil Chalamalasetty Akhil is a Director of Market Regulation Technology at FINRA. He is a Big Data expert, specializing in building advanced large-scale solutions, along with optimizing workloads, data, and their processing capabilities. Akhil enjoys sim racing and watching Formula 1 in his spare time. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives Stabilize AWS SAM/Serverless deployment environment and resolve critical issues. Focus on debugging core problems: CORS configuration, template validation errors, and API response formatting. Integrate Frontend/Backend to enable end-to-end testing on user interface. Complete basic Read and Delete functionalities with proper error handling. Participate in AWS Cloud Mastery Series event to receive expert guidance and address project challenges. Workshop: Load Balancer Configuration - Set up Application Load Balancer for traffic distribution. Tasks Completed This Week Day Task Start Date Completion Date Resources Mon - Debug CORS: Analyze CORS configuration in API Gateway (CORS headers, preflight OPTIONS requests) and Lambda response headers to allow Frontend access. - Fix template validation errors: Review and optimize template.yaml file to prevent deployment loop errors and resource dependency issues during sam deploy. 11/11/2024 11/11/2024 API Gateway/CORS Documentation Tue - Strengthen Read function (Retrieving flashcard sets): Ensure data is queried from DynamoDB correctly and returned in proper JSON format for Frontend consumption. - Implement error handling for missing records and invalid queries. - Add logging for debugging purposes. 12/11/2024 12/11/2024 DynamoDB Query Documentation Wed - Frontend Integration: Begin combining Frontend codebase with project and test deployed API endpoints. - Successfully display flashcard sets list on user interface. - Test API connectivity and data rendering in React/Vue components. - Workshop Activity: Create Application Load Balancer (ALB) in public subnets and configure target groups for ECS services. 13/11/2024 13/11/2024 Frontend Framework Documentation, Workshop 5.5 Thu - Deploy and test Delete function (Removing flashcard sets). - Encountered Error: Identified authorization issue with Cognito User Sub ID when executing Delete function - Lambda unable to extract/process Sub ID from Cognito token correctly. - Begin troubleshooting authentication flow. 14/11/2024 14/11/2024 AWS Cognito Documentation Fri - Participation in AWS Cloud Mastery Series: + Received expert guidance and clarified questions regarding Serverless architecture, Lambda best practices, and authentication patterns. - Analyze Update/Delete errors: Apply mentor guidance to resolve authorization issues and Cognito token parsing problems. - Document solutions for future reference. 15/11/2024 15/11/2024 Mentor, AWS Cloud Mastery Series Week 10 Achievements Successfully fixed CORS error and stabilized SAM deployment process (mitigated template validation errors). Participated in AWS Cloud Mastery Series event and gathered essential information to solve major project blockers. Completed Frontend and Backend integration, achieving first functional user interface for end-to-end testing. Successfully deployed Read (Retrieving flashcard sets) and Delete (Removing flashcard sets) functionalities, operational on web interface. Identified and gained direction to solve critical bottlenecks: Authorization error: Lambda fails to retrieve/incorrectly process Cognito Sub ID from JWT token, affecting privileged operations Update function dependency: Requires proper authentication flow and token validation The project has transitioned to basic user testing phase with working CRUD operations. Established debugging workflow and error handling patterns for team. Workshop Progress - Load Balancer Configuration:\nCreated Application Load Balancer (ALB) in public subnets across two AZs Configured target groups for Frontend (port 3000) and Backend (port 8080) ECS services Set up health checks for automatic unhealthy target removal Configured SSL/TLS termination using ACM certificates Integrated ALB with Route 53 for DNS routing Implemented listener rules for path-based routing (Frontend: /, Backend: /api/*) Configured security groups to allow ALB traffic to ECS tasks Key Takeaways:\nCORS requires proper configuration in both API Gateway and Lambda response headers Cognito JWT tokens must be properly decoded to extract user identity (Sub ID) Frontend-Backend integration requires careful attention to API contracts and data formats Error handling and logging are essential for debugging production issues AWS Cloud Mastery Series provides valuable real-world insights from experienced practitioners ALB provides intelligent traffic distribution and automatic failover Health checks ensure only healthy targets receive traffic SSL/TLS termination at ALB reduces computational load on backend services "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Participate in AWS Cloud Mastery Series #2 to continue resolving specialized technical issues. Refactor and standardize the Frontend structure for improved stability and maintainability. Implement a Multi-Stack architecture to optimize deployment speed and Serverless project management. Integrate basic CRUD functionalities with AI Image Processing (using Rekognition) into the website. Completely resolve deployment errors (especially CORS issues) to stabilize the system. Workshop: AI Service Architecture, Security \u0026amp; IAM, Monitoring - Complete serverless AI pipeline setup. Tasks to be Deployed This Week: Day Task Start Date Completion Date Resources Sun - Participate in AWS Cloud Mastery Series #2 (Nov 17th): Continue receiving guidance and addressing deeper technical questions about authorization errors and the AI workflow. 17/11/2024 17/11/2024 Mentor, AWS Cloud Mastery Series Mon - Frontend Structure Unification and Refactor: Hold team meeting to standardize the Frontend code structure for maintainability. - Research Multi-Stack Solution: Begin analyzing how to split the template.yaml file into smaller Stacks (Multi-Stack) to optimize the sam deploy process. 18/11/2024 18/11/2024 Serverless Architecture Docs Tue - Implement Multi-Stack Architecture: Start splitting and configuring separate Stacks (e.g., API Backend Stack, Frontend Hosting Stack). - Proceed with AI Image Processing Integration: Combine basic CRUD functions with image processing logic (e.g., calling Rekognition API/S3 trigger) in preparation for the Update function. - Workshop Activity: Set up API Gateway REST endpoints and SQS queues for asynchronous AI processing. 19/11/2024 19/11/2024 Backend Codebase, AWS Rekognition, Workshop 5.7 Wed - Error Encountered after AI Integration: The system faced errors after combining AI functionality, necessitating a full Stack deletion and redeployment. - Leader Develops Backup Stack: The team leader created a separate, optimized Multi-Stack as a contingency and reference for future optimal deployments. 20/11/2024 20/11/2024 Leader\u0026rsquo;s Backup Stack Thu - Persistent CORS Error: After redeploying, the CORS issue re-emerged. - In-depth CORS Debugging: Spent time thoroughly analyzing the root cause and permanently fixing the CORS error, ensuring correct header configuration on both API Gateway and Lambda. - Workshop Activity: Configure IAM roles and policies for Lambda functions, set up Secrets Manager for API keys, and implement WAF rules. 21/11/2024 21/11/2024 API Gateway/Lambda Configuration, Workshop 5.9 Fri - Team Meeting and Project Stabilization: Held a team meeting to review the new Frontend structure, stabilize the main project Stack, and synchronize the fixes for CORS and basic template errors. - Optimization for Maintenance: Finalized the solution to use a separate stack (developed by the leader) for flexibility and easier optimization in future development. 22/11/2024 22/11/2024 New Structure Report Week 11 Achievements: Participated in AWS Cloud Mastery Series #2, gaining deeper knowledge of Serverless, Rekognition, and solutions for authorization errors. Successfully refactored the Frontend and standardized the overall project structure, improving maintainability. Implemented the Multi-Stack architecture (or at least established a reliable solution/backup stack), which speeds up deployment and simplifies resource management. Completely resolved the persistent CORS error after identifying the root cause, ensuring stable communication between Frontend and Backend. Acquired knowledge on fixing basic Template errors and gained a clearer understanding of AWS SAM deployment issues. Developed a separate stack for backup/optimization, enhancing project flexibility and safety during future major updates. The project has moved into the AI functionality testing phase, and although errors were encountered, a clear path for troubleshooting has been established. Workshop Progress - AI Services, Security \u0026amp; Monitoring:\nConfigured API Gateway REST API with three endpoints: /writing/evaluate, /speaking/evaluate, /flashcard/generate Set up SQS queues for asynchronous message processing (writing-queue, speaking-queue, flashcard-queue) Deployed Lambda functions: writing_evaluator, speaking_evaluator, rag_flashcard with proper IAM roles Configured DynamoDB tables: bandup-evaluations and bandup-flashcard-sets for storing AI results Integrated Amazon Bedrock (Titan Embeddings V2) and Google Gemini API for AI processing Set up AWS Secrets Manager for secure API key storage Configured IAM roles with least-privilege permissions for Lambda functions Implemented AWS WAF rules for application-level protection Set up CloudWatch Logs and Alarms for monitoring Lambda execution and errors Configured CloudWatch Insights for log analysis and troubleshooting "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"AWS for Industries: 150 Models and Counting: Your Guide to Generative AI Models for Healthcare and Life Sciences","tags":[],"description":"","content":"Post date: May 07, 2025 – Authors: Malvika Viswanathan, Amrita Sarkar, and Stephanie Dattoli in Amazon Athena, Amazon Bedrock, Healthcare, Industries.\nGenerative AI is reshaping Healthcare and Life Sciences (HCLS). From AstraZeneca and Pfizer to 3M and the Allen Institute, innovators are using AI to drive drug discovery, boost scientist productivity, and streamline clinical workflows.\nAs adoption grows, so does the need for models tailored to unique HCLS challenges—from summarizing medical text and classifying DICOM images to improving drug discovery efficiency. Yet, many organizations still rely on word-of-mouth or industry events to identify the right models, causing both friction and delays in the AI development lifecycle.\nThat’s where Amazon Web Services (AWS) Marketplace comes in. As the largest curated catalog of AI models for healthcare and life sciences, it provides a pre-approved, enterprise-grade procurement path to streamline deployment at scale, accelerating the discovery of industry-specialized models and their rapid integration into clinical, research, or operational workflows. Whether you’re looking to improve cohort selection, accelerate drug development, or scale imaging workflows, AWS Marketplace makes adopting the right generative AI model easy—and fast.\nMany customers are turning to Agentic AI to enable autonomous workflows across the HCLS value chain. Using the right model can help quickly deploy agents to make intelligent, context-aware decisions without model training.\nWe’ll highlight some of the most relevant generative AI models available on AWS Marketplace and explore the AWS services that help operationalize AI at scale in HCLS environments.\nFinding the Right Model With many organizations having laid the groundwork for generative AI adoption, we are seeing a distinct shift. Customers are increasingly moving to industry-specialized models to solve more complex challenges. Some customers are also looking to enhance existing applications with deep-domain models that offer higher accuracy, improved safety, and better outcomes.\nHere are a few standout examples of how leading organizations are leveraging these specialized models (all available on AWS Marketplace):\nBio-FMs for Novel Drug Identification: Generative AI models can help predict critical absorption, distribution, metabolism, and excretion (ADME) properties of drug candidates. By analyzing molecular structures and physicochemical data, these models can forecast how a compound will behave in the body. Evolutionary Scales’ ESMC models are a prime example of this category. Medical Operations and Patient Safety: Generative AI plays a critical role in clinical trial safety monitoring, particularly in identifying potential adverse drug reactions (ADRs). The John Snow Labs Adverse Drug Events (ADE) model is trained on over 400 healthcare entity types, enabling it to accurately detect ADRs in unstructured data sources like social media and clinical notes. Clinical Trial Design: QuantHealth models predict clinical trial endpoints and enrollment. QuantHealth possesses a vast library of clinical development models trained on data from millions of patient lives, predicting safety and efficacy with 85% accuracy across oncology, cardiovascular, and autoimmune diseases. These models will be available on AWS Marketplace in May 2025. Clinical Trial Optimization: Life sciences organizations are increasingly looking to large language models (LLMs) on Amazon Bedrock, such as Anthropic’s Claude, to streamline the creation of clinical trial documentation (e.g., randomized controlled trial protocols). This automation helps research teams focus on study design and patient recruitment. Computer Vision in Clinical Settings: Used for a range of applications from PPE monitoring to assisted diagnostics. For example, VITech Lab\u0026rsquo;s Diabetic Retinopathy Detector analyzes retinal scans to identify and grade diabetes-related anomalies, helping prioritize urgent cases. Pathology Image Analysis for Early Cancer Detection: The Bioptimus H-Optimus-O model is the world’s largest open-source foundation model for pathology, with 1.1 billion parameters. It enables patch-level classification of pathology slides, supporting early cancer detection and reducing diagnostic delays. Medical Summarization for Clinical Workflows: Models like John Snow Medical LLMs are purpose-built to summarize discharge notes, radiology reports, and pathology results. They help clinicians navigate through dense, unstructured data. AI-Assisted Diagnostics and Treatment Planning: Palmyra-Med 70B by Writer is an LLM specifically designed for healthcare, achieving a biomedical benchmark score of 85.87%. It performs advanced entity recognition, extracting medical concepts from unstructured text to support clinical decision-making. Discover, Refine, and Deploy Generative AI Models in HCLS Once you’ve identified the right model, AWS provides a full suite of tools to discover, refine, and deploy them in a secure and compliant environment.\nAmazon SageMaker Unified Studio – Complete Model Development and Tuning Provides a unified, integrated environment designed specifically for building and refining HCLS models. It integrates seamlessly with Amazon EMR, AWS Glue, Amazon Athena, Amazon Redshift, and SageMaker ML services, allowing teams to:\nFine-tune models using specialized HCLS datasets. Handle multimodal data (structured, text, images, omics). Develop and deploy full-stack generative AI applications. Amazon Bedrock – Rapid Model Inference and Evaluation Ideal for quickly deploying and operationalizing pre-trained foundation models (FMs) without managing infrastructure. With over 160 FMs, key features include:\nAmazon Bedrock Evaluations: Quickly evaluate and compare models. API-based access to leading models. Seamless integration with SageMaker and other AWS tools. AWS HealthOmics – Accelerating Scientific Discovery Accelerates biological insights with Ready2Run workflows (from NVIDIA, Sentieon, Element Biosciences), supporting:\nBroad Institute GATK Best Practices. AlphaFold for protein structure prediction. Proprietary drug discovery pipelines. AWS Storage Solutions Secure, scalable data infrastructure:\nAmazon S3: Versatile storage for structured and unstructured data. AWS HealthImaging: Specialized for cloud-native medical imaging. AWS HealthOmics: Optimized for storing and analyzing omics data. Conclusion We have explored key use cases in Healthcare and Life Sciences (HCLS) where specialized generative AI models can drive significant improvements in business performance and patient outcomes.\nWith tools like Amazon SageMaker and Amazon Bedrock, AWS offers one of the richest selections of models tailored to HCLS challenges.\nNext Steps:\nExplore the catalog of generative AI models on AWS Marketplace. Work with AWS teams to identify models that align with your clinical goals. Collaborate with AWS Solution Architects or AWS HCLS Partners to design scalable architectures. Learn more at:\nPre-training genomic language models using AWS HealthOmics and Amazon SageMaker John Snow Labs Medical LLMs are now available in Amazon SageMaker JumpStart Accelerate digital pathology slide annotation workflows on AWS using H-optimus-0 Malvika Viswanathan Malvika Viswanathan is the GenAI Solutions Lead for Healthcare and Life Sciences at AWS. She has 15 years of experience working with provider organizations, insurers, and life sciences entities. Malvika specializes in helping customers apply the latest technology to transform their businesses and deliver optimal patient outcomes. Amrita Sarkar Amrita Sarkar, PhD, is a Principal in the Healthcare \u0026amp; Life Sciences Startups team at AWS. She works with founders and investors to help HCLS startups build at scale. Previously, she was a venture capitalist based in Paris and has mentored hundreds of startups. She holds a PhD in Computational Biology and an MBA from Collège des Ingénieurs. Stephanie Dattoli Stephanie Dattoli is the Worldwide Head of Life Sciences and Genomics Marketing at AWS. Specializing in the intersection of life sciences and cloud technology, she helps organizations bring new products to market. She holds a graduate certificate in genetics from Stanford University. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Complete 100% of the basic CRUD functionalities and AI image processing (including the Update function). Upgrade the image processing architecture by integrating SQS for asynchronous processing and clear flow segmentation. Finalize essential missing project features: Security, Map Pinning, and SNS. Complete the main interfaces of the Frontend, prepare for domain name purchase, and attend the final AWS Cloud Mastery Series event. Tasks to be Deployed This Week: Day Task Start Date Completion Date Resources Mon - Finalize Update Function and AI: Resolve remaining bugs (Sub ID, Rekognition) to ensure CRUD and image processing are fully functional. 25/11/2024 25/11/2024 Mentor Guidance, Backend Codebase Tue - Upgrade AI Flow with SQS: Implement AWS SQS to create an asynchronous image processing queue, enhancing flow segmentation and performance under high load. - Define Clear Processing Flows: Redefine the data flow (Upload -\u0026gt; S3 -\u0026gt; SQS -\u0026gt; Lambda (AI) -\u0026gt; DynamoDB). 26/11/2024 26/11/2024 AWS SQS Documentation, Lambda Architecture Wed - Frontend Interface Finalization: Complete interfaces for the main pages (Homepage, Article Detail, Personal Management Page). - Implement Map Pinning: Integrate Map Pinning functionality for posts, utilizing geo data in DynamoDB or an appropriate map service. 27/11/2024 27/11/2024 Frontend Codebase, DynamoDB Geo Thu - Finalize Security (Authorization): Optimize authentication and permissions (IAM Policy/Cognito), especially accurate Sub retrieval for user operations. - Implement SNS: Integrate AWS SNS for basic notification features (e.g., notification when a post is successfully processed/uploaded). 28/11/2024 28/11/2024 AWS SNS, Cognito/IAM Documentation Fri - Attend the Final AWS Cloud Mastery Series Event: Receive overall project guidance, review and finalize missing parts (domain, security, SNS) before the demo. - Domain Name: Conduct research and prepare for purchasing the website domain, configuring basic DNS (Route 53) as necessary (based on mentor guidance). 29/11/2024 29/11/2024 Mentor, AWS Cloud Mastery Series, Route 53 Week 12 Achievements: Completed 100% of basic CRUD functionalities and AI image processing, ensuring system stability. Upgraded the image processing architecture by integrating SQS and defining clear asynchronous processing flows, improving performance and reliability. Finalized essential features: Implemented Security, Map Pinning, and SNS notifications. The basic Frontend interface is complete, ready for presentation. Successfully participated in the final AWS Cloud Mastery Series event, receiving comprehensive guidance for project completion. Research for domain name purchase has been conducted and planned. The project has reached Demo Readiness status. "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Let&#39;s Architect! Modern Data Architectures","tags":[],"description":"","content":"Post date: Nov 05, 2024 – Authors: Luca Mezzalira, Federica Ciuffo, Vittorio Denti, and Zamira Jaupaj in Amazon Machine Learning, Amazon S3, Architecture, Artificial Intelligence, AWS Big Data.\nData is the fuel for AI; modern data is even more critical for generative AI and advanced data analytics, enabling more accurate, relevant, and impactful results. Modern data comes in many forms: real-time, unstructured, or user-generated. Each form requires a distinct solution.\nAWS\u0026rsquo;s data journey began with Amazon Simple Storage Service (Amazon S3) in 2006, marking the beginning of cloud storage at scale. Since then, AWS has expanded its data services to cover the entire data lifecycle, offering a comprehensive ecosystem designed to maximize the potential of modern data—from ingestion and storage to processing and analytics—supporting the full lifecycle of AI-driven innovation.\nIn this blog post, we will cover several AWS use cases for modern data architectures, demonstrating how AWS helps organizations leverage the power of data and generative AI technologies.\nKey considerations when choosing a database for your generative AI applications This blog post focuses on selecting the right database for generative AI applications and provides knowledge that can enhance your understanding, guide decision-making, and ultimately lead to more successful AI projects. Choosing the right database for generative AI applications is not just about storage; it significantly impacts performance, scalability, ease of integration, and the overall efficiency of the AI solution.\nFigure 1. Diagram describing the key steps in the RAG workflow\nTake me to this blog\nStrategies for building a data mesh-based enterprise solution on AWS Adopting a data mesh architecture can enhance an organization\u0026rsquo;s effective data management, thereby improving performance, driving innovation, and achieving overall business success. In this guidance, you will explore several strategies for building data mesh solutions on AWS.\nFigure 2. Data mesh architecture organizes data into domains, where data is treated as quality products to be served to users\nTake me to this guidance\nOptimizing storage price and performance with Amazon S3 Amazon S3 is an object storage service that supports many use cases, including data architectures. Big data pipelines can use Amazon S3 to store input data, output data, and temporary results. Machine learning systems use Amazon S3 to process application logs and build datasets for both testing and production model training.\nGiven the importance of this service and the number of use cases a foundational storage service can support, we want to share best practices, performance optimization strategies, and cost optimization strategies when working with Amazon S3. This video shows how Anthropic designed their architecture around Amazon S3 in their data architecture.\nFigure 3. Workloads with predictable access patterns often have low retrieval rates over a long period, so we can design to apply cheaper storage classes for them.\nTake me to this video\nNote: If you are curious about the foundational architecture of Amazon S3 and want to dive deeper into its internal design, you can watch the video Dive deep on Amazon S3 from re:Invent.\nHow HPE Aruba Supply Chain optimized cost and performance by migrating to an AWS modern data architecture This is an AWS case study on how HPE Aruba Supply Chain successfully re-architected and deployed their data solution by adopting a modern data architecture on AWS.\nThe new solution helped Aruba integrate data from multiple sources while optimizing cost, performance, and scalability. This also enabled Aruba Supply Chain leadership to receive timely insights to make better decisions, thereby enhancing the customer experience.\nFigure 4. Reference architecture diagram describing the HPE Aruba Supply Chain architecture, with Amazon S3 as a prominent component.\nTake me to this blog\nAWS Modern Data Architecture Immersion Day This workshop highlights the advantages of adopting a modern data architecture on AWS. By integrating the flexibility of a data lake with specialized analytics services, organizations can significantly enhance their data-driven decision-making capabilities.\nWe encourage everyone to explore how this architecture can streamline analytics processes and support diverse use cases, from gathering real-time insights to advanced machine learning. This is a great opportunity to leverage modern data architecture.\nFigure 5. Data architectures are the foundation for powering use cases, from analytics to machine learning.\nTake me to this workshop\nSee you next time!\nThanks for reading! In our next blog post, we will share some tips to help you have the best experience working as a developer on AWS. To revisit any previous posts or explore the entire series, visit the Let’s Architect! page.\nLuca Mezzalira Luca is a Principal Solutions Architect based in London. He is an author of multiple books and an international speaker. Luca specializes in solutions architecture and has received accolades for revolutionizing front-end architecture scalability using micro-frontends. Federica Ciuffo Federica is a Solutions Architect at Amazon Web Services. She specializes in container services and is passionate about infrastructure as code. Outside of work, she enjoys reading, drawing, and exploring cuisines. Vittorio Denti Vittorio Denti is a Machine Learning Engineer at Amazon, based in London. He has a background in distributed systems and machine learning. He is particularly passionate about software engineering and the latest innovations in machine learning science. Zamira Jaupaj Zamira is an Enterprise Solutions Architect based in the Netherlands. She is a passionate IT professional with over 10 years of experience designing and implementing complex solutions using containers, serverless, and data analytics. "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]